run.log_dir = "./logs/"
run.data_settings = {
    'path_to_train_data': './data/raw/570-ws-training.csv',
    'path_to_test_data': './data/raw/570-ws-testing.csv',
    'input_features': ['_value'],
    'labels': ['_value],
    'preprocessing_method': 'min_max',
    'preprocess_input_features': False,
    'preprocess_label': False,
    'frequency':'5min'
}
run.llm_settings = {
    'task_name':'long_term_forecast',
    'mode': 'training+inference',    # available modes: 'inference', 'training', 'training+inference'
    'method': 'time_llm',    # available methods: 'chronos','time_llm'
    'model': 'LLAMA',  # available models: 'LLAMA', 'GPT2', 'BERT'
    'llama_layers:'32,
    'llm_dim':4096,


    'sequence_length': 1,
    'context_length': 6,
    'prediction_length': 6,    # Note: maximal prediction length is 64 for chronos pretrained models

    'prediction_batch_size': 64,    # Note: Large batch size may cause OOM error
    'train_batch_size': 32,
    'learning_rate':0.001,
    'train_epochs':1,

    'd_model':32,
    'd_ff':32,
    'factor':1,
    'enc_in':1,
    'dec_in':1,
    'c_out':1,
    'e_layers':2,
    'd_layers':1,
    'n_heads':8,
    'drop_out':0.1,
    'moving_avg':25,
    'activation':'gelu',
    'embed':'timeF',
    'stride': 8,
    'path_len':16,
    'patience':10,
    'lradj':'COS',
    'des':'test',




    'model_comment':'time_llm_t1dm',
    'prompt_domain'0,

    'eval_metrics': ['rmse', 'mae', 'mape'],    # available metrics: 'rmse', 'mae', 'mape'

}
