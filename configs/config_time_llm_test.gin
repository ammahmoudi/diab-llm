run.log_dir = "./logs/"
run.data_settings = {
    'path_to_train_data': './data/raw/570-ws-training.csv',
    'path_to_test_data': './data/raw/570-ws-testing.csv',
    'input_features': ['_value'],
    'labels': ['_value'],
    'prompt_path':'./data/raw/t1dm_prompt.txt',
    'preprocessing_method': 'min_max',
    'preprocess_input_features': False,
    'preprocess_label': False,
    'frequency':'5min',
    'percent':3,
}
run.llm_settings = {
    'task_name':'long_term_forecast',
    'mode': 'inference',    # available modes: 'inference', 'training', 'training+inference'
    'method': 'time_llm',    # available methods: 'chronos','time_llm'
    'llm_model': 'GPT2',  # available models: 'LLAMA', 'GPT2', 'BERT'
    'llm_layers':32,
    'llm_dim':768, #4096 for llama 768 for gpt2
    'num_workers':1,
    'restore_from_checkpoint':'true', # completly remove it to avoid
    'restore_checkpoint_path':'logs/logs_2025-01-16_17-58-46/checkpoints/final_checkpoint',
    'torch_dtype':'bfloat16',


    'model_id':'test',


    'sequence_length': 6,
    'context_length': 6,
    'prediction_length': 6,
    'patch_len':6,
    'stride': 8,

    'prediction_batch_size': 64,    # Note: Large batch size may cause OOM error
    'train_batch_size': 1,
    'learning_rate':0.001,
    'train_epochs':1,
    'features':'S',

    'd_model':32,
    'd_ff':32,
    'factor':1,
    'enc_in':1,
    'dec_in':1,
    'c_out':1,
    'e_layers':2,
    'd_layers':1,
    'n_heads':8,
    'dropout':0.1,
    'moving_avg':25,
    'activation':'gelu',
    'embed':'timeF',
    'patience':10,
    'lradj':'COS',
    'des':'test',
    'model_comment':'time_llm_t1dm',
    'prompt_domain':0,

    'eval_metrics': ['rmse', 'mae', 'mape'],    # available metrics: 'rmse', 'mae', 'mape'

}
