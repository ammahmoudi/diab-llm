run.log_dir = "./logs/"
run.chronos_dir = "/home/amma/"
run.data_settings = {
    'path_to_train_data': '/home/amma/LLM-TIME/data/standardized_arrow/570-ws-training.arrow',
    'path_to_test_data': './data/standardized/570-ws-testing.csv',
    'input_features': ['target'],
    'labels': ['target'],
    'preprocessing_method': 'min_max',
    'preprocess_input_features': False,
    'preprocess_label': False,
    'percent':100
}
run.llm_settings = {
    'mode': 'training',    # available modes: 'inference', 'training', 'training+inference'
    'method': 'chronos',    # available methods: 'chronos'
    'model': 'amazon/chronos-t5-base',  # available models: 'amazon/chronos-t5-tiny', '...-mini', '...-small', '...base', '...-large'
    'torch_dtype': 'bfloat16',   # available dtypes: 'float16', 'bfloat16', 'float32'
    'ntokens': 4096,
    'tokenizer_kwargs': "{'low_limit': 35,'high_limit': 500}",
    'context_length': 512,
    'prediction_length': 64,    # Note: maximal prediction length is 64 for chronos pretrained models
    'min_past': 60,
    'learning_rate':0.001,
    'num_samples': 20,
    'max_train_steps': 200_000,
    'save_steps': 100_000,
    'log_steps':500,
    'train_batch_size': 32,
    'random_init': False,
    'seed':202231  #optional seed to reproduce results. remove to get random seed.


}
