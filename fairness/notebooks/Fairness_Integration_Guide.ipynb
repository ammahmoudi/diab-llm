{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9c24c0",
   "metadata": {},
   "source": [
    "# Fairness-Aware Training Integration Guide\n",
    "\n",
    "This notebook shows you **exactly** how to integrate fairness-aware training into your distillation pipeline.\n",
    "\n",
    "## üéØ Your Situation\n",
    "Based on your analysis:\n",
    "- **Current fairness**: 1.10x ratio (excellent!)\n",
    "- **Distillation impact**: Slightly improves fairness\n",
    "- **Recommendation**: No immediate intervention needed, but this guide shows you how to add fairness-aware training for future experiments\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to modify your training code for fairness\n",
    "2. Different fairness loss functions and when to use them\n",
    "3. How to monitor fairness during training\n",
    "4. Code examples you can copy-paste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d47686",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c7541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully!\n",
      "Ready for fairness-aware training integration\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for fairness-aware training\n",
    "import sys\n",
    "sys.path.append('../')  # Add parent directory to path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import your existing training modules\n",
    "# from models.time_llm import TimeLLM  # Adjust path as needed\n",
    "# from distillation.distillation_trainer import DistillationTrainer  # Adjust path as needed\n",
    "\n",
    "# Import our fairness analyzer\n",
    "from gender_fairness_analyzer import GenderFairnessAnalyzer\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully!\")\n",
    "print(\"Ready for fairness-aware training integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffceaf5c",
   "metadata": {},
   "source": [
    "## Step 2: Fairness Loss Functions\n",
    "\n",
    "Here are practical loss functions you can add to your training loop to ensure fair predictions across gender groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ef93c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fairness loss functions defined!\n",
      "Available losses:\n",
      "- demographic_parity_loss: Ensures similar prediction distributions\n",
      "- equalized_odds_loss: Ensures similar accuracy across groups\n",
      "- group_regularization_loss: Minimizes performance variance\n"
     ]
    }
   ],
   "source": [
    "class FairnessLosses(nn.Module):\n",
    "    \"\"\"Collection of fairness loss functions for training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def demographic_parity_loss(self, predictions, gender_labels, lambda_fairness=0.1):\n",
    "        \"\"\"\n",
    "        Enforces similar prediction distributions across gender groups\n",
    "        Lower is more fair (0 = perfect parity)\n",
    "        \"\"\"\n",
    "        male_mask = (gender_labels == 1)  # Assuming 1 = male, 0 = female\n",
    "        female_mask = (gender_labels == 0)\n",
    "        \n",
    "        if male_mask.sum() == 0 or female_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        male_mean = predictions[male_mask].mean()\n",
    "        female_mean = predictions[female_mask].mean()\n",
    "        \n",
    "        # Penalize difference in means\n",
    "        parity_loss = torch.abs(male_mean - female_mean)\n",
    "        return lambda_fairness * parity_loss\n",
    "    \n",
    "    def equalized_odds_loss(self, predictions, targets, gender_labels, lambda_fairness=0.1):\n",
    "        \"\"\"\n",
    "        Enforces similar true/false positive rates across groups\n",
    "        \"\"\"\n",
    "        male_mask = (gender_labels == 1)\n",
    "        female_mask = (gender_labels == 0)\n",
    "        \n",
    "        if male_mask.sum() == 0 or female_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # For regression: use threshold to create binary classification\n",
    "        threshold = targets.mean()\n",
    "        pred_binary = (predictions > threshold).float()\n",
    "        target_binary = (targets > threshold).float()\n",
    "        \n",
    "        # True positive rates\n",
    "        male_tpr = (pred_binary[male_mask] * target_binary[male_mask]).sum() / (target_binary[male_mask].sum() + 1e-8)\n",
    "        female_tpr = (pred_binary[female_mask] * target_binary[female_mask]).sum() / (target_binary[female_mask].sum() + 1e-8)\n",
    "        \n",
    "        # False positive rates  \n",
    "        male_fpr = (pred_binary[male_mask] * (1-target_binary[male_mask])).sum() / ((1-target_binary[male_mask]).sum() + 1e-8)\n",
    "        female_fpr = (pred_binary[female_mask] * (1-target_binary[female_mask])).sum() / ((1-target_binary[female_mask]).sum() + 1e-8)\n",
    "        \n",
    "        tpr_diff = torch.abs(male_tpr - female_tpr)\n",
    "        fpr_diff = torch.abs(male_fpr - female_fpr)\n",
    "        \n",
    "        return lambda_fairness * (tpr_diff + fpr_diff)\n",
    "    \n",
    "    def group_regularization_loss(self, predictions, gender_labels, lambda_fairness=0.1):\n",
    "        \"\"\"\n",
    "        Minimizes variance in performance across groups\n",
    "        \"\"\"\n",
    "        male_mask = (gender_labels == 1)\n",
    "        female_mask = (gender_labels == 0)\n",
    "        \n",
    "        if male_mask.sum() == 0 or female_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        male_var = predictions[male_mask].var()\n",
    "        female_var = predictions[female_mask].var()\n",
    "        \n",
    "        # Encourage similar variance across groups\n",
    "        variance_diff = torch.abs(male_var - female_var)\n",
    "        return lambda_fairness * variance_diff\n",
    "\n",
    "# Initialize fairness losses\n",
    "fairness_losses = FairnessLosses()\n",
    "\n",
    "print(\"‚úÖ Fairness loss functions defined!\")\n",
    "print(\"Available losses:\")\n",
    "print(\"- demographic_parity_loss: Ensures similar prediction distributions\")  \n",
    "print(\"- equalized_odds_loss: Ensures similar accuracy across groups\")\n",
    "print(\"- group_regularization_loss: Minimizes performance variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c3ab3",
   "metadata": {},
   "source": [
    "## Step 3: Modified Training Loop\n",
    "\n",
    "Here's how to integrate fairness losses into your existing training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b8082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fairness-aware training step defined!\n",
      "This function integrates fairness constraints into your training loop\n",
      "Use lambda_fairness to control accuracy vs fairness tradeoff\n"
     ]
    }
   ],
   "source": [
    "def fairness_aware_training_step(model, batch, optimizer, fairness_losses, lambda_fairness=0.1):\n",
    "    \"\"\"\n",
    "    Modified training step that includes fairness constraints\n",
    "    \n",
    "    Args:\n",
    "        model: Your TimeLLM or distillation model\n",
    "        batch: Training batch (should include gender labels)\n",
    "        optimizer: Your optimizer\n",
    "        fairness_losses: FairnessLosses instance\n",
    "        lambda_fairness: Weight for fairness vs accuracy tradeoff\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch components (adjust based on your data structure)\n",
    "    inputs = batch['inputs']  # Time series data\n",
    "    targets = batch['targets']  # Prediction targets\n",
    "    gender_labels = batch['gender']  # Gender labels (0=female, 1=male)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.train()\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # Primary loss (your existing loss function)\n",
    "    primary_loss = F.mse_loss(predictions, targets)  # Adjust based on your task\n",
    "    \n",
    "    # Fairness losses\n",
    "    dp_loss = fairness_losses.demographic_parity_loss(predictions, gender_labels, lambda_fairness)\n",
    "    eo_loss = fairness_losses.equalized_odds_loss(predictions, targets, gender_labels, lambda_fairness)\n",
    "    gr_loss = fairness_losses.group_regularization_loss(predictions, gender_labels, lambda_fairness)\n",
    "    \n",
    "    # Total loss with fairness constraints\n",
    "    total_loss = primary_loss + dp_loss + eo_loss + gr_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return detailed loss breakdown for monitoring\n",
    "    return {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'primary_loss': primary_loss.item(),\n",
    "        'demographic_parity_loss': dp_loss.item(),\n",
    "        'equalized_odds_loss': eo_loss.item(),\n",
    "        'group_regularization_loss': gr_loss.item()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Fairness-aware training step defined!\")\n",
    "print(\"This function integrates fairness constraints into your training loop\")\n",
    "print(\"Use lambda_fairness to control accuracy vs fairness tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f89111",
   "metadata": {},
   "source": [
    "## Step 4: Data Loader with Gender Labels\n",
    "\n",
    "Your data loader needs to include gender information. Here's how to modify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbcf84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fairness-aware dataset class defined!\n",
      "Modify the load_time_series_data, get_targets, and get_patient_id methods\n",
      "to match your specific data format and structure\n"
     ]
    }
   ],
   "source": [
    "class FairnessAwareDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper that includes gender labels for fairness training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, patient_info_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to your time series data\n",
    "            patient_info_path: Path to patient demographics (should include gender)\n",
    "        \"\"\"\n",
    "        # Load your existing data\n",
    "        self.data = self.load_time_series_data(data_path)\n",
    "        \n",
    "        # Load patient demographics\n",
    "        self.patient_info = pd.read_csv(patient_info_path)\n",
    "        \n",
    "        # Create gender mapping (adjust column names as needed)\n",
    "        self.gender_map = {}\n",
    "        for _, row in self.patient_info.iterrows():\n",
    "            patient_id = row['patient_id']  # Adjust column name\n",
    "            gender = 1 if row['gender'].lower() in ['m', 'male'] else 0  # Adjust column name\n",
    "            self.gender_map[patient_id] = gender\n",
    "    \n",
    "    def load_time_series_data(self, data_path):\n",
    "        \"\"\"Load your time series data - implement based on your format\"\"\"\n",
    "        # This is a placeholder - replace with your actual data loading logic\n",
    "        return torch.randn(1000, 100, 1)  # Example: 1000 samples, 100 timesteps, 1 feature\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get time series data\n",
    "        inputs = self.data[idx]\n",
    "        targets = self.get_targets(idx)  # Implement based on your task\n",
    "        \n",
    "        # Get patient ID and corresponding gender\n",
    "        patient_id = self.get_patient_id(idx)  # Implement based on your data structure\n",
    "        gender = self.gender_map.get(patient_id, 0)  # Default to female if unknown\n",
    "        \n",
    "        return {\n",
    "            'inputs': inputs,\n",
    "            'targets': targets,\n",
    "            'gender': torch.tensor(gender, dtype=torch.float32),\n",
    "            'patient_id': patient_id\n",
    "        }\n",
    "    \n",
    "    def get_targets(self, idx):\n",
    "        \"\"\"Implement based on your prediction task\"\"\"\n",
    "        # Placeholder - replace with actual target extraction\n",
    "        return torch.randn(1)\n",
    "    \n",
    "    def get_patient_id(self, idx):\n",
    "        \"\"\"Extract patient ID from your data structure\"\"\"\n",
    "        # Placeholder - implement based on how you store patient IDs\n",
    "        return f\"patient_{idx % 100}\"  # Example mapping\n",
    "\n",
    "# Example usage\n",
    "def create_fairness_aware_dataloader(data_path, patient_info_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a DataLoader that includes gender information\n",
    "    \"\"\"\n",
    "    dataset = FairnessAwareDataset(data_path, patient_info_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "print(\"‚úÖ Fairness-aware dataset class defined!\")\n",
    "print(\"Modify the load_time_series_data, get_targets, and get_patient_id methods\")\n",
    "print(\"to match your specific data format and structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf191c6",
   "metadata": {},
   "source": [
    "## Step 5: Complete Training Integration\n",
    "\n",
    "Put it all together - here's a complete training function with fairness monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a689fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete fairness-aware training system defined!\n",
      "Key features:\n",
      "- Integrates fairness losses into training\n",
      "- Monitors fairness metrics during validation\n",
      "- Saves best model based on fairness criteria\n",
      "- Provides detailed loss and fairness tracking\n"
     ]
    }
   ],
   "source": [
    "def train_with_fairness_monitoring(model, train_loader, val_loader, num_epochs=10, lambda_fairness=0.1):\n",
    "    \"\"\"\n",
    "    Complete training function with fairness monitoring and checkpointing\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    fairness_losses = FairnessLosses()\n",
    "    analyzer = GenderFairnessAnalyzer()\n",
    "    \n",
    "    # Tracking\n",
    "    train_history = defaultdict(list)\n",
    "    fairness_history = []\n",
    "    \n",
    "    best_fairness_ratio = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_losses = defaultdict(list)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Fairness-aware training step\n",
    "            loss_dict = fairness_aware_training_step(\n",
    "                model, batch, optimizer, fairness_losses, lambda_fairness\n",
    "            )\n",
    "            \n",
    "            # Track losses\n",
    "            for key, value in loss_dict.items():\n",
    "                epoch_losses[key].append(value)\n",
    "            \n",
    "            if batch_idx % 50 == 0:  # Print every 50 batches\n",
    "                print(f\"  Batch {batch_idx}: Total Loss = {loss_dict['total_loss']:.4f}\")\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        for key, values in epoch_losses.items():\n",
    "            avg_value = np.mean(values)\n",
    "            train_history[key].append(avg_value)\n",
    "            print(f\"  Avg {key}: {avg_value:.4f}\")\n",
    "        \n",
    "        # Validation and fairness evaluation\n",
    "        if val_loader is not None:\n",
    "            fairness_metrics = evaluate_fairness(model, val_loader, analyzer)\n",
    "            fairness_history.append(fairness_metrics)\n",
    "            \n",
    "            current_fairness_ratio = fairness_metrics['fairness_ratio']\n",
    "            print(f\"  Fairness Ratio: {current_fairness_ratio:.3f}\")\n",
    "            print(f\"  Fairness Level: {fairness_metrics['fairness_level']}\")\n",
    "            \n",
    "            # Save best model based on fairness\n",
    "            if current_fairness_ratio < best_fairness_ratio and current_fairness_ratio >= 1.0:\n",
    "                best_fairness_ratio = current_fairness_ratio\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'fairness_ratio': current_fairness_ratio,\n",
    "                    'fairness_metrics': fairness_metrics\n",
    "                }, 'best_fair_model.pth')\n",
    "                print(f\"  ‚úÖ New best fair model saved! Ratio: {current_fairness_ratio:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_history': train_history,\n",
    "        'fairness_history': fairness_history,\n",
    "        'best_fairness_ratio': best_fairness_ratio\n",
    "    }\n",
    "\n",
    "def evaluate_fairness(model, val_loader, analyzer):\n",
    "    \"\"\"\n",
    "    Evaluate fairness metrics on validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_genders = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['inputs']\n",
    "            targets = batch['targets']\n",
    "            genders = batch['gender']\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_genders.extend(genders.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate group-specific performance\n",
    "    male_mask = np.array(all_genders) == 1\n",
    "    female_mask = np.array(all_genders) == 0\n",
    "    \n",
    "    male_mse = np.mean((np.array(all_predictions)[male_mask] - np.array(all_targets)[male_mask]) ** 2)\n",
    "    female_mse = np.mean((np.array(all_predictions)[female_mask] - np.array(all_targets)[female_mask]) ** 2)\n",
    "    \n",
    "    # Calculate fairness ratio (should be close to 1.0)\n",
    "    fairness_ratio = max(male_mse, female_mse) / min(male_mse, female_mse)\n",
    "    \n",
    "    # Determine fairness level\n",
    "    if fairness_ratio <= 1.10:\n",
    "        fairness_level = \"Excellent\"\n",
    "    elif fairness_ratio <= 1.25:\n",
    "        fairness_level = \"Good\"\n",
    "    elif fairness_ratio <= 1.50:\n",
    "        fairness_level = \"Acceptable\"\n",
    "    else:\n",
    "        fairness_level = \"Poor\"\n",
    "    \n",
    "    return {\n",
    "        'male_mse': male_mse,\n",
    "        'female_mse': female_mse,\n",
    "        'fairness_ratio': fairness_ratio,\n",
    "        'fairness_level': fairness_level,\n",
    "        'male_count': male_mask.sum(),\n",
    "        'female_count': female_mask.sum()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Complete fairness-aware training system defined!\")\n",
    "print(\"Key features:\")\n",
    "print(\"- Integrates fairness losses into training\")\n",
    "print(\"- Monitors fairness metrics during validation\") \n",
    "print(\"- Saves best model based on fairness criteria\")\n",
    "print(\"- Provides detailed loss and fairness tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80e23f",
   "metadata": {},
   "source": [
    "## Step 6: Usage Example\n",
    "\n",
    "Here's how to use the fairness-aware training in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9178b6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/patient_demographics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: How to integrate with your existing training pipeline\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Create fairness-aware data loaders\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fairness_aware_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath/to/your/train_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatient_info_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath/to/patient_demographics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m create_fairness_aware_dataloader(\n\u001b[1;32m     11\u001b[0m     data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/val_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     12\u001b[0m     patient_info_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/patient_demographics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Initialize your model (use your existing model)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model = TimeLLM(config)  # Your existing model initialization\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model = DistillationModel(teacher, student)  # Or distillation setup\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#     plt.tight_layout()\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#     plt.show()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mcreate_fairness_aware_dataloader\u001b[0;34m(data_path, patient_info_path, batch_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_fairness_aware_dataloader\u001b[39m(data_path, patient_info_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Create a DataLoader that includes gender information\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFairnessAwareDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_info_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mFairnessAwareDataset.__init__\u001b[0;34m(self, data_path, patient_info_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_time_series_data(data_path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load patient demographics\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatient_info \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_info_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create gender mapping (adjust column names as needed)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgender_map \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/workspace/LLM-TIME/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/LLM-TIME/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/workspace/LLM-TIME/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/LLM-TIME/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/workspace/LLM-TIME/venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/patient_demographics.csv'"
     ]
    }
   ],
   "source": [
    "# Example: How to integrate with your existing training pipeline\n",
    "\n",
    "# 1. Create fairness-aware data loaders\n",
    "train_loader = create_fairness_aware_dataloader(\n",
    "    data_path=\"path/to/your/train_data\",\n",
    "    patient_info_path=\"path/to/patient_demographics.csv\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_loader = create_fairness_aware_dataloader(\n",
    "    data_path=\"path/to/your/val_data\", \n",
    "    patient_info_path=\"path/to/patient_demographics.csv\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 2. Initialize your model (use your existing model)\n",
    "# model = TimeLLM(config)  # Your existing model initialization\n",
    "# model = DistillationModel(teacher, student)  # Or distillation setup\n",
    "\n",
    "# 3. Train with fairness constraints\n",
    "# results = train_with_fairness_monitoring(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     num_epochs=50,\n",
    "#     lambda_fairness=0.1  # Adjust this to balance fairness vs accuracy\n",
    "# )\n",
    "\n",
    "# 4. Plot training progress\n",
    "# def plot_fairness_training_progress(results):\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "#     \n",
    "#     # Loss evolution\n",
    "#     axes[0,0].plot(results['train_history']['total_loss'], label='Total Loss')\n",
    "#     axes[0,0].plot(results['train_history']['primary_loss'], label='Primary Loss')\n",
    "#     axes[0,0].set_title('Training Loss Evolution')\n",
    "#     axes[0,0].legend()\n",
    "#     \n",
    "#     # Fairness losses\n",
    "#     axes[0,1].plot(results['train_history']['demographic_parity_loss'], label='Demographic Parity')\n",
    "#     axes[0,1].plot(results['train_history']['equalized_odds_loss'], label='Equalized Odds')\n",
    "#     axes[0,1].plot(results['train_history']['group_regularization_loss'], label='Group Regularization')\n",
    "#     axes[0,1].set_title('Fairness Losses')\n",
    "#     axes[0,1].legend()\n",
    "#     \n",
    "#     # Fairness ratio evolution\n",
    "#     fairness_ratios = [f['fairness_ratio'] for f in results['fairness_history']]\n",
    "#     axes[1,0].plot(fairness_ratios)\n",
    "#     axes[1,0].axhline(y=1.10, color='green', linestyle='--', label='Excellent (‚â§1.10)')\n",
    "#     axes[1,0].axhline(y=1.25, color='yellow', linestyle='--', label='Good (‚â§1.25)')\n",
    "#     axes[1,0].set_title('Fairness Ratio Evolution')\n",
    "#     axes[1,0].legend()\n",
    "#     \n",
    "#     # Group performance\n",
    "#     male_mses = [f['male_mse'] for f in results['fairness_history']]\n",
    "#     female_mses = [f['female_mse'] for f in results['fairness_history']]\n",
    "#     axes[1,1].plot(male_mses, label='Male MSE')\n",
    "#     axes[1,1].plot(female_mses, label='Female MSE')\n",
    "#     axes[1,1].set_title('Group-specific Performance')\n",
    "#     axes[1,1].legend()\n",
    "#     \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "print(\"üìã Usage Example Defined!\")\n",
    "print(\"\\nüîß Integration Steps:\")\n",
    "print(\"1. Replace placeholder paths with your actual data paths\")\n",
    "print(\"2. Replace model initialization with your actual model\") \n",
    "print(\"3. Uncomment and run the training code\")\n",
    "print(\"4. Use plot_fairness_training_progress() to visualize results\")\n",
    "print(\"\\n‚öñÔ∏è Tuning Tips:\")\n",
    "print(\"- Start with lambda_fairness=0.05 for subtle fairness constraints\")\n",
    "print(\"- Increase to 0.1-0.2 for stronger fairness enforcement\") \n",
    "print(\"- Monitor both accuracy and fairness metrics during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6ddd6",
   "metadata": {},
   "source": [
    "## Step 7: Quick Test with Your Current Setup\n",
    "\n",
    "Let's test the fairness analyzer with your current distillation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fairness analyzer on your current results\n",
    "import os\n",
    "\n",
    "# Check current fairness level of your distillation experiments\n",
    "analyzer = GenderFairnessAnalyzer()\n",
    "\n",
    "# Look for the latest experiment\n",
    "distillation_dir = \"/workspace/LLM-TIME/distillation_experiments\"\n",
    "if os.path.exists(distillation_dir):\n",
    "    print(\"üîç Analyzing current fairness level...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the analyzer\n",
    "        results = analyzer.analyze_latest_experiment()\n",
    "        \n",
    "        print(f\"\\nüìä Current Fairness Analysis:\")\n",
    "        print(f\"üöπ Male patients: {results['male_performance']['count']}\")\n",
    "        print(f\"üö∫ Female patients: {results['female_performance']['count']}\")\n",
    "        print(f\"‚öñÔ∏è  Fairness ratio: {results['fairness_ratio']:.3f}\")\n",
    "        print(f\"üéØ Fairness level: {results['fairness_level']}\")\n",
    "        \n",
    "        if results['fairness_ratio'] <= 1.10:\n",
    "            print(\"‚úÖ Great! Your current model already shows excellent fairness\")\n",
    "            print(\"   The integration guide above can help maintain this during future training\")\n",
    "        elif results['fairness_ratio'] <= 1.25:\n",
    "            print(\"üëç Good fairness level - slight improvements possible\")\n",
    "            print(\"   Consider using lambda_fairness=0.05 for gentle fairness constraints\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Fairness could be improved\")\n",
    "            print(\"   Recommend using lambda_fairness=0.1-0.2 for stronger fairness constraints\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not analyze current results: {e}\")\n",
    "        print(\"   Make sure you have run some distillation experiments first\")\n",
    "        \n",
    "else:\n",
    "    print(\"üìù No distillation experiments found yet\")\n",
    "    print(\"   Run some experiments, then use this guide to make them fairer!\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"1. If fairness is already good: Use this guide to maintain it during training\")\n",
    "print(\"2. If fairness needs improvement: Integrate the fairness losses above\") \n",
    "print(\"3. Always monitor both accuracy AND fairness during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccc60f",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You now have a complete fairness-aware training system.\n",
    "\n",
    "### What You've Learned:\n",
    "- **Fairness Loss Functions**: Demographic parity, equalized odds, and group regularization\n",
    "- **Modified Training Loop**: Integrates fairness constraints with your existing training\n",
    "- **Monitoring System**: Tracks both accuracy and fairness metrics during training  \n",
    "- **Best Model Selection**: Saves models based on fairness criteria\n",
    "\n",
    "### Integration Checklist:\n",
    "- [ ] Modify your dataset class to include gender labels\n",
    "- [ ] Add fairness losses to your training loop\n",
    "- [ ] Set appropriate `lambda_fairness` values (start with 0.05-0.1)\n",
    "- [ ] Monitor fairness metrics during validation\n",
    "- [ ] Save best models based on fairness criteria\n",
    "\n",
    "### Recommended Workflow:\n",
    "1. **Baseline**: Train your model normally and measure fairness\n",
    "2. **Integration**: Add fairness constraints with low Œª (0.05)\n",
    "3. **Tuning**: Gradually increase Œª until you achieve desired fairness\n",
    "4. **Monitoring**: Always validate on both accuracy and fairness metrics\n",
    "\n",
    "### Fairness Thresholds:\n",
    "- **Excellent**: Ratio ‚â§ 1.10 (‚â§10% difference between groups)  \n",
    "- **Good**: Ratio ‚â§ 1.25 (‚â§25% difference)\n",
    "- **Acceptable**: Ratio ‚â§ 1.50 (‚â§50% difference)\n",
    "\n",
    "### Support Resources:\n",
    "- `gender_fairness_analyzer.py` - Analyze existing results\n",
    "- `Gender_Fairness_Analysis.ipynb` - Interactive fairness analysis\n",
    "- `README.md` - Complete documentation and guides\n",
    "\n",
    "**Happy Fair Training! ‚öñÔ∏èüöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1baf2b",
   "metadata": {},
   "source": [
    "## Extended Fairness Analysis - Multiple Demographics\n",
    "\n",
    "Want to check fairness across other features like age, race, or disease severity? Here's how to extend the framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFairnessAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze fairness across multiple demographic features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_features = {\n",
    "            'gender': {'male': 1, 'female': 0, 'm': 1, 'f': 0},\n",
    "            'age_group': {'young': 0, 'middle': 1, 'old': 2},  # You can define age ranges\n",
    "            'race': {'white': 0, 'black': 1, 'hispanic': 2, 'asian': 3, 'other': 4},\n",
    "            'disease_severity': {'mild': 0, 'moderate': 1, 'severe': 2},\n",
    "            'bmi_category': {'underweight': 0, 'normal': 1, 'overweight': 2, 'obese': 3}\n",
    "        }\n",
    "    \n",
    "    def load_patient_demographics(self, demographics_path):\n",
    "        \"\"\"Load patient demographics with multiple attributes\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(demographics_path)\n",
    "            print(f\"üìä Loaded demographics for {len(df)} patients\")\n",
    "            print(f\"üìã Available columns: {list(df.columns)}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading demographics: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_feature_fairness(self, results_path, demographics_df, feature_column, feature_name=None):\n",
    "        \"\"\"\n",
    "        Analyze fairness for any demographic feature\n",
    "        \n",
    "        Args:\n",
    "            results_path: Path to experiment results JSON\n",
    "            demographics_df: DataFrame with patient demographics\n",
    "            feature_column: Column name for the demographic feature\n",
    "            feature_name: Display name for the feature (optional)\n",
    "        \"\"\"\n",
    "        \n",
    "        if feature_name is None:\n",
    "            feature_name = feature_column.title()\n",
    "            \n",
    "        print(f\"\\nüîç Analyzing {feature_name} Fairness\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load experiment results\n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading results: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Group patients by feature\n",
    "        feature_groups = {}\n",
    "        group_performance = {}\n",
    "        \n",
    "        for patient_id, patient_results in results.items():\n",
    "            # Find demographic info for this patient\n",
    "            patient_demo = demographics_df[demographics_df['patient_id'] == patient_id]\n",
    "            \n",
    "            if len(patient_demo) == 0:\n",
    "                continue\n",
    "                \n",
    "            feature_value = patient_demo[feature_column].iloc[0]\n",
    "            \n",
    "            if feature_value not in feature_groups:\n",
    "                feature_groups[feature_value] = []\n",
    "                \n",
    "            feature_groups[feature_value].append({\n",
    "                'patient_id': patient_id,\n",
    "                'mse': patient_results.get('mse', 0),\n",
    "                'mae': patient_results.get('mae', 0)\n",
    "            })\n",
    "        \n",
    "        # Calculate performance for each group\n",
    "        for group_value, patients in feature_groups.items():\n",
    "            group_mse = np.mean([p['mse'] for p in patients])\n",
    "            group_mae = np.mean([p['mae'] for p in patients])\n",
    "            group_count = len(patients)\n",
    "            \n",
    "            group_performance[group_value] = {\n",
    "                'count': group_count,\n",
    "                'mse': group_mse,\n",
    "                'mae': group_mae,\n",
    "                'patients': patients\n",
    "            }\n",
    "            \n",
    "            print(f\"üè∑Ô∏è  {group_value}: {group_count} patients, MSE = {group_mse:.6f}\")\n",
    "        \n",
    "        # Calculate fairness metrics\n",
    "        mse_values = [perf['mse'] for perf in group_performance.values()]\n",
    "        mae_values = [perf['mae'] for perf in group_performance.values()]\n",
    "        \n",
    "        # Fairness ratio (worst/best performance)\n",
    "        mse_ratio = max(mse_values) / min(mse_values) if min(mse_values) > 0 else float('inf')\n",
    "        mae_ratio = max(mae_values) / min(mae_values) if min(mae_values) > 0 else float('inf')\n",
    "        \n",
    "        # Coefficient of variation (std/mean)\n",
    "        mse_cv = np.std(mse_values) / np.mean(mse_values) if np.mean(mse_values) > 0 else float('inf')\n",
    "        mae_cv = np.std(mae_values) / np.mean(mae_values) if np.mean(mae_values) > 0 else float('inf')\n",
    "        \n",
    "        # Fairness level classification\n",
    "        fairness_level = self.classify_fairness_level(mse_ratio)\n",
    "        \n",
    "        print(f\"\\nüìä {feature_name} Fairness Metrics:\")\n",
    "        print(f\"‚öñÔ∏è  MSE Fairness Ratio: {mse_ratio:.3f}\")\n",
    "        print(f\"‚öñÔ∏è  MAE Fairness Ratio: {mae_ratio:.3f}\")\n",
    "        print(f\"üìà MSE Coefficient of Variation: {mse_cv:.3f}\")\n",
    "        print(f\"üìà MAE Coefficient of Variation: {mae_cv:.3f}\")\n",
    "        print(f\"üéØ Fairness Level: {fairness_level}\")\n",
    "        \n",
    "        return {\n",
    "            'feature_name': feature_name,\n",
    "            'group_performance': group_performance,\n",
    "            'mse_ratio': mse_ratio,\n",
    "            'mae_ratio': mae_ratio,\n",
    "            'mse_cv': mse_cv,\n",
    "            'mae_cv': mae_cv,\n",
    "            'fairness_level': fairness_level\n",
    "        }\n",
    "    \n",
    "    def classify_fairness_level(self, ratio):\n",
    "        \"\"\"Classify fairness level based on ratio\"\"\"\n",
    "        if ratio <= 1.10:\n",
    "            return \"Excellent\"\n",
    "        elif ratio <= 1.25:\n",
    "            return \"Good\" \n",
    "        elif ratio <= 1.50:\n",
    "            return \"Acceptable\"\n",
    "        else:\n",
    "            return \"Poor\"\n",
    "    \n",
    "    def analyze_all_features(self, results_path, demographics_df, features_to_analyze):\n",
    "        \"\"\"\n",
    "        Analyze fairness across multiple demographic features\n",
    "        \n",
    "        Args:\n",
    "            results_path: Path to experiment results\n",
    "            demographics_df: DataFrame with patient demographics  \n",
    "            features_to_analyze: List of column names to analyze\n",
    "        \"\"\"\n",
    "        \n",
    "        fairness_summary = {}\n",
    "        \n",
    "        print(\"üîç Multi-Feature Fairness Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for feature_col in features_to_analyze:\n",
    "            if feature_col in demographics_df.columns:\n",
    "                result = self.analyze_feature_fairness(\n",
    "                    results_path, demographics_df, feature_col\n",
    "                )\n",
    "                if result:\n",
    "                    fairness_summary[feature_col] = result\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Feature '{feature_col}' not found in demographics\")\n",
    "        \n",
    "        # Summary comparison\n",
    "        print(f\"\\nüìã Fairness Summary Across All Features:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for feature, metrics in fairness_summary.items():\n",
    "            print(f\"{metrics['feature_name']:15} | Ratio: {metrics['mse_ratio']:5.2f} | Level: {metrics['fairness_level']}\")\n",
    "        \n",
    "        # Find most/least fair features\n",
    "        if fairness_summary:\n",
    "            most_fair = min(fairness_summary.items(), key=lambda x: x[1]['mse_ratio'])\n",
    "            least_fair = max(fairness_summary.items(), key=lambda x: x[1]['mse_ratio'])\n",
    "            \n",
    "            print(f\"\\nüèÜ Most Fair Feature: {most_fair[1]['feature_name']} (ratio: {most_fair[1]['mse_ratio']:.3f})\")\n",
    "            print(f\"‚ö†Ô∏è  Least Fair Feature: {least_fair[1]['feature_name']} (ratio: {least_fair[1]['mse_ratio']:.3f})\")\n",
    "        \n",
    "        return fairness_summary\n",
    "\n",
    "# Initialize the multi-feature analyzer\n",
    "multi_analyzer = MultiFairnessAnalyzer()\n",
    "\n",
    "print(\"‚úÖ Multi-Feature Fairness Analyzer Ready!\")\n",
    "print(\"üéØ Supported features:\")\n",
    "for feature, mapping in multi_analyzer.supported_features.items():\n",
    "    print(f\"   ‚Ä¢ {feature}: {list(mapping.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c7573",
   "metadata": {},
   "source": [
    "### Example: Analyze Age Group Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a sample demographics file with multiple features\n",
    "sample_demographics = pd.DataFrame({\n",
    "    'patient_id': [f'patient_{i:03d}' for i in range(100)],\n",
    "    'gender': np.random.choice(['male', 'female'], 100),\n",
    "    'age': np.random.randint(18, 80, 100),\n",
    "    'race': np.random.choice(['white', 'black', 'hispanic', 'asian'], 100),\n",
    "    'bmi': np.random.normal(25, 5, 100),\n",
    "    'disease_severity': np.random.choice(['mild', 'moderate', 'severe'], 100)\n",
    "})\n",
    "\n",
    "# Create age groups\n",
    "def categorize_age(age):\n",
    "    if age < 30:\n",
    "        return 'young'\n",
    "    elif age < 60:\n",
    "        return 'middle'\n",
    "    else:\n",
    "        return 'old'\n",
    "\n",
    "sample_demographics['age_group'] = sample_demographics['age'].apply(categorize_age)\n",
    "\n",
    "# Create BMI categories\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 'underweight'\n",
    "    elif bmi < 25:\n",
    "        return 'normal'\n",
    "    elif bmi < 30:\n",
    "        return 'overweight'\n",
    "    else:\n",
    "        return 'obese'\n",
    "\n",
    "sample_demographics['bmi_category'] = sample_demographics['bmi'].apply(categorize_bmi)\n",
    "\n",
    "print(\"üìä Sample Demographics Created:\")\n",
    "print(f\"   ‚Ä¢ Age groups: {sample_demographics['age_group'].value_counts().to_dict()}\")\n",
    "print(f\"   ‚Ä¢ Race distribution: {sample_demographics['race'].value_counts().to_dict()}\")\n",
    "print(f\"   ‚Ä¢ BMI categories: {sample_demographics['bmi_category'].value_counts().to_dict()}\")\n",
    "\n",
    "# Save sample demographics (you can replace this with your actual data)\n",
    "# sample_demographics.to_csv('/workspace/LLM-TIME/fairness/sample_demographics.csv', index=False)\n",
    "\n",
    "print(\"üíæ Sample saved to sample_demographics.csv (uncomment to save)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ea1db",
   "metadata": {},
   "source": [
    "### Multi-Feature Fairness Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1725dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFeatureFairnessLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Fairness losses that work across multiple demographic features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_to_consider=['gender', 'age_group', 'race']):\n",
    "        super().__init__()\n",
    "        self.features = features_to_consider\n",
    "    \n",
    "    def multi_feature_parity_loss(self, predictions, demographic_features, lambda_fairness=0.1):\n",
    "        \"\"\"\n",
    "        Enforce fairness across multiple demographic features simultaneously\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions\n",
    "            demographic_features: Dict with feature names as keys, labels as values\n",
    "            lambda_fairness: Weight for fairness constraint\n",
    "        \"\"\"\n",
    "        total_fairness_loss = 0.0\n",
    "        \n",
    "        for feature_name in self.features:\n",
    "            if feature_name in demographic_features:\n",
    "                feature_labels = demographic_features[feature_name]\n",
    "                \n",
    "                # Get unique groups for this feature\n",
    "                unique_groups = torch.unique(feature_labels)\n",
    "                \n",
    "                if len(unique_groups) <= 1:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate mean prediction for each group\n",
    "                group_means = []\n",
    "                for group in unique_groups:\n",
    "                    group_mask = (feature_labels == group)\n",
    "                    if group_mask.sum() > 0:\n",
    "                        group_mean = predictions[group_mask].mean()\n",
    "                        group_means.append(group_mean)\n",
    "                \n",
    "                if len(group_means) > 1:\n",
    "                    # Penalize variance across groups for this feature\n",
    "                    group_means = torch.stack(group_means)\n",
    "                    feature_fairness_loss = group_means.var()\n",
    "                    total_fairness_loss += feature_fairness_loss\n",
    "        \n",
    "        return lambda_fairness * total_fairness_loss\n",
    "    \n",
    "    def intersectional_fairness_loss(self, predictions, demographic_features, lambda_fairness=0.1):\n",
    "        \"\"\"\n",
    "        Consider intersectional fairness (e.g., young Black women vs old white men)\n",
    "        \"\"\"\n",
    "        if len(demographic_features) < 2:\n",
    "            return torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Create intersectional groups by combining features\n",
    "        feature_names = list(demographic_features.keys())[:2]  # Use first 2 features\n",
    "        \n",
    "        feature1_labels = demographic_features[feature_names[0]]\n",
    "        feature2_labels = demographic_features[feature_names[1]]\n",
    "        \n",
    "        # Combine features to create intersectional groups\n",
    "        unique_groups = []\n",
    "        group_means = []\n",
    "        \n",
    "        for val1 in torch.unique(feature1_labels):\n",
    "            for val2 in torch.unique(feature2_labels):\n",
    "                intersect_mask = (feature1_labels == val1) & (feature2_labels == val2)\n",
    "                \n",
    "                if intersect_mask.sum() > 0:  # Only if this intersection exists\n",
    "                    group_mean = predictions[intersect_mask].mean()\n",
    "                    group_means.append(group_mean)\n",
    "                    unique_groups.append((val1.item(), val2.item()))\n",
    "        \n",
    "        if len(group_means) > 1:\n",
    "            group_means = torch.stack(group_means)\n",
    "            intersectional_loss = group_means.var()\n",
    "            return lambda_fairness * intersectional_loss\n",
    "        \n",
    "        return torch.tensor(0.0, device=predictions.device)\n",
    "\n",
    "def multi_feature_training_step(model, batch, optimizer, fairness_loss_fn, lambda_fairness=0.1):\n",
    "    \"\"\"\n",
    "    Training step with multi-feature fairness constraints\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch data\n",
    "    inputs = batch['inputs']\n",
    "    targets = batch['targets']\n",
    "    \n",
    "    # Extract all demographic features\n",
    "    demographic_features = {}\n",
    "    for key, value in batch.items():\n",
    "        if key not in ['inputs', 'targets', 'patient_id']:\n",
    "            demographic_features[key] = value\n",
    "    \n",
    "    # Forward pass\n",
    "    model.train()\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # Primary loss\n",
    "    primary_loss = F.mse_loss(predictions, targets)\n",
    "    \n",
    "    # Multi-feature fairness losses\n",
    "    parity_loss = fairness_loss_fn.multi_feature_parity_loss(predictions, demographic_features, lambda_fairness)\n",
    "    intersectional_loss = fairness_loss_fn.intersectional_fairness_loss(predictions, demographic_features, lambda_fairness)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = primary_loss + parity_loss + intersectional_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'primary_loss': primary_loss.item(),\n",
    "        'multi_feature_parity_loss': parity_loss.item(),\n",
    "        'intersectional_fairness_loss': intersectional_loss.item()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Multi-Feature Fairness Training System Ready!\")\n",
    "print(\"üéØ Capabilities:\")\n",
    "print(\"   ‚Ä¢ Simultaneous fairness across multiple demographics\")\n",
    "print(\"   ‚Ä¢ Intersectional fairness (e.g., age + gender combinations)\")\n",
    "print(\"   ‚Ä¢ Flexible feature selection\")\n",
    "print(\"   ‚Ä¢ Comprehensive loss tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eacab87",
   "metadata": {},
   "source": [
    "### Usage Examples for Multi-Feature Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15891f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Analyze fairness across multiple features\n",
    "print(\"üîç Example 1: Multi-Feature Fairness Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load your actual demographics (replace with real path)\n",
    "# demographics_df = multi_analyzer.load_patient_demographics('/path/to/demographics.csv')\n",
    "\n",
    "# For demo, use sample data\n",
    "demographics_df = sample_demographics\n",
    "\n",
    "# Analyze specific features\n",
    "features_to_check = ['age_group', 'race', 'bmi_category', 'disease_severity']\n",
    "\n",
    "# Example: If you have experiment results\n",
    "# results_path = '/workspace/LLM-TIME/distillation_experiments/latest/results.json'\n",
    "# fairness_results = multi_analyzer.analyze_all_features(results_path, demographics_df, features_to_check)\n",
    "\n",
    "print(\"üìã To use with your real data:\")\n",
    "print(\"1. Load demographics: demographics_df = multi_analyzer.load_patient_demographics('your_file.csv')\")\n",
    "print(\"2. Analyze features: fairness_results = multi_analyzer.analyze_all_features(results_path, demographics_df, features_list)\")\n",
    "\n",
    "# Example 2: Multi-feature dataset\n",
    "print(\"\\nüîç Example 2: Multi-Feature Dataset\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class MultiFeatureDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that includes multiple demographic features\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, demographics_df):\n",
    "        self.data = self.load_time_series_data(data_path)\n",
    "        self.demographics = demographics_df\n",
    "        \n",
    "        # Create mappings for categorical features\n",
    "        self.feature_mappings = {\n",
    "            'gender': {'male': 1, 'female': 0},\n",
    "            'age_group': {'young': 0, 'middle': 1, 'old': 2},\n",
    "            'race': {'white': 0, 'black': 1, 'hispanic': 2, 'asian': 3},\n",
    "            'bmi_category': {'underweight': 0, 'normal': 1, 'overweight': 2, 'obese': 3},\n",
    "            'disease_severity': {'mild': 0, 'moderate': 1, 'severe': 2}\n",
    "        }\n",
    "    \n",
    "    def load_time_series_data(self, data_path):\n",
    "        # Placeholder - replace with your data loading\n",
    "        return torch.randn(100, 100, 1)  # 100 samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data[idx]\n",
    "        targets = torch.randn(1)  # Placeholder target\n",
    "        \n",
    "        # Get patient demographics\n",
    "        patient_demo = self.demographics.iloc[idx % len(self.demographics)]\n",
    "        \n",
    "        # Convert categorical features to numeric\n",
    "        batch_item = {\n",
    "            'inputs': inputs,\n",
    "            'targets': targets,\n",
    "            'patient_id': patient_demo['patient_id']\n",
    "        }\n",
    "        \n",
    "        # Add all demographic features\n",
    "        for feature, mapping in self.feature_mappings.items():\n",
    "            if feature in patient_demo:\n",
    "                feature_value = patient_demo[feature]\n",
    "                if feature_value in mapping:\n",
    "                    batch_item[feature] = torch.tensor(mapping[feature_value], dtype=torch.float32)\n",
    "                else:\n",
    "                    batch_item[feature] = torch.tensor(0, dtype=torch.float32)  # Default\n",
    "        \n",
    "        return batch_item\n",
    "\n",
    "# Create multi-feature data loader\n",
    "# dataset = MultiFeatureDataset('path/to/data', demographics_df)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"üìã Multi-feature dataset ready! Includes:\")\n",
    "for feature in ['gender', 'age_group', 'race', 'bmi_category', 'disease_severity']:\n",
    "    print(f\"   ‚Ä¢ {feature}\")\n",
    "\n",
    "# Example 3: Training with multiple fairness constraints\n",
    "print(\"\\nüîç Example 3: Multi-Feature Training\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize multi-feature fairness loss\n",
    "# fairness_loss_fn = MultiFeatureFairnessLoss(features_to_consider=['gender', 'age_group', 'race'])\n",
    "\n",
    "# Training loop example\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         loss_dict = multi_feature_training_step(\n",
    "#             model=model,\n",
    "#             batch=batch,\n",
    "#             optimizer=optimizer,\n",
    "#             fairness_loss_fn=fairness_loss_fn,\n",
    "#             lambda_fairness=0.1\n",
    "#         )\n",
    "\n",
    "print(\"üéØ Training tracks fairness across:\")\n",
    "print(\"   ‚Ä¢ Individual features (age, race, etc.)\")\n",
    "print(\"   ‚Ä¢ Intersectional combinations (young + Black, old + female, etc.)\")\n",
    "print(\"   ‚Ä¢ Overall fairness across all groups\")\n",
    "\n",
    "print(\"\\n‚úÖ Multi-Feature Fairness System Complete!\")\n",
    "print(\"üöÄ You can now analyze and enforce fairness across any demographic features!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
