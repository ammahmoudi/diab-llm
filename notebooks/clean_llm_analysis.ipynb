{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1833ff2b",
   "metadata": {},
   "source": [
    "# üöÄ Clean LLM Efficiency Analysis\n",
    "\n",
    "**Simple, function-based notebook that actually works!**\n",
    "\n",
    "No more messy code scattered across 79 cells. Just clean functions that do what they're supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a26a2d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All working modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import working modules from ultimate notebook\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from enhanced_data_loader import EnhancedEfficiencyDataLoader\n",
    "from analysis_utils import create_inference_plots, calculate_energy_metrics, calculate_inference_summary, create_efficiency_table, generate_efficiency_report\n",
    "from training_analysis import TrainingAnalyzer\n",
    "from latex_table_generator import generate_all_tables, create_real_data_latex_table, create_corrected_latex_table\n",
    "\n",
    "print(\"‚úÖ All working modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500a35f",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Load Data\n",
    "\n",
    "First, load the experimental data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01cc6260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for experiment files...\n",
      "üìä Found 53 JSON files\n",
      "üìä Processing 24 efficiency_reports...\n",
      "üìä Processing 11 comprehensive_reports...\n",
      "üìä Processing 18 real_performance_reports...\n",
      "‚úÖ Loaded 51 total records\n",
      "Loaded 51 inference records\n"
     ]
    }
   ],
   "source": [
    "# Load actual data using the working loader\n",
    "BASE_PATH = Path.cwd().parent\n",
    "loader = EnhancedEfficiencyDataLoader(BASE_PATH)\n",
    "inference_data = loader.parse_all_data()\n",
    "print(f\"Loaded {len(inference_data)} inference records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2960f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU/VRAM/Memory columns: ['inference_peak_gpu_mb', 'current_vram_usage_mb', 'gpu_memory_reserved_mb', 'estimated_gpu_latency_ms', 'estimated_memory_usage_mb', 'average_gpu_allocated_mb', 'peak_gpu_reserved_mb', 'peak_gpu_utilization_percent', 'average_gpu_utilization_percent']\n",
      "\n",
      "All metric columns: ['total_parameters', 'model_size_mb', 'avg_inference_time_ms', 'min_inference_time_ms', 'max_inference_time_ms', 'inference_peak_ram_mb', 'inference_peak_gpu_mb', 'current_vram_usage_mb', 'gpu_memory_reserved_mb', 'estimated_cpu_latency_ms', 'estimated_gpu_latency_ms', 'estimated_memory_usage_mb', 'estimated_throughput_samples_per_sec', 'throughput_predictions_per_sec', 'median_latency_ms', 'p95_latency_ms', 'process_average_ram_mb', 'system_peak_ram_mb', 'average_gpu_allocated_mb', 'peak_gpu_reserved_mb', 'peak_gpu_utilization_percent', 'average_gpu_utilization_percent', 'inference_avg_power_w', 'peak_power_usage_watts']\n",
      "\n",
      "Dataset shape: (51, 36)\n",
      "\n",
      "Unique models: ['chronos-t5-base' 'chronos-t5-tiny' 'LLAMA' 'GPT2' 'BERT' 'tinybert']\n",
      "\n",
      "Sample data with key metrics:\n",
      "        model_name  total_parameters  model_size_mb  avg_inference_time_ms  \\\n",
      "0  chronos-t5-base         201374976     768.184570              84.120291   \n",
      "1  chronos-t5-base         201374976     768.184570              84.120291   \n",
      "2  chronos-t5-tiny           8394496      32.022461              37.162438   \n",
      "\n",
      "   min_inference_time_ms  max_inference_time_ms  inference_peak_ram_mb  \\\n",
      "0              83.998960              84.281159             843.792969   \n",
      "1              83.998960              84.281159             843.792969   \n",
      "2              36.951937              37.532820             842.531250   \n",
      "\n",
      "   inference_peak_gpu_mb  current_vram_usage_mb  gpu_memory_reserved_mb  \\\n",
      "0             768.184570           11804.035156                   854.0   \n",
      "1             768.184570           11804.035156                   854.0   \n",
      "2              32.022461            9583.097656                    50.0   \n",
      "\n",
      "   estimated_cpu_latency_ms  \n",
      "0                    201.75  \n",
      "1                    201.75  \n",
      "2                      8.87  \n"
     ]
    }
   ],
   "source": [
    "# Check for GPU/VRAM related columns\n",
    "gpu_cols = [col for col in inference_data.columns if 'gpu' in col.lower() or 'vram' in col.lower() or 'memory' in col.lower()]\n",
    "print(\"GPU/VRAM/Memory columns:\", gpu_cols)\n",
    "\n",
    "# Check for all available metric columns\n",
    "metric_cols = [col for col in inference_data.columns if any(x in col.lower() for x in ['time', 'latency', 'memory', 'ram', 'gpu', 'power', 'throughput', 'size'])]\n",
    "print(\"\\nAll metric columns:\", metric_cols)\n",
    "\n",
    "# Show unique values for key columns\n",
    "print(f\"\\nDataset shape: {inference_data.shape}\")\n",
    "if 'model_name' in inference_data.columns:\n",
    "    print(f\"\\nUnique models: {inference_data['model_name'].unique()}\")\n",
    "if 'dataset_name' in inference_data.columns:\n",
    "    print(f\"\\nUnique datasets: {inference_data['dataset_name'].unique()}\")\n",
    "\n",
    "# Show sample of key metrics\n",
    "key_cols = ['model_name', 'dataset_name'] + metric_cols[:10]  # Show first 10 metric columns\n",
    "sample_cols = [col for col in key_cols if col in inference_data.columns]\n",
    "print(f\"\\nSample data with key metrics:\")\n",
    "print(inference_data[sample_cols].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91683c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in inference_metrics:\n",
      "['model_name', 'total_records', 'records_with_timing', 'records_with_memory', 'records_with_power', 'avg_inference_time_ms', 'min_inference_time_ms', 'max_inference_time_ms', 'std_inference_time_ms', 'throughput_predictions_per_sec', 'inference_peak_ram_mb', 'min_ram_mb', 'max_ram_mb', 'inference_avg_power_w', 'min_power_w', 'max_power_w', 'total_parameters', 'model_size_mb', 'model_architecture', 'edge_feasibility']\n",
      "\n",
      "GPU/VRAM columns with actual data:\n",
      "\n",
      "Sample row for debugging:\n",
      "Model: Chronos-T5-Base\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check if GPU/VRAM data exists in the aggregated summary\n",
    "from analysis_utils import calculate_inference_summary\n",
    "inference_metrics = calculate_inference_summary(inference_data)\n",
    "print(\"Columns in inference_metrics:\")\n",
    "print(inference_metrics.columns.tolist())\n",
    "\n",
    "print(\"\\nGPU/VRAM columns with actual data:\")\n",
    "gpu_cols = [col for col in inference_metrics.columns if 'gpu' in col.lower() or 'vram' in col.lower()]\n",
    "for col in gpu_cols:\n",
    "    non_null_count = inference_metrics[col].notna().sum()\n",
    "    print(f\"{col}: {non_null_count} non-null values out of {len(inference_metrics)}\")\n",
    "    if non_null_count > 0:\n",
    "        print(f\"  Sample values: {inference_metrics[col].dropna().head(3).tolist()}\")\n",
    "\n",
    "print(\"\\nSample row for debugging:\")\n",
    "sample_row = inference_metrics.iloc[0]\n",
    "print(\"Model:\", sample_row.get('model_name', 'N/A'))\n",
    "for col in gpu_cols:\n",
    "    print(f\"{col}: {sample_row.get(col, 'Missing column')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7505e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reloaded both modules\n"
     ]
    }
   ],
   "source": [
    "# Force reload modules to get GPU/VRAM metrics\n",
    "import importlib\n",
    "import analysis_utils\n",
    "import latex_table_generator\n",
    "\n",
    "# Reload both modules\n",
    "importlib.reload(analysis_utils)\n",
    "importlib.reload(latex_table_generator)\n",
    "\n",
    "from analysis_utils import calculate_inference_summary, standardize_model_names\n",
    "from latex_table_generator import generate_all_tables\n",
    "\n",
    "print(\"üîÑ Reloaded both modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7229c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated columns in inference_metrics:\n",
      "['model_name', 'total_records', 'records_with_timing', 'records_with_memory', 'records_with_power', 'avg_inference_time_ms', 'min_inference_time_ms', 'max_inference_time_ms', 'std_inference_time_ms', 'throughput_predictions_per_sec', 'inference_peak_ram_mb', 'min_ram_mb', 'max_ram_mb', 'inference_avg_power_w', 'min_power_w', 'max_power_w', 'inference_peak_gpu_mb', 'current_vram_usage_mb', 'gpu_memory_reserved_mb', 'estimated_cpu_latency_ms', 'estimated_gpu_latency_ms', 'peak_power_usage_watts', 'peak_gpu_utilization_percent', 'average_gpu_utilization_percent', 'total_parameters', 'model_size_mb', 'model_architecture', 'edge_feasibility']\n",
      "\n",
      "GPU/VRAM columns with actual data:\n",
      "inference_peak_gpu_mb: 6 non-null values out of 6\n",
      "  Sample values: [783.49453125, 36.0373046875, 10931.703776041666]\n",
      "current_vram_usage_mb: 6 non-null values out of 6\n",
      "  Sample values: [12590.79296875, 10086.69921875, 33778.251953125]\n",
      "gpu_memory_reserved_mb: 6 non-null values out of 6\n",
      "  Sample values: [854.0, 50.0, 0.0]\n",
      "estimated_gpu_latency_ms: 6 non-null values out of 6\n",
      "  Sample values: [20.14, 0.84, 664.25]\n",
      "peak_gpu_utilization_percent: 6 non-null values out of 6\n",
      "  Sample values: [23.0, 13.0, 79.5]\n",
      "average_gpu_utilization_percent: 6 non-null values out of 6\n",
      "  Sample values: [2.7027027027027026, 0.8059701492537313, 45.554722141777766]\n"
     ]
    }
   ],
   "source": [
    "# Now test the updated function with GPU/VRAM metrics\n",
    "inference_metrics_updated = calculate_inference_summary(inference_data)\n",
    "print(\"Updated columns in inference_metrics:\")\n",
    "print(inference_metrics_updated.columns.tolist())\n",
    "\n",
    "print(\"\\nGPU/VRAM columns with actual data:\")\n",
    "gpu_cols = [col for col in inference_metrics_updated.columns if 'gpu' in col.lower() or 'vram' in col.lower()]\n",
    "for col in gpu_cols:\n",
    "    non_null_count = inference_metrics_updated[col].notna().sum()\n",
    "    print(f\"{col}: {non_null_count} non-null values out of {len(inference_metrics_updated)}\")\n",
    "    if non_null_count > 0:\n",
    "        print(f\"  Sample values: {inference_metrics_updated[col].dropna().head(3).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bff37a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating comprehensive table with GPU/VRAM data...\n",
      "üìä Generating comprehensive standardized table...\n",
      "‚úÖ Table generated: /home/amma/LLM-TIME/notebooks/outputs/latex_tables/comprehensive_standardized_metrics.tex\n",
      "‚úÖ Generated comprehensive table with GPU/VRAM metrics!\n",
      "  - comprehensive_standardized: /home/amma/LLM-TIME/notebooks/outputs/latex_tables/comprehensive_standardized_metrics.tex\n",
      "\n",
      "üìÑ Table preview:\n",
      "    \\begin{tabular}{@{}l|cc|cc|cc|c|c@{}}\n",
      "        \\toprule\n",
      "        & \\multicolumn{2}{c|}{\\textbf{Latency (ms)}} & \\multicolumn{2}{c|}{\\textbf{Memory (MB)}} & \\multicolumn{2}{c|}{\\textbf{Performance}} & \\textbf{Model} & \\textbf{Edge} \\\\\n",
      "        \\textbf{Model} & \\textbf{CPU} & \\textbf{GPU} & \\textbf{RAM} & \\textbf{VRAM} & \\textbf{Power (W)} & \\textbf{Thru. (p/s)} & \\textbf{Size (MB)} & \\textbf{Ready} \\\\\n",
      "        \\midrule\n",
      "        Chronos-T5-Base & 182.0 & 20.1 & 1001 & 12591 & 142.0 & 0.64 & 768 & \\textcolor{red}{No} \\\\\n",
      "        Chronos-T5-Tiny & 8.9 & 0.8 & 993 & 10087 & 107.5 & 0.72 & 32 & \\textcolor{orange}{Partial} \\\\\n",
      "        Time-LLM-BERT & 264.6 & 28.2 & 2005 & 9349 & 201.4 & 4.8e-03 & 1077 & \\textcolor{red}{No} \\\\\n"
     ]
    }
   ],
   "source": [
    "# Now generate the comprehensive table with actual GPU/VRAM data\n",
    "print(\"üöÄ Generating comprehensive table with GPU/VRAM data...\")\n",
    "\n",
    "# Clean up old outputs\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "output_path = Path.cwd() / \"outputs\" / \"latex_tables\"\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "# Generate the updated table\n",
    "table_results = generate_all_tables(inference_data, output_dir=output_path)\n",
    "\n",
    "print(\"‚úÖ Generated comprehensive table with GPU/VRAM metrics!\")\n",
    "for name, path in table_results.items():\n",
    "    print(f\"  - {name}: {path}\")\n",
    "\n",
    "# Show a preview of the new table\n",
    "if table_results:\n",
    "    first_table_path = list(table_results.values())[0]\n",
    "    print(f\"\\nüìÑ Table preview:\")\n",
    "    with open(first_table_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        # Show the header and first few data rows\n",
    "        lines = content.split('\\n')\n",
    "        preview_lines = []\n",
    "        in_table = False\n",
    "        data_rows = 0\n",
    "        for line in lines:\n",
    "            if '\\\\begin{tabular}' in line:\n",
    "                in_table = True\n",
    "            if in_table:\n",
    "                preview_lines.append(line)\n",
    "                if '&' in line and '\\\\\\\\' in line and not 'textbf' in line:  # Data row\n",
    "                    data_rows += 1\n",
    "                    if data_rows >= 3:  # Show first 3 data rows\n",
    "                        break\n",
    "        print('\\n'.join(preview_lines[:20]))  # Show first 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89c83d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ANALYZING DATA SOURCES FOR GPU/VRAM/RAM METRICS\n",
      "============================================================\n",
      "Sample from model: chronos-t5-base\n",
      "\n",
      "üíæ MEMORY METRICS - DATA SOURCES:\n",
      "----------------------------------------\n",
      "üìç RAM (inference_peak_ram_mb): 843.8 MB\n",
      "   ‚îî‚îÄ Source: process_peak_ram_mb (ML model process only)\n",
      "üìç System RAM (system_peak_ram_mb): nan MB\n",
      "   ‚îî‚îÄ Source: system_peak_ram_mb (includes all processes)\n",
      "\n",
      "üéÆ GPU/VRAM METRICS - DATA SOURCES:\n",
      "----------------------------------------\n",
      "üìç VRAM Usage (current_vram_usage_mb): 11804.0 MB\n",
      "   ‚îî‚îÄ Source: nvidia-ml-py (NVIDIA ML API)\n",
      "üìç GPU Allocated (inference_peak_gpu_mb): 768.2 MB\n",
      "   ‚îî‚îÄ Source: torch.cuda.max_memory_allocated() (PyTorch)\n",
      "üìç GPU Reserved (gpu_memory_reserved_mb): 854.0 MB\n",
      "   ‚îî‚îÄ Source: torch.cuda.max_memory_reserved() (PyTorch)\n",
      "\n",
      "‚ö° PERFORMANCE METRICS - DATA SOURCES:\n",
      "----------------------------------------\n",
      "üìç Peak Power: nan W\n",
      "   ‚îî‚îÄ Source: nvidia-ml-py (NVIDIA ML API)\n",
      "üìç GPU Utilization: nan%\n",
      "   ‚îî‚îÄ Source: nvidia-ml-py (NVIDIA ML API)\n",
      "üìç CPU Latency: 201.8 ms\n",
      "   ‚îî‚îÄ Source: theoretical_performance (calculated)\n",
      "üìç GPU Latency: 20.1 ms\n",
      "   ‚îî‚îÄ Source: theoretical_performance (calculated)\n",
      "\n",
      "üè∑Ô∏è  DATA SOURCE PRIORITY IN TABLE:\n",
      "----------------------------------------\n",
      "RAM: process_peak_ram_mb (process-specific, not system-wide)\n",
      "VRAM: current_vram_usage_mb (nvidia-ml-py, hardware-level)\n",
      "GPU Alloc: inference_peak_gpu_mb (PyTorch memory tracking)\n",
      "Power: peak_power_usage_watts (nvidia-ml-py, hardware sensors)\n",
      "CPU Latency: estimated_cpu_latency_ms or avg_inference_time_ms\n",
      "GPU Latency: estimated_gpu_latency_ms (theoretical calculation)\n"
     ]
    }
   ],
   "source": [
    "# üìä DATA SOURCE ANALYSIS FOR GPU/VRAM/RAM METRICS\n",
    "print(\"üîç ANALYZING DATA SOURCES FOR GPU/VRAM/RAM METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what specific data sources are being used\n",
    "sample_data = inference_data.iloc[0]\n",
    "print(f\"Sample from model: {sample_data['model_name']}\")\n",
    "print()\n",
    "\n",
    "print(\"üíæ MEMORY METRICS - DATA SOURCES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# RAM Sources\n",
    "if 'inference_peak_ram_mb' in sample_data:\n",
    "    print(f\"üìç RAM (inference_peak_ram_mb): {sample_data['inference_peak_ram_mb']:.1f} MB\")\n",
    "    print(\"   ‚îî‚îÄ Source: process_peak_ram_mb (ML model process only)\")\n",
    "    \n",
    "if 'system_peak_ram_mb' in sample_data:\n",
    "    print(f\"üìç System RAM (system_peak_ram_mb): {sample_data['system_peak_ram_mb']:.1f} MB\") \n",
    "    print(\"   ‚îî‚îÄ Source: system_peak_ram_mb (includes all processes)\")\n",
    "\n",
    "print()\n",
    "print(\"üéÆ GPU/VRAM METRICS - DATA SOURCES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# VRAM Sources\n",
    "if 'current_vram_usage_mb' in sample_data:\n",
    "    print(f\"üìç VRAM Usage (current_vram_usage_mb): {sample_data['current_vram_usage_mb']:.1f} MB\")\n",
    "    print(\"   ‚îî‚îÄ Source: nvidia-ml-py (NVIDIA ML API)\")\n",
    "    \n",
    "if 'inference_peak_gpu_mb' in sample_data:\n",
    "    print(f\"üìç GPU Allocated (inference_peak_gpu_mb): {sample_data['inference_peak_gpu_mb']:.1f} MB\")\n",
    "    print(\"   ‚îî‚îÄ Source: torch.cuda.max_memory_allocated() (PyTorch)\")\n",
    "\n",
    "if 'gpu_memory_reserved_mb' in sample_data:\n",
    "    print(f\"üìç GPU Reserved (gpu_memory_reserved_mb): {sample_data['gpu_memory_reserved_mb']:.1f} MB\")  \n",
    "    print(\"   ‚îî‚îÄ Source: torch.cuda.max_memory_reserved() (PyTorch)\")\n",
    "\n",
    "print()\n",
    "print(\"‚ö° PERFORMANCE METRICS - DATA SOURCES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Power Sources  \n",
    "if 'peak_power_usage_watts' in sample_data:\n",
    "    print(f\"üìç Peak Power: {sample_data['peak_power_usage_watts']:.1f} W\")\n",
    "    print(\"   ‚îî‚îÄ Source: nvidia-ml-py (NVIDIA ML API)\")\n",
    "\n",
    "if 'peak_gpu_utilization_percent' in sample_data:\n",
    "    print(f\"üìç GPU Utilization: {sample_data['peak_gpu_utilization_percent']:.1f}%\")\n",
    "    print(\"   ‚îî‚îÄ Source: nvidia-ml-py (NVIDIA ML API)\")\n",
    "\n",
    "# Latency Sources\n",
    "if 'estimated_cpu_latency_ms' in sample_data:\n",
    "    print(f\"üìç CPU Latency: {sample_data['estimated_cpu_latency_ms']:.1f} ms\")\n",
    "    print(\"   ‚îî‚îÄ Source: theoretical_performance (calculated)\")\n",
    "\n",
    "if 'estimated_gpu_latency_ms' in sample_data:  \n",
    "    print(f\"üìç GPU Latency: {sample_data['estimated_gpu_latency_ms']:.1f} ms\")\n",
    "    print(\"   ‚îî‚îÄ Source: theoretical_performance (calculated)\")\n",
    "\n",
    "print()\n",
    "print(\"üè∑Ô∏è  DATA SOURCE PRIORITY IN TABLE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAM: process_peak_ram_mb (process-specific, not system-wide)\")\n",
    "print(\"VRAM: current_vram_usage_mb (nvidia-ml-py, hardware-level)\")\n",
    "print(\"GPU Alloc: inference_peak_gpu_mb (PyTorch memory tracking)\")\n",
    "print(\"Power: peak_power_usage_watts (nvidia-ml-py, hardware sensors)\")\n",
    "print(\"CPU Latency: estimated_cpu_latency_ms or avg_inference_time_ms\")\n",
    "print(\"GPU Latency: estimated_gpu_latency_ms (theoretical calculation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4fd7b",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Calculate Metrics\n",
    "\n",
    "Calculate performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bf716cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference summary calculated!\n"
     ]
    }
   ],
   "source": [
    "# Calculate inference summary using working function\n",
    "inference_metrics = calculate_inference_summary(inference_data)\n",
    "print(\"Inference summary calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17869789",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Load Additional Data\n",
    "\n",
    "Load training and other data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4972c978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: 10 records\n"
     ]
    }
   ],
   "source": [
    "# Load training data using working analyzer\n",
    "training_analyzer = TrainingAnalyzer(BASE_PATH)\n",
    "training_data = training_analyzer.load_training_efficiency_data()\n",
    "print(f\"Loaded training data: {len(training_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cea8e6",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Generate Plots and Tables\n",
    "\n",
    "Create visualizations and LaTeX tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b97031e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Plotting 6 unique models: ['Chronos-T5-Base', 'Chronos-T5-Tiny', 'Time-LLM-BERT', 'Time-LLM-GPT-2', 'Time-LLM-LLaMA', 'Time-LLM-TinyBERT']\n",
      "üìä Dashboard plot saved to: outputs/clean_inference_plots_dashboard.png\n",
      "üìä Detailed analysis plot saved to: outputs/clean_inference_plots_detailed.png\n"
     ]
    }
   ],
   "source": [
    "# Create inference plots with standardized model names (saves to file only, no display)\n",
    "create_inference_plots(inference_data, save_path=\"outputs/clean_inference_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fea1ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy metrics calculated!\n"
     ]
    }
   ],
   "source": [
    "# Calculate energy metrics using working function (needs inference summary first)\n",
    "energy_metrics = calculate_energy_metrics(inference_metrics)\n",
    "print(\"Energy metrics calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7618abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating comprehensive standardized table...\n",
      "‚úÖ Table generated: /home/amma/LLM-TIME/notebooks/outputs/latex_tables/comprehensive_standardized_metrics.tex\n",
      "‚úÖ LaTeX tables generated!\n",
      "  comprehensive_standardized: /home/amma/LLM-TIME/notebooks/outputs/latex_tables/comprehensive_standardized_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "# Generate LaTeX tables using working functions\n",
    "latex_tables = generate_all_tables(inference_data, training_data)\n",
    "print(\"‚úÖ LaTeX tables generated!\")\n",
    "for name, path in latex_tables.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2df1263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Efficiency table created: (51, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>avg_inference_time_ms</th>\n",
       "      <th>inference_peak_ram_mb</th>\n",
       "      <th>inference_avg_power_w</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>edge_feasibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>84.12</td>\n",
       "      <td>843.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>84.12</td>\n",
       "      <td>843.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>37.16</td>\n",
       "      <td>842.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>37.16</td>\n",
       "      <td>842.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26104.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26104.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2199.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2199.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1968.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1968.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>82.73</td>\n",
       "      <td>870.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>82.73</td>\n",
       "      <td>870.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>35.84</td>\n",
       "      <td>873.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>35.84</td>\n",
       "      <td>873.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>943.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>943.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1925.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1925.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2126.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2126.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26105.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26105.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>7505.92</td>\n",
       "      <td>1577.42</td>\n",
       "      <td>107.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>6757.45</td>\n",
       "      <td>1534.32</td>\n",
       "      <td>99.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>91435.46</td>\n",
       "      <td>26192.66</td>\n",
       "      <td>80.46</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>2425.93</td>\n",
       "      <td>2283.16</td>\n",
       "      <td>72.27</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>2250.55</td>\n",
       "      <td>2052.39</td>\n",
       "      <td>72.53</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>1271.30</td>\n",
       "      <td>1060.69</td>\n",
       "      <td>55.22</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>414187.00</td>\n",
       "      <td>2259.30</td>\n",
       "      <td>255.78</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>621029.61</td>\n",
       "      <td>2372.30</td>\n",
       "      <td>278.12</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>885277.15</td>\n",
       "      <td>26198.37</td>\n",
       "      <td>291.46</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>12669.57</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>604.74</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>538.57</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44998814</td>\n",
       "      <td>85.83</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>538.57</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>604.74</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>12669.57</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  avg_inference_time_ms  inference_peak_ram_mb  \\\n",
       "0     Chronos-T5-Base                  84.12                 843.79   \n",
       "1     Chronos-T5-Base                  84.12                 843.79   \n",
       "2     Chronos-T5-Tiny                  37.16                 842.53   \n",
       "3     Chronos-T5-Tiny                  37.16                 842.53   \n",
       "4      Time-LLM-LLaMA                    NaN               26104.16   \n",
       "5      Time-LLM-LLaMA                    NaN               26104.16   \n",
       "6      Time-LLM-GPT-2                    NaN                2199.79   \n",
       "7      Time-LLM-GPT-2                    NaN                2199.79   \n",
       "8       Time-LLM-BERT                    NaN                1968.51   \n",
       "9       Time-LLM-BERT                    NaN                1968.51   \n",
       "10    Chronos-T5-Base                  82.73                 870.21   \n",
       "11    Chronos-T5-Base                  82.73                 870.21   \n",
       "12    Chronos-T5-Tiny                  35.84                 873.79   \n",
       "13    Chronos-T5-Tiny                  35.84                 873.79   \n",
       "14  Time-LLM-TinyBERT                    NaN                 943.95   \n",
       "15  Time-LLM-TinyBERT                    NaN                 943.95   \n",
       "16      Time-LLM-BERT                    NaN                1967.88   \n",
       "17      Time-LLM-BERT                    NaN                1967.88   \n",
       "18      Time-LLM-BERT                    NaN                1925.90   \n",
       "19      Time-LLM-BERT                    NaN                1925.90   \n",
       "20     Time-LLM-GPT-2                    NaN                2126.16   \n",
       "21     Time-LLM-GPT-2                    NaN                2126.16   \n",
       "22     Time-LLM-LLaMA                    NaN               26105.75   \n",
       "23     Time-LLM-LLaMA                    NaN               26105.75   \n",
       "24    Chronos-T5-Base                7505.92                1577.42   \n",
       "25    Chronos-T5-Tiny                6757.45                1534.32   \n",
       "26     Time-LLM-LLaMA               91435.46               26192.66   \n",
       "27     Time-LLM-GPT-2                2425.93                2283.16   \n",
       "28      Time-LLM-BERT                2250.55                2052.39   \n",
       "29  Time-LLM-TinyBERT                1271.30                1060.69   \n",
       "30      Time-LLM-BERT              414187.00                2259.30   \n",
       "31     Time-LLM-GPT-2              621029.61                2372.30   \n",
       "32     Time-LLM-LLaMA              885277.15               26198.37   \n",
       "33    Chronos-T5-Base                    NaN                    NaN   \n",
       "34    Chronos-T5-Tiny                    NaN                    NaN   \n",
       "35     Time-LLM-LLaMA                    NaN                    NaN   \n",
       "36     Time-LLM-LLaMA                    NaN                    NaN   \n",
       "37     Time-LLM-GPT-2                    NaN                    NaN   \n",
       "38     Time-LLM-GPT-2                    NaN                    NaN   \n",
       "39      Time-LLM-BERT                    NaN                    NaN   \n",
       "40      Time-LLM-BERT                    NaN                    NaN   \n",
       "41    Chronos-T5-Base                    NaN                    NaN   \n",
       "42    Chronos-T5-Tiny                    NaN                    NaN   \n",
       "43  Time-LLM-TinyBERT                    NaN                    NaN   \n",
       "44  Time-LLM-TinyBERT                    NaN                    NaN   \n",
       "45      Time-LLM-BERT                    NaN                    NaN   \n",
       "46      Time-LLM-BERT                    NaN                    NaN   \n",
       "47     Time-LLM-GPT-2                    NaN                    NaN   \n",
       "48     Time-LLM-GPT-2                    NaN                    NaN   \n",
       "49     Time-LLM-LLaMA                    NaN                    NaN   \n",
       "50     Time-LLM-LLaMA                    NaN                    NaN   \n",
       "\n",
       "    inference_avg_power_w  total_parameters  model_size_mb  \\\n",
       "0                     NaN         201374976         768.18   \n",
       "1                     NaN         201374976         768.18   \n",
       "2                     NaN           8394496          32.02   \n",
       "3                     NaN           8394496          32.02   \n",
       "4                     NaN        6642504366       25339.14   \n",
       "5                     NaN        6642504366       25339.14   \n",
       "6                     NaN         317055766        1209.47   \n",
       "7                     NaN         317055766        1209.47   \n",
       "8                     NaN         282363198        1077.13   \n",
       "9                     NaN         282363198        1077.13   \n",
       "10                    NaN         201374976         768.18   \n",
       "11                    NaN         201374976         768.18   \n",
       "12                    NaN           8394496          32.02   \n",
       "13                    NaN           8394496          32.02   \n",
       "14                    NaN          44998814         171.66   \n",
       "15                    NaN          44998814         171.66   \n",
       "16                    NaN         282363198        1077.13   \n",
       "17                    NaN         282363198        1077.13   \n",
       "18                    NaN         282363198        1077.13   \n",
       "19                    NaN         282363198        1077.13   \n",
       "20                    NaN         317055766        1209.47   \n",
       "21                    NaN         317055766        1209.47   \n",
       "22                    NaN        6642504366       25339.14   \n",
       "23                    NaN        6642504366       25339.14   \n",
       "24                 107.95                 0           0.00   \n",
       "25                  99.73                 0           0.00   \n",
       "26                  80.46        6642504366       25339.14   \n",
       "27                  72.27         317055766        1209.47   \n",
       "28                  72.53         282363198        1077.13   \n",
       "29                  55.22          44998814         171.66   \n",
       "30                 255.78         282363198        1077.13   \n",
       "31                 278.12         317055766        1209.47   \n",
       "32                 291.46        6642504366       25339.14   \n",
       "33                    NaN                 0            NaN   \n",
       "34                    NaN                 0            NaN   \n",
       "35                    NaN        6642504366       12669.57   \n",
       "36                    NaN        6642504366       25339.14   \n",
       "37                    NaN         317055766        1209.47   \n",
       "38                    NaN         317055766         604.74   \n",
       "39                    NaN         282363198         538.57   \n",
       "40                    NaN         282363198        1077.13   \n",
       "41                    NaN                 0            NaN   \n",
       "42                    NaN                 0            NaN   \n",
       "43                    NaN          44998814          85.83   \n",
       "44                    NaN          44998814         171.66   \n",
       "45                    NaN         282363198         538.57   \n",
       "46                    NaN         282363198        1077.13   \n",
       "47                    NaN         317055766         604.74   \n",
       "48                    NaN         317055766        1209.47   \n",
       "49                    NaN        6642504366       12669.57   \n",
       "50                    NaN        6642504366       25339.14   \n",
       "\n",
       "         edge_feasibility  \n",
       "0                feasible  \n",
       "1                feasible  \n",
       "2                feasible  \n",
       "3                feasible  \n",
       "4   requires_optimization  \n",
       "5   requires_optimization  \n",
       "6                feasible  \n",
       "7                feasible  \n",
       "8                feasible  \n",
       "9                feasible  \n",
       "10               feasible  \n",
       "11               feasible  \n",
       "12               feasible  \n",
       "13               feasible  \n",
       "14               feasible  \n",
       "15               feasible  \n",
       "16               feasible  \n",
       "17               feasible  \n",
       "18               feasible  \n",
       "19               feasible  \n",
       "20               feasible  \n",
       "21               feasible  \n",
       "22  requires_optimization  \n",
       "23  requires_optimization  \n",
       "24            challenging  \n",
       "25            challenging  \n",
       "26            challenging  \n",
       "27            challenging  \n",
       "28            challenging  \n",
       "29            challenging  \n",
       "30            challenging  \n",
       "31            challenging  \n",
       "32            challenging  \n",
       "33                unknown  \n",
       "34                unknown  \n",
       "35                unknown  \n",
       "36                unknown  \n",
       "37                unknown  \n",
       "38                unknown  \n",
       "39                unknown  \n",
       "40                unknown  \n",
       "41                unknown  \n",
       "42                unknown  \n",
       "43                unknown  \n",
       "44                unknown  \n",
       "45                unknown  \n",
       "46                unknown  \n",
       "47                unknown  \n",
       "48                unknown  \n",
       "49                unknown  \n",
       "50                unknown  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create efficiency comparison table using working function\n",
    "comparison_table = create_efficiency_table(inference_data)\n",
    "print(f\"‚úÖ Efficiency table created: {comparison_table.shape}\")\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17a05b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary report generated!\n",
      "==================================================\n",
      "# üöÄ LLM Performance Analysis Report\n",
      "==================================================\n",
      "\n",
      "## üìä Dataset Overview\n",
      "- **Total Records**: 51\n",
      "- **Unique Models**: 6\n",
      "- **Experiment Types**: 5\n",
      "- **Records with Timing Data**: 17\n",
      "- **Records with Power Data**: 9\n",
      "\n",
      "## ‚ö° Performance Summary\n",
      "- **Fastest Model**: Time-LLM-TinyBERT (1271.3ms)\n",
      "- **Most Memory Efficient**: Time-LLM-TinyBERT (982.9MB)\n",
      "\n",
      "## üì± Edge Deployment Readiness\n",
      "- üü° **Feasible**: 5 models\n",
      "- üî¥ **Challenging**: 1 models\n",
      "\n",
      "---\n",
      "*Report generated by Enhanced LLM Efficiency Analysis Tool*\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive efficiency analysis report\n",
    "summary_report = generate_efficiency_report(inference_data)\n",
    "print(\"‚úÖ Summary report generated!\")\n",
    "print(\"=\" * 50)\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef1fe8",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Generate Reports\n",
    "\n",
    "Create analysis reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcabfaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found distillation results at: /home/amma/LLM-TIME/efficiency_experiments/distillation_experiments/pipeline_results.csv\n",
      "Loaded distillation data: 1 records\n"
     ]
    }
   ],
   "source": [
    "# Load distillation results using working analyzer\n",
    "distillation_data = training_analyzer.load_distillation_results()\n",
    "print(f\"Loaded distillation data: {len(distillation_data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a8bb51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference data loaded: (51, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>mode</th>\n",
       "      <th>report_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>model_dtype</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>avg_inference_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>total_inferences</th>\n",
       "      <th>process_average_ram_mb</th>\n",
       "      <th>system_peak_ram_mb</th>\n",
       "      <th>average_gpu_allocated_mb</th>\n",
       "      <th>peak_gpu_reserved_mb</th>\n",
       "      <th>peak_gpu_utilization_percent</th>\n",
       "      <th>average_gpu_utilization_percent</th>\n",
       "      <th>peak_temperature_celsius</th>\n",
       "      <th>inference_avg_power_w</th>\n",
       "      <th>peak_power_usage_watts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-base</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.184570</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>84.120291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-base</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.184570</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>84.120291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-tiny</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.022461</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>37.162438</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-tiny</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.022461</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>37.162438</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_llm_inference_ohiot1dm</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.143242</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>Model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               experiment_type       model_name       mode  \\\n",
       "0   chronos_inference_ohiot1dm  chronos-t5-base  inference   \n",
       "1   chronos_inference_ohiot1dm  chronos-t5-base  inference   \n",
       "2   chronos_inference_ohiot1dm  chronos-t5-tiny  inference   \n",
       "3   chronos_inference_ohiot1dm  chronos-t5-tiny  inference   \n",
       "4  time_llm_inference_ohiot1dm            LLAMA  inference   \n",
       "\n",
       "          report_type                                          file_path  \\\n",
       "0  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "1  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "2  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "3  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "4  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "\n",
       "   total_parameters  model_size_mb    model_dtype model_architecture  \\\n",
       "0         201374976     768.184570  torch.float32       ChronosModel   \n",
       "1         201374976     768.184570  torch.float32       ChronosModel   \n",
       "2           8394496      32.022461  torch.float32       ChronosModel   \n",
       "3           8394496      32.022461  torch.float32       ChronosModel   \n",
       "4        6642504366   25339.143242  torch.float32              Model   \n",
       "\n",
       "   avg_inference_time_ms  ...  total_inferences  process_average_ram_mb  \\\n",
       "0              84.120291  ...               NaN                     NaN   \n",
       "1              84.120291  ...               NaN                     NaN   \n",
       "2              37.162438  ...               NaN                     NaN   \n",
       "3              37.162438  ...               NaN                     NaN   \n",
       "4                    NaN  ...               NaN                     NaN   \n",
       "\n",
       "   system_peak_ram_mb  average_gpu_allocated_mb  peak_gpu_reserved_mb  \\\n",
       "0                 NaN                       NaN                   NaN   \n",
       "1                 NaN                       NaN                   NaN   \n",
       "2                 NaN                       NaN                   NaN   \n",
       "3                 NaN                       NaN                   NaN   \n",
       "4                 NaN                       NaN                   NaN   \n",
       "\n",
       "   peak_gpu_utilization_percent  average_gpu_utilization_percent  \\\n",
       "0                           NaN                              NaN   \n",
       "1                           NaN                              NaN   \n",
       "2                           NaN                              NaN   \n",
       "3                           NaN                              NaN   \n",
       "4                           NaN                              NaN   \n",
       "\n",
       "   peak_temperature_celsius  inference_avg_power_w  peak_power_usage_watts  \n",
       "0                       NaN                    NaN                     NaN  \n",
       "1                       NaN                    NaN                     NaN  \n",
       "2                       NaN                    NaN                     NaN  \n",
       "3                       NaN                    NaN                     NaN  \n",
       "4                       NaN                    NaN                     NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the loaded inference data\n",
    "print(f\"‚úÖ Inference data loaded: {inference_data.shape}\")\n",
    "inference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "959cdb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Energy metrics: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>avg_power_w</th>\n",
       "      <th>energy_per_prediction_wh</th>\n",
       "      <th>daily_energy_moderate_wh</th>\n",
       "      <th>carbon_per_prediction_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>107.946905</td>\n",
       "      <td>0.047015</td>\n",
       "      <td>47.014532</td>\n",
       "      <td>0.023507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>99.727776</td>\n",
       "      <td>0.038248</td>\n",
       "      <td>38.248095</td>\n",
       "      <td>0.019124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>185.961041</td>\n",
       "      <td>25.226457</td>\n",
       "      <td>25226.457354</td>\n",
       "      <td>12.613229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>175.197675</td>\n",
       "      <td>15.170550</td>\n",
       "      <td>15170.550003</td>\n",
       "      <td>7.585275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>164.152812</td>\n",
       "      <td>9.494360</td>\n",
       "      <td>9494.360427</td>\n",
       "      <td>4.747180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>55.221700</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>19.500974</td>\n",
       "      <td>0.009750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  avg_power_w  energy_per_prediction_wh  \\\n",
       "0    Chronos-T5-Base   107.946905                  0.047015   \n",
       "1    Chronos-T5-Tiny    99.727776                  0.038248   \n",
       "2     Time-LLM-LLaMA   185.961041                 25.226457   \n",
       "3     Time-LLM-GPT-2   175.197675                 15.170550   \n",
       "4      Time-LLM-BERT   164.152812                  9.494360   \n",
       "5  Time-LLM-TinyBERT    55.221700                  0.019501   \n",
       "\n",
       "   daily_energy_moderate_wh  carbon_per_prediction_g  \n",
       "0                 47.014532                 0.023507  \n",
       "1                 38.248095                 0.019124  \n",
       "2              25226.457354                12.613229  \n",
       "3              15170.550003                 7.585275  \n",
       "4               9494.360427                 4.747180  \n",
       "5                 19.500974                 0.009750  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show energy metrics table\n",
    "print(f\"‚úÖ Energy metrics: {energy_metrics.shape}\")\n",
    "energy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6748796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference summary: (6, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>total_records</th>\n",
       "      <th>records_with_timing</th>\n",
       "      <th>records_with_memory</th>\n",
       "      <th>records_with_power</th>\n",
       "      <th>avg_inference_time_ms</th>\n",
       "      <th>min_inference_time_ms</th>\n",
       "      <th>max_inference_time_ms</th>\n",
       "      <th>std_inference_time_ms</th>\n",
       "      <th>throughput_predictions_per_sec</th>\n",
       "      <th>inference_peak_ram_mb</th>\n",
       "      <th>min_ram_mb</th>\n",
       "      <th>max_ram_mb</th>\n",
       "      <th>inference_avg_power_w</th>\n",
       "      <th>min_power_w</th>\n",
       "      <th>max_power_w</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>edge_feasibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1567.921873</td>\n",
       "      <td>82.726276</td>\n",
       "      <td>7505.916233</td>\n",
       "      <td>3319.439833</td>\n",
       "      <td>0.637787</td>\n",
       "      <td>1001.083594</td>\n",
       "      <td>843.792969</td>\n",
       "      <td>1577.417969</td>\n",
       "      <td>107.946905</td>\n",
       "      <td>107.946905</td>\n",
       "      <td>107.946905</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.184570</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1380.689999</td>\n",
       "      <td>35.835989</td>\n",
       "      <td>6757.453138</td>\n",
       "      <td>3005.702043</td>\n",
       "      <td>0.724276</td>\n",
       "      <td>993.389844</td>\n",
       "      <td>842.531250</td>\n",
       "      <td>1534.316406</td>\n",
       "      <td>99.727776</td>\n",
       "      <td>99.727776</td>\n",
       "      <td>99.727776</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.022461</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>488356.303489</td>\n",
       "      <td>91435.458492</td>\n",
       "      <td>885277.148487</td>\n",
       "      <td>561330.842184</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>26135.142578</td>\n",
       "      <td>26104.164062</td>\n",
       "      <td>26198.371094</td>\n",
       "      <td>185.961041</td>\n",
       "      <td>80.464554</td>\n",
       "      <td>291.457527</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.143242</td>\n",
       "      <td>Model</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>311727.767871</td>\n",
       "      <td>2425.929184</td>\n",
       "      <td>621029.606558</td>\n",
       "      <td>437418.855138</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>2217.891276</td>\n",
       "      <td>2126.156250</td>\n",
       "      <td>2372.304688</td>\n",
       "      <td>175.197675</td>\n",
       "      <td>72.270524</td>\n",
       "      <td>278.124825</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.471764</td>\n",
       "      <td>Model</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>208218.775673</td>\n",
       "      <td>2250.552981</td>\n",
       "      <td>414186.998365</td>\n",
       "      <td>291283.053948</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>2004.532715</td>\n",
       "      <td>1925.898438</td>\n",
       "      <td>2259.304688</td>\n",
       "      <td>164.152812</td>\n",
       "      <td>72.525650</td>\n",
       "      <td>255.779974</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.130119</td>\n",
       "      <td>Model</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Time-LLM-TinyBERT</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1271.302853</td>\n",
       "      <td>1271.302853</td>\n",
       "      <td>1271.302853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.786595</td>\n",
       "      <td>982.861979</td>\n",
       "      <td>943.949219</td>\n",
       "      <td>1060.687500</td>\n",
       "      <td>55.221700</td>\n",
       "      <td>55.221700</td>\n",
       "      <td>55.221700</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.656853</td>\n",
       "      <td>Model</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  total_records  records_with_timing  records_with_memory  \\\n",
       "0    Chronos-T5-Base              7                    5                    5   \n",
       "1    Chronos-T5-Tiny              7                    5                    5   \n",
       "2     Time-LLM-LLaMA             10                    2                    6   \n",
       "3     Time-LLM-GPT-2             10                    2                    6   \n",
       "4      Time-LLM-BERT             12                    2                    8   \n",
       "5  Time-LLM-TinyBERT              5                    1                    3   \n",
       "\n",
       "   records_with_power  avg_inference_time_ms  min_inference_time_ms  \\\n",
       "0                   1            1567.921873              82.726276   \n",
       "1                   1            1380.689999              35.835989   \n",
       "2                   2          488356.303489           91435.458492   \n",
       "3                   2          311727.767871            2425.929184   \n",
       "4                   2          208218.775673            2250.552981   \n",
       "5                   1            1271.302853            1271.302853   \n",
       "\n",
       "   max_inference_time_ms  std_inference_time_ms  \\\n",
       "0            7505.916233            3319.439833   \n",
       "1            6757.453138            3005.702043   \n",
       "2          885277.148487          561330.842184   \n",
       "3          621029.606558          437418.855138   \n",
       "4          414186.998365          291283.053948   \n",
       "5            1271.302853                    NaN   \n",
       "\n",
       "   throughput_predictions_per_sec  inference_peak_ram_mb    min_ram_mb  \\\n",
       "0                        0.637787            1001.083594    843.792969   \n",
       "1                        0.724276             993.389844    842.531250   \n",
       "2                        0.002048           26135.142578  26104.164062   \n",
       "3                        0.003208            2217.891276   2126.156250   \n",
       "4                        0.004803            2004.532715   1925.898438   \n",
       "5                        0.786595             982.861979    943.949219   \n",
       "\n",
       "     max_ram_mb  inference_avg_power_w  min_power_w  max_power_w  \\\n",
       "0   1577.417969             107.946905   107.946905   107.946905   \n",
       "1   1534.316406              99.727776    99.727776    99.727776   \n",
       "2  26198.371094             185.961041    80.464554   291.457527   \n",
       "3   2372.304688             175.197675    72.270524   278.124825   \n",
       "4   2259.304688             164.152812    72.525650   255.779974   \n",
       "5   1060.687500              55.221700    55.221700    55.221700   \n",
       "\n",
       "   total_parameters  model_size_mb model_architecture edge_feasibility  \n",
       "0         201374976     768.184570       ChronosModel         feasible  \n",
       "1           8394496      32.022461       ChronosModel         feasible  \n",
       "2        6642504366   25339.143242              Model      challenging  \n",
       "3         317055766    1209.471764              Model         feasible  \n",
       "4         282363198    1077.130119              Model         feasible  \n",
       "5          44998814     171.656853              Model         feasible  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show inference summary table  \n",
    "print(f\"‚úÖ Inference summary: {inference_metrics.shape}\")\n",
    "inference_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2377084",
   "metadata": {},
   "source": [
    "## üìä Quick Data Inspection\n",
    "\n",
    "Check what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03b765fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>training_time_hours</th>\n",
       "      <th>training_time_minutes</th>\n",
       "      <th>epochs_completed</th>\n",
       "      <th>final_train_loss</th>\n",
       "      <th>final_val_loss</th>\n",
       "      <th>best_train_loss</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>peak_ram_mb</th>\n",
       "      <th>avg_power_w</th>\n",
       "      <th>peak_power_w</th>\n",
       "      <th>peak_gpu_mb</th>\n",
       "      <th>avg_gpu_util_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>chronos-t5-base</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1531.207031</td>\n",
       "      <td>207.952649</td>\n",
       "      <td>377.355</td>\n",
       "      <td>776.309570</td>\n",
       "      <td>51.754746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>chronos-t5-tiny</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1531.285156</td>\n",
       "      <td>127.787712</td>\n",
       "      <td>259.712</td>\n",
       "      <td>40.147461</td>\n",
       "      <td>12.288202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2742.804688</td>\n",
       "      <td>134.960307</td>\n",
       "      <td>230.475</td>\n",
       "      <td>1832.453125</td>\n",
       "      <td>10.802920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2730.183594</td>\n",
       "      <td>167.733600</td>\n",
       "      <td>292.939</td>\n",
       "      <td>2206.436523</td>\n",
       "      <td>25.664516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1907.636719</td>\n",
       "      <td>300.858576</td>\n",
       "      <td>356.820</td>\n",
       "      <td>39177.378906</td>\n",
       "      <td>76.196970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment_type                                          file_path  \\\n",
       "0        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "1        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "2        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "3        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "4        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "\n",
       "        model_name training_time_hours training_time_minutes epochs_completed  \\\n",
       "0  chronos-t5-base                None                  None             None   \n",
       "1  chronos-t5-tiny                None                  None             None   \n",
       "2             BERT                None                  None             None   \n",
       "3             GPT2                None                  None             None   \n",
       "4            LLAMA                None                  None             None   \n",
       "\n",
       "  final_train_loss final_val_loss best_train_loss best_val_loss  peak_ram_mb  \\\n",
       "0             None           None            None          None  1531.207031   \n",
       "1             None           None            None          None  1531.285156   \n",
       "2             None           None            None          None  2742.804688   \n",
       "3             None           None            None          None  2730.183594   \n",
       "4             None           None            None          None  1907.636719   \n",
       "\n",
       "   avg_power_w  peak_power_w   peak_gpu_mb  avg_gpu_util_percent  \n",
       "0   207.952649       377.355    776.309570             51.754746  \n",
       "1   127.787712       259.712     40.147461             12.288202  \n",
       "2   134.960307       230.475   1832.453125             10.802920  \n",
       "3   167.733600       292.939   2206.436523             25.664516  \n",
       "4   300.858576       356.820  39177.378906             76.196970  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Distillation data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pipeline_id</th>\n",
       "      <th>pipeline_dir</th>\n",
       "      <th>patient_ids</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>teacher_model</th>\n",
       "      <th>student_model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>distilled_mape</th>\n",
       "      <th>distillation_time</th>\n",
       "      <th>distillation_status</th>\n",
       "      <th>teacher_to_distilled_rmse_improvement</th>\n",
       "      <th>teacher_to_distilled_rmse_improvement_pct</th>\n",
       "      <th>student_to_distilled_rmse_improvement</th>\n",
       "      <th>student_to_distilled_rmse_improvement_pct</th>\n",
       "      <th>pipeline_status</th>\n",
       "      <th>total_runtime</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-21 08:02:17</td>\n",
       "      <td>patient_570</td>\n",
       "      <td>distillation_experiments/pipeline_runs/pipelin...</td>\n",
       "      <td>570</td>\n",
       "      <td>ohiot1dm</td>\n",
       "      <td>831363</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>prajjwal1/bert-tiny</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>5.579967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.097586</td>\n",
       "      <td>0.572448</td>\n",
       "      <td>-0.306728</td>\n",
       "      <td>-1.843003</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>451.0</td>\n",
       "      <td>Patient 570 complete 3-phase pipeline run</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  pipeline_id  \\\n",
       "0  2025-10-21 08:02:17  patient_570   \n",
       "\n",
       "                                        pipeline_dir  patient_ids  \\\n",
       "0  distillation_experiments/pipeline_runs/pipelin...          570   \n",
       "\n",
       "  dataset_name    seed      teacher_model        student_model  learning_rate  \\\n",
       "0     ohiot1dm  831363  bert-base-uncased  prajjwal1/bert-tiny          0.001   \n",
       "\n",
       "   batch_size  ...  distilled_mape  distillation_time  distillation_status  \\\n",
       "0          32  ...        5.579967                NaN            completed   \n",
       "\n",
       "   teacher_to_distilled_rmse_improvement  \\\n",
       "0                               0.097586   \n",
       "\n",
       "   teacher_to_distilled_rmse_improvement_pct  \\\n",
       "0                                   0.572448   \n",
       "\n",
       "   student_to_distilled_rmse_improvement  \\\n",
       "0                              -0.306728   \n",
       "\n",
       "   student_to_distilled_rmse_improvement_pct  pipeline_status  total_runtime  \\\n",
       "0                                  -1.843003          SUCCESS          451.0   \n",
       "\n",
       "                                       notes  \n",
       "0  Patient 570 complete 3-phase pipeline run  \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show training data with pandas display\n",
    "print(\"‚úÖ Training data:\")\n",
    "if isinstance(training_data, pd.DataFrame) and not training_data.empty:\n",
    "    display(training_data.head())\n",
    "else:\n",
    "    print(f\"Training data: {type(training_data)}\")\n",
    "    \n",
    "print(\"‚úÖ Distillation data:\")\n",
    "if isinstance(distillation_data, pd.DataFrame) and not distillation_data.empty:\n",
    "    display(distillation_data.head())\n",
    "else:\n",
    "    print(f\"Distillation data: {type(distillation_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7824424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating comprehensive standardized LaTeX table...\n",
      "üìä Generating comprehensive standardized table...\n",
      "‚úÖ Table generated: outputs/latex_tables/comprehensive_standardized_metrics.tex\n",
      "‚úÖ Generated tables:\n",
      "  - comprehensive_standardized: outputs/latex_tables/comprehensive_standardized_metrics.tex\n",
      "\n",
      "üìÑ Table preview (first 500 characters):\n",
      "\\documentclass{article}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage{array}\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{rotating}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\begin{table*}[htbp]\n",
      "    \\centering\n",
      "    \\caption{Comprehensive Model Performance: Standardized Inference Metrics for Edge Deployment Assessment}\n",
      "    \\label{tab:comprehensive_standardized_metrics}\n",
      "    \\tiny\n",
      "    \\begin{tabular}{@{}l|cc|cc|cc|c|c@{}}\n",
      "        \\toprule\n",
      "        & \\multicolumn{2}{c|}{\\textbf{Latency (ms)}} & \\multicolumn{2}{c|}{\\textbf{Memo...\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive standardized table addressing all reviewer concerns\n",
    "print(\"üìä Generating comprehensive standardized LaTeX table...\")\n",
    "\n",
    "# Reload the module to get the updated functions\n",
    "import importlib\n",
    "import latex_table_generator\n",
    "importlib.reload(latex_table_generator)\n",
    "from latex_table_generator import generate_all_tables, create_comprehensive_standardized_table\n",
    "\n",
    "# Use the new comprehensive function\n",
    "latex_tables = generate_all_tables(inference_data, training_data, Path(\"outputs/latex_tables\"))\n",
    "\n",
    "print(\"‚úÖ Generated tables:\")\n",
    "for table_name, file_path in latex_tables.items():\n",
    "    print(f\"  - {table_name}: {file_path}\")\n",
    "    \n",
    "# Also generate just the table content for preview\n",
    "table_content = create_comprehensive_standardized_table(inference_data, training_data)\n",
    "\n",
    "print(f\"\\nüìÑ Table preview (first 500 characters):\")\n",
    "print(table_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to ensure we get the updated version\n",
    "import importlib\n",
    "import latex_table_generator\n",
    "importlib.reload(latex_table_generator)\n",
    "from latex_table_generator import generate_all_tables\n",
    "\n",
    "print(\"üîÑ Reloaded latex_table_generator module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046557b",
   "metadata": {},
   "source": [
    "## üéØ That's it!\n",
    "\n",
    "**No more 79-cell nightmare!** \n",
    "\n",
    "‚úÖ Clean, organized functions  \n",
    "‚úÖ One-click complete analysis  \n",
    "‚úÖ Real experimental data  \n",
    "‚úÖ CPU + GPU metrics  \n",
    "‚úÖ LaTeX tables ready for publication  \n",
    "\n",
    "**All outputs saved to:** `/home/amma/LLM-TIME/notebooks/outputs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
