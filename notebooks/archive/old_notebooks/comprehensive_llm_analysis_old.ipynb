{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db65a68c",
   "metadata": {},
   "source": [
    "# ğŸ” LLM Efficiency Analysis - Clean & Comprehensive\n",
    "\n",
    "**Purpose**: Complete analysis of LLM performance across all phases\n",
    "\n",
    "**Analysis Phases**:\n",
    "1. **Inference Performance** - Speed, memory, power efficiency\n",
    "2. **Training Efficiency** - Training time, convergence, resource usage  \n",
    "3. **Distillation Results** - Knowledge transfer effectiveness\n",
    "4. **Edge Deployment** - Feasibility assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c0c0a",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('/home/amma/LLM-TIME')\n",
    "RESULTS_PATH = Path('./comprehensive_analysis_results')\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Import and reload modules\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Reload for latest changes\n",
    "for module_name in ['enhanced_data_loader', 'analysis_utils', 'edge_analysis', 'training_analysis']:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "from enhanced_data_loader import EnhancedEfficiencyDataLoader\n",
    "from analysis_utils import calculate_inference_metrics, create_inference_plots, calculate_energy_metrics\n",
    "from edge_analysis import assess_edge_compatibility, get_edge_recommendations\n",
    "from training_analysis import TrainingAnalyzer, analyze_all_phases\n",
    "\n",
    "print(\"âœ… Setup complete - all modules loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badccb9b",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze all phases\n",
    "print(\"ğŸ”„ Loading comprehensive analysis data...\")\n",
    "\n",
    "inference_summary, training_summary, distill_summary = analyze_all_phases(PROJECT_ROOT)\n",
    "\n",
    "print(f\"\\nğŸ“Š Analysis Overview:\")\n",
    "print(f\"  â€¢ Inference models analyzed: {len(inference_summary)}\")\n",
    "print(f\"  â€¢ Training experiments: {len(training_summary)}\")\n",
    "print(f\"  â€¢ Distillation pairs: {len(distill_summary)}\")\n",
    "\n",
    "# Show available models\n",
    "if not inference_summary.empty:\n",
    "    print(f\"\\nğŸ¤– Models with inference data:\")\n",
    "    for model in inference_summary['model_name'].unique():\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca38002",
   "metadata": {},
   "source": [
    "## 3. Inference Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference performance analysis\n",
    "print(\"âš¡ Analyzing inference performance...\")\n",
    "\n",
    "if not inference_summary.empty:\n",
    "    # Display summary table\n",
    "    print(\"\\nğŸ“Š Inference Performance Summary:\")\n",
    "    display_cols = ['model_name', 'avg_inference_time_ms', 'inference_peak_ram_mb', 'inference_avg_power_w', 'edge_feasibility']\n",
    "    available_cols = [col for col in display_cols if col in inference_summary.columns]\n",
    "    print(inference_summary[available_cols].to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nğŸ“Š Creating inference visualizations...\")\n",
    "    create_inference_plots(inference_summary, str(RESULTS_PATH / 'inference_performance.png'))\n",
    "    \n",
    "    # Performance rankings\n",
    "    complete_data = inference_summary.dropna(subset=['avg_inference_time_ms', 'inference_peak_ram_mb'])\n",
    "    if not complete_data.empty:\n",
    "        fastest = complete_data.loc[complete_data['avg_inference_time_ms'].idxmin()]\n",
    "        most_efficient = complete_data.loc[complete_data['inference_peak_ram_mb'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nğŸ† Performance Winners:\")\n",
    "        print(f\"  âš¡ Fastest: {fastest['model_name']} ({fastest['avg_inference_time_ms']:.1f}ms)\")\n",
    "        print(f\"  ğŸ’¾ Memory Efficient: {most_efficient['model_name']} ({most_efficient['inference_peak_ram_mb']:.1f}MB)\")\n",
    "        \n",
    "        if 'inference_avg_power_w' in complete_data.columns:\n",
    "            power_data = complete_data.dropna(subset=['inference_avg_power_w'])\n",
    "            if not power_data.empty:\n",
    "                most_power_efficient = power_data.loc[power_data['inference_avg_power_w'].idxmin()]\n",
    "                print(f\"  ğŸ”‹ Power Efficient: {most_power_efficient['model_name']} ({most_power_efficient['inference_avg_power_w']:.1f}W)\")\n",
    "    \n",
    "    print(\"âœ… Inference analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No inference data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aceeacf",
   "metadata": {},
   "source": [
    "## 4. Training Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abba6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training efficiency analysis\n",
    "print(\"ğŸš€ Analyzing training efficiency...\")\n",
    "\n",
    "if not training_summary.empty:\n",
    "    # Display training summary\n",
    "    print(\"\\nğŸ“Š Training Performance Summary:\")\n",
    "    training_cols = ['model_name', 'avg_training_time_hours', 'avg_final_val_loss', 'avg_peak_ram_mb', 'avg_power_w']\n",
    "    available_training_cols = [col for col in training_cols if col in training_summary.columns]\n",
    "    print(training_summary[available_training_cols].round(3).to_string(index=False))\n",
    "    \n",
    "    # Create training visualizations\n",
    "    analyzer = TrainingAnalyzer(PROJECT_ROOT)\n",
    "    print(\"\\nğŸ“Š Creating training visualizations...\")\n",
    "    analyzer.create_training_plots(training_summary, str(RESULTS_PATH / 'training_performance.png'))\n",
    "    \n",
    "    # Training insights\n",
    "    if 'avg_training_time_hours' in training_summary.columns:\n",
    "        fastest_training = training_summary.loc[training_summary['avg_training_time_hours'].idxmin()]\n",
    "        print(f\"\\nğŸ† Training Winners:\")\n",
    "        print(f\"  âš¡ Fastest Training: {fastest_training['model_name']} ({fastest_training['avg_training_time_hours']:.1f}h)\")\n",
    "        \n",
    "        if 'avg_final_val_loss' in training_summary.columns:\n",
    "            best_convergence = training_summary.loc[training_summary['avg_final_val_loss'].idxmin()]\n",
    "            print(f\"  ğŸ¯ Best Convergence: {best_convergence['model_name']} (loss: {best_convergence['avg_final_val_loss']:.4f})\")\n",
    "    \n",
    "    print(\"âœ… Training analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No training efficiency data found\")\n",
    "    print(\"ğŸ’¡ Training data may be in different format or location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858d433",
   "metadata": {},
   "source": [
    "## 5. Distillation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation performance analysis\n",
    "print(\"ğŸ“ Analyzing distillation performance...\")\n",
    "\n",
    "if not distill_summary.empty:\n",
    "    # Display distillation summary\n",
    "    print(\"\\nğŸ“Š Distillation Performance Summary:\")\n",
    "    distill_cols = ['teacher_model', 'student_model', 'avg_rmse', 'avg_mae', 'improvement_percent']\n",
    "    available_distill_cols = [col for col in distill_cols if col in distill_summary.columns]\n",
    "    print(distill_summary[available_distill_cols].round(4).to_string(index=False))\n",
    "    \n",
    "    # Create distillation visualizations\n",
    "    analyzer = TrainingAnalyzer(PROJECT_ROOT)\n",
    "    print(\"\\nğŸ“Š Creating distillation visualizations...\")\n",
    "    analyzer.create_distillation_plots(distill_summary, str(RESULTS_PATH / 'distillation_performance.png'))\n",
    "    \n",
    "    # Distillation insights\n",
    "    if 'improvement_percent' in distill_summary.columns:\n",
    "        valid_improvements = distill_summary.dropna(subset=['improvement_percent'])\n",
    "        if not valid_improvements.empty:\n",
    "            best_distill = valid_improvements.loc[valid_improvements['improvement_percent'].idxmax()]\n",
    "            print(f\"\\nğŸ† Distillation Winners:\")\n",
    "            print(f\"  ğŸ¯ Best Knowledge Transfer: {best_distill['teacher_model']} â†’ {best_distill['student_model']}\")\n",
    "            print(f\"  ğŸ“ˆ Improvement: {best_distill['improvement_percent']:.1f}%\")\n",
    "    \n",
    "    print(\"âœ… Distillation analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No distillation results found\")\n",
    "    print(\"ğŸ’¡ Check distillation_experiments folder for pipeline results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2360041",
   "metadata": {},
   "source": [
    "## 6. Edge Deployment Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6163c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge deployment analysis\n",
    "print(\"ğŸ“± Analyzing edge deployment feasibility...\")\n",
    "\n",
    "if not inference_summary.empty:\n",
    "    # Edge compatibility assessment\n",
    "    edge_assessment = assess_edge_compatibility(inference_summary)\n",
    "    recommendations = get_edge_recommendations(edge_assessment)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Edge Deployment Summary:\")\n",
    "    for model_name in inference_summary['model_name'].unique():\n",
    "        model_compatible = edge_assessment[edge_assessment['model_name'] == model_name]['compatible'].sum()\n",
    "        total_devices = len(edge_assessment[edge_assessment['model_name'] == model_name])\n",
    "        print(f\"  â€¢ {model_name}: {model_compatible}/{total_devices} devices compatible\")\n",
    "    \n",
    "    # Device recommendations\n",
    "    print(f\"\\nğŸ† Recommended Edge Devices:\")\n",
    "    for model_name, recs in recommendations.items():\n",
    "        if recs:\n",
    "            best_device = recs[0]\n",
    "            print(f\"  â€¢ {model_name}: {best_device['device_name']} (${best_device['device_cost_usd']})\")\n",
    "        else:\n",
    "            print(f\"  â€¢ {model_name}: No suitable devices found\")\n",
    "    \n",
    "    # Save edge assessment\n",
    "    edge_assessment.to_csv(RESULTS_PATH / 'edge_compatibility_assessment.csv', index=False)\n",
    "    print(f\"\\nğŸ’¾ Edge assessment saved to: {RESULTS_PATH / 'edge_compatibility_assessment.csv'}\")\n",
    "    \n",
    "    print(\"âœ… Edge deployment analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No inference data available for edge assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab824e1",
   "metadata": {},
   "source": [
    "## 7. Energy and Sustainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy efficiency analysis\n",
    "print(\"ğŸŒ± Analyzing energy efficiency and sustainability...\")\n",
    "\n",
    "if not inference_summary.empty:\n",
    "    energy_metrics = calculate_energy_metrics(inference_summary)\n",
    "    \n",
    "    if not energy_metrics.empty:\n",
    "        print(f\"\\nğŸ”‹ Energy Efficiency Summary:\")\n",
    "        print(energy_metrics.to_string(index=False))\n",
    "        \n",
    "        # Most energy efficient\n",
    "        most_efficient = energy_metrics.loc[energy_metrics['energy_per_prediction_wh'].idxmin()]\n",
    "        print(f\"\\nğŸ† Most Energy Efficient: {most_efficient['model_name']}\")\n",
    "        print(f\"   Energy per prediction: {most_efficient['energy_per_prediction_wh']:.6f} Wh\")\n",
    "        print(f\"   Carbon footprint: {most_efficient['carbon_per_prediction_g']:.8f} g COâ‚‚\")\n",
    "        print(f\"   Annual carbon footprint: {most_efficient['annual_carbon_footprint_kg']:.2f} kg COâ‚‚\")\n",
    "        \n",
    "        # Save energy analysis\n",
    "        energy_metrics.to_csv(RESULTS_PATH / 'energy_sustainability_analysis.csv', index=False)\n",
    "        print(f\"\\nğŸ’¾ Energy analysis saved to: {RESULTS_PATH / 'energy_sustainability_analysis.csv'}\")\n",
    "        \n",
    "        print(\"âœ… Energy analysis complete\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No power consumption data available for energy analysis\")\n",
    "else:\n",
    "    print(\"âš ï¸ No inference data available for energy analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e82e5",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"ğŸ“‹ Generating comprehensive analysis report...\")\n",
    "\n",
    "analyzer = TrainingAnalyzer(PROJECT_ROOT)\n",
    "comprehensive_report = analyzer.generate_comprehensive_report(inference_summary, training_summary, distill_summary)\n",
    "\n",
    "# Save report\n",
    "report_file = RESULTS_PATH / 'comprehensive_llm_analysis_report.md'\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(comprehensive_report)\n",
    "\n",
    "print(f\"ğŸ“„ Comprehensive report saved to: {report_file}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"ğŸ¯ COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "# Analysis overview\n",
    "print(f\"\\nğŸ“Š Analysis Coverage:\")\n",
    "print(f\"  â€¢ Inference models: {len(inference_summary)} models analyzed\")\n",
    "print(f\"  â€¢ Training experiments: {len(training_summary)} models trained\")\n",
    "print(f\"  â€¢ Distillation pairs: {len(distill_summary)} teacher-student combinations\")\n",
    "\n",
    "# Overall recommendations\n",
    "print(f\"\\nğŸ† Overall Recommendations:\")\n",
    "\n",
    "if not inference_summary.empty:\n",
    "    complete_data = inference_summary.dropna(subset=['avg_inference_time_ms', 'inference_peak_ram_mb'])\n",
    "    if not complete_data.empty:\n",
    "        # Calculate overall efficiency score\n",
    "        if all(col in complete_data.columns for col in ['avg_inference_time_ms', 'inference_peak_ram_mb']):\n",
    "            score_data = complete_data.copy()\n",
    "            \n",
    "            # Normalize metrics (lower is better)\n",
    "            score_data['time_score'] = score_data['avg_inference_time_ms'] / score_data['avg_inference_time_ms'].max()\n",
    "            score_data['memory_score'] = score_data['inference_peak_ram_mb'] / score_data['inference_peak_ram_mb'].max()\n",
    "            \n",
    "            # Power score if available\n",
    "            if 'inference_avg_power_w' in score_data.columns:\n",
    "                power_data = score_data.dropna(subset=['inference_avg_power_w'])\n",
    "                if not power_data.empty:\n",
    "                    score_data['power_score'] = score_data['inference_avg_power_w'] / score_data['inference_avg_power_w'].max()\n",
    "                    score_data['overall_score'] = (score_data['time_score'] + score_data['memory_score'] + score_data['power_score']) / 3\n",
    "                else:\n",
    "                    score_data['overall_score'] = (score_data['time_score'] + score_data['memory_score']) / 2\n",
    "            else:\n",
    "                score_data['overall_score'] = (score_data['time_score'] + score_data['memory_score']) / 2\n",
    "            \n",
    "            overall_winner = score_data.loc[score_data['overall_score'].idxmin()]\n",
    "            print(f\"  ğŸ¥‡ Best Overall Model: {overall_winner['model_name']}\")\n",
    "            print(f\"     âš¡ Speed: {overall_winner['avg_inference_time_ms']:.1f}ms\")\n",
    "            print(f\"     ğŸ’¾ Memory: {overall_winner['inference_peak_ram_mb']:.1f}MB\")\n",
    "            if 'inference_avg_power_w' in overall_winner.index:\n",
    "                print(f\"     ğŸ”‹ Power: {overall_winner['inference_avg_power_w']:.1f}W\")\n",
    "            print(f\"     ğŸ† Overall Score: {overall_winner['overall_score']:.3f}\")\n",
    "\n",
    "# Edge deployment summary\n",
    "if not inference_summary.empty and 'edge_feasibility' in inference_summary.columns:\n",
    "    feasibility_counts = inference_summary['edge_feasibility'].value_counts()\n",
    "    print(f\"\\nğŸ“± Edge Deployment Readiness:\")\n",
    "    for category, count in feasibility_counts.items():\n",
    "        emoji_map = {\"highly_feasible\": \"ğŸŸ¢\", \"feasible\": \"ğŸŸ¡\", \"challenging\": \"ğŸ”´\", \"unknown\": \"âšª\"}\n",
    "        emoji = emoji_map.get(str(category), \"ğŸ”˜\")\n",
    "        category_str = str(category).replace('_', ' ').title()\n",
    "        print(f\"  {emoji} {category_str}: {count} models\")\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive LLM analysis complete!\")\n",
    "print(f\"ğŸ“ All results and visualizations saved to: {RESULTS_PATH}\")\n",
    "print(f\"ğŸ“Š Generated files:\")\n",
    "for file_path in RESULTS_PATH.glob('*'):\n",
    "    print(f\"  - {file_path.name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
