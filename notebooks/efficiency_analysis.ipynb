{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1833ff2b",
   "metadata": {},
   "source": [
    "# üìä Model Efficiency Analysis\n",
    "\n",
    "**Comprehensive efficiency analysis with automated table and chart generation**\n",
    "\n",
    "This notebook analyzes model performance metrics including:\n",
    "- CPU/GPU latency and throughput\n",
    "- Memory usage (RAM/VRAM)\n",
    "- Power consumption\n",
    "- Model size and parameters\n",
    "\n",
    "Outputs publication-ready **LaTeX tables** and **efficiency plots**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26a2d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All working modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import working modules from ultimate notebook\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.enhanced_data_loader import EnhancedEfficiencyDataLoader\n",
    "from utils.analysis_utils import create_inference_plots, calculate_energy_metrics, calculate_inference_summary, create_efficiency_table, generate_efficiency_report\n",
    "from utils.training_analysis import TrainingAnalyzer\n",
    "from utils.latex_table_generator import generate_all_tables, create_real_data_latex_table, create_corrected_latex_table\n",
    "\n",
    "print(\"‚úÖ All working modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500a35f",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Load Data\n",
    "\n",
    "First, load the experimental data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "01cc6260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for experiment files...\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/time_llm_inference_ohiot1dm/seed_831363_model_LLAMA_dim_4096_seq_6_context_6_pred_6_patch_6_epochs_0/patient_570/logs/real_performance_reports\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/time_llm_inference_ohiot1dm/seed_831363_model_GPT2_dim_768_seq_6_context_6_pred_6_patch_6_epochs_0/patient_570/logs/real_performance_reports\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/time_llm_inference_ohiot1dm/seed_831363_model_BERT_dim_768_seq_6_context_6_pred_6_patch_6_epochs_0/patient_570/logs/real_performance_reports\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/distillation_inference_ohiot1dm/seed_831363_model_tinybert/patient_570/logs/real_performance_reports\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/time_llm_training_ohiot1dm/seed_831363_model_BERT_dim_768_seq_6_context_6_pred_6_patch_6_epochs_10/patient_570/logs/real_performance_reports\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/time_llm_training_ohiot1dm/seed_831363_model_GPT2_dim_768_seq_6_context_6_pred_6_patch_6_epochs_10/patient_570/logs/real_performance_reports\n",
      "üéØ Using inference files for //home/amma/LLM-TIME/efficiency_experiments/experiments/time_llm_training_ohiot1dm/seed_831363_model_LLAMA_dim_4096_seq_6_context_6_pred_6_patch_6_epochs_1/patient_570/logs/real_performance_reports\n",
      "üìä Found 46 JSON files\n",
      "üìä Processing 24 efficiency_reports...\n",
      "üìä Processing 11 comprehensive_reports...\n",
      "üìä Processing 11 real_performance_reports...\n",
      "‚úÖ Loaded 44 total records\n",
      "Loaded 44 inference records\n"
     ]
    }
   ],
   "source": [
    "# Load actual data using the working loader\n",
    "BASE_PATH = Path.cwd().parent\n",
    "loader = EnhancedEfficiencyDataLoader(BASE_PATH)\n",
    "inference_data = loader.parse_all_data()\n",
    "print(f\"Loaded {len(inference_data)} inference records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4fd7b",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Calculate Metrics\n",
    "\n",
    "Calculate performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bf716cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference summary calculated!\n"
     ]
    }
   ],
   "source": [
    "# Calculate inference summary using working function\n",
    "inference_metrics = calculate_inference_summary(inference_data)\n",
    "print(\"Inference summary calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17869789",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Load Additional Data\n",
    "\n",
    "Load training and other data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4972c978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: 10 records\n"
     ]
    }
   ],
   "source": [
    "# Load training data using working analyzer\n",
    "training_analyzer = TrainingAnalyzer(BASE_PATH)\n",
    "training_data = training_analyzer.load_training_efficiency_data()\n",
    "print(f\"Loaded training data: {len(training_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cea8e6",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Generate Plots and Tables\n",
    "\n",
    "Create visualizations and LaTeX tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b97031e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Plotting 6 unique models: ['Chronos-T5-Base', 'Chronos-T5-Tiny', 'Time-LLM-BERT', 'Time-LLM-GPT-2', 'Time-LLM-LLaMA', 'Time-LLM-TinyBERT (Distilled)']\n",
      "üìä Dashboard plot saved to: outputs/clean_inference_plots_dashboard.png\n",
      "üìä Detailed analysis plot saved to: outputs/clean_inference_plots_detailed.png\n"
     ]
    }
   ],
   "source": [
    "# Create inference plots with standardized model names (saves to file only, no display)\n",
    "create_inference_plots(inference_data, save_path=\"outputs/clean_inference_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fea1ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy metrics calculated!\n"
     ]
    }
   ],
   "source": [
    "# Calculate energy metrics using working function (needs inference summary first)\n",
    "energy_metrics = calculate_energy_metrics(inference_metrics)\n",
    "print(\"Energy metrics calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7618abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating comprehensive standardized table...\n",
      "‚úÖ Table generated: /home/amma/LLM-TIME/notebooks/outputs/latex_tables/comprehensive_standardized_metrics.tex\n",
      "‚úÖ LaTeX tables generated!\n",
      "  comprehensive_standardized: /home/amma/LLM-TIME/notebooks/outputs/latex_tables/comprehensive_standardized_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "# Generate LaTeX tables using working functions\n",
    "latex_tables = generate_all_tables(inference_data, training_data)\n",
    "print(\"‚úÖ LaTeX tables generated!\")\n",
    "for name, path in latex_tables.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d2df1263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Efficiency table created: (44, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>avg_inference_time_ms</th>\n",
       "      <th>inference_peak_ram_mb</th>\n",
       "      <th>inference_avg_power_w</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>edge_feasibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>84.12</td>\n",
       "      <td>843.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>84.12</td>\n",
       "      <td>843.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>37.16</td>\n",
       "      <td>842.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>37.16</td>\n",
       "      <td>842.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26104.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26104.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2199.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2199.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1968.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1968.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>82.73</td>\n",
       "      <td>870.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>82.73</td>\n",
       "      <td>870.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.18</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>35.84</td>\n",
       "      <td>873.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>35.84</td>\n",
       "      <td>873.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.02</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Time-LLM-TinyBERT (Distilled)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>943.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Time-LLM-TinyBERT (Distilled)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>943.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1925.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1925.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2126.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2126.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26105.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26105.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>requires_optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>7505.92</td>\n",
       "      <td>1577.42</td>\n",
       "      <td>107.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>6757.45</td>\n",
       "      <td>1534.32</td>\n",
       "      <td>99.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>91435.46</td>\n",
       "      <td>26192.66</td>\n",
       "      <td>80.46</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>2425.93</td>\n",
       "      <td>2283.16</td>\n",
       "      <td>72.27</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>2250.55</td>\n",
       "      <td>2052.39</td>\n",
       "      <td>72.53</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Time-LLM-TinyBERT (Distilled)</td>\n",
       "      <td>1271.30</td>\n",
       "      <td>1060.69</td>\n",
       "      <td>55.22</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>414187.00</td>\n",
       "      <td>2259.30</td>\n",
       "      <td>255.78</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>621029.61</td>\n",
       "      <td>2372.30</td>\n",
       "      <td>278.12</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>885277.15</td>\n",
       "      <td>26198.37</td>\n",
       "      <td>291.46</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>7505.92</td>\n",
       "      <td>1577.42</td>\n",
       "      <td>107.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>6757.45</td>\n",
       "      <td>1534.32</td>\n",
       "      <td>99.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>91435.46</td>\n",
       "      <td>26192.66</td>\n",
       "      <td>80.46</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>2425.93</td>\n",
       "      <td>2283.16</td>\n",
       "      <td>72.27</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>2250.55</td>\n",
       "      <td>2052.39</td>\n",
       "      <td>72.53</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>390011.06</td>\n",
       "      <td>1531.21</td>\n",
       "      <td>207.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>179945.97</td>\n",
       "      <td>1531.29</td>\n",
       "      <td>127.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Time-LLM-TinyBERT (Distilled)</td>\n",
       "      <td>1271.30</td>\n",
       "      <td>1060.69</td>\n",
       "      <td>55.22</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.66</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>414187.00</td>\n",
       "      <td>2259.30</td>\n",
       "      <td>255.78</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.13</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>621029.61</td>\n",
       "      <td>2372.30</td>\n",
       "      <td>278.12</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.47</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>885277.15</td>\n",
       "      <td>26198.37</td>\n",
       "      <td>291.46</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.14</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_name  avg_inference_time_ms  \\\n",
       "0                 Chronos-T5-Base                  84.12   \n",
       "1                 Chronos-T5-Base                  84.12   \n",
       "2                 Chronos-T5-Tiny                  37.16   \n",
       "3                 Chronos-T5-Tiny                  37.16   \n",
       "4                  Time-LLM-LLaMA                    NaN   \n",
       "5                  Time-LLM-LLaMA                    NaN   \n",
       "6                  Time-LLM-GPT-2                    NaN   \n",
       "7                  Time-LLM-GPT-2                    NaN   \n",
       "8                   Time-LLM-BERT                    NaN   \n",
       "9                   Time-LLM-BERT                    NaN   \n",
       "10                Chronos-T5-Base                  82.73   \n",
       "11                Chronos-T5-Base                  82.73   \n",
       "12                Chronos-T5-Tiny                  35.84   \n",
       "13                Chronos-T5-Tiny                  35.84   \n",
       "14  Time-LLM-TinyBERT (Distilled)                    NaN   \n",
       "15  Time-LLM-TinyBERT (Distilled)                    NaN   \n",
       "16                  Time-LLM-BERT                    NaN   \n",
       "17                  Time-LLM-BERT                    NaN   \n",
       "18                  Time-LLM-BERT                    NaN   \n",
       "19                  Time-LLM-BERT                    NaN   \n",
       "20                 Time-LLM-GPT-2                    NaN   \n",
       "21                 Time-LLM-GPT-2                    NaN   \n",
       "22                 Time-LLM-LLaMA                    NaN   \n",
       "23                 Time-LLM-LLaMA                    NaN   \n",
       "24                Chronos-T5-Base                7505.92   \n",
       "25                Chronos-T5-Tiny                6757.45   \n",
       "26                 Time-LLM-LLaMA               91435.46   \n",
       "27                 Time-LLM-GPT-2                2425.93   \n",
       "28                  Time-LLM-BERT                2250.55   \n",
       "29  Time-LLM-TinyBERT (Distilled)                1271.30   \n",
       "30                  Time-LLM-BERT              414187.00   \n",
       "31                 Time-LLM-GPT-2              621029.61   \n",
       "32                 Time-LLM-LLaMA              885277.15   \n",
       "33                Chronos-T5-Base                7505.92   \n",
       "34                Chronos-T5-Tiny                6757.45   \n",
       "35                 Time-LLM-LLaMA               91435.46   \n",
       "36                 Time-LLM-GPT-2                2425.93   \n",
       "37                  Time-LLM-BERT                2250.55   \n",
       "38                Chronos-T5-Base              390011.06   \n",
       "39                Chronos-T5-Tiny              179945.97   \n",
       "40  Time-LLM-TinyBERT (Distilled)                1271.30   \n",
       "41                  Time-LLM-BERT              414187.00   \n",
       "42                 Time-LLM-GPT-2              621029.61   \n",
       "43                 Time-LLM-LLaMA              885277.15   \n",
       "\n",
       "    inference_peak_ram_mb  inference_avg_power_w  total_parameters  \\\n",
       "0                  843.79                    NaN         201374976   \n",
       "1                  843.79                    NaN         201374976   \n",
       "2                  842.53                    NaN           8394496   \n",
       "3                  842.53                    NaN           8394496   \n",
       "4                26104.16                    NaN        6642504366   \n",
       "5                26104.16                    NaN        6642504366   \n",
       "6                 2199.79                    NaN         317055766   \n",
       "7                 2199.79                    NaN         317055766   \n",
       "8                 1968.51                    NaN         282363198   \n",
       "9                 1968.51                    NaN         282363198   \n",
       "10                 870.21                    NaN         201374976   \n",
       "11                 870.21                    NaN         201374976   \n",
       "12                 873.79                    NaN           8394496   \n",
       "13                 873.79                    NaN           8394496   \n",
       "14                 943.95                    NaN          44998814   \n",
       "15                 943.95                    NaN          44998814   \n",
       "16                1967.88                    NaN         282363198   \n",
       "17                1967.88                    NaN         282363198   \n",
       "18                1925.90                    NaN         282363198   \n",
       "19                1925.90                    NaN         282363198   \n",
       "20                2126.16                    NaN         317055766   \n",
       "21                2126.16                    NaN         317055766   \n",
       "22               26105.75                    NaN        6642504366   \n",
       "23               26105.75                    NaN        6642504366   \n",
       "24                1577.42                 107.95                 0   \n",
       "25                1534.32                  99.73                 0   \n",
       "26               26192.66                  80.46        6642504366   \n",
       "27                2283.16                  72.27         317055766   \n",
       "28                2052.39                  72.53         282363198   \n",
       "29                1060.69                  55.22          44998814   \n",
       "30                2259.30                 255.78         282363198   \n",
       "31                2372.30                 278.12         317055766   \n",
       "32               26198.37                 291.46        6642504366   \n",
       "33                1577.42                 107.95                 0   \n",
       "34                1534.32                  99.73                 0   \n",
       "35               26192.66                  80.46        6642504366   \n",
       "36                2283.16                  72.27         317055766   \n",
       "37                2052.39                  72.53         282363198   \n",
       "38                1531.21                 207.95                 0   \n",
       "39                1531.29                 127.79                 0   \n",
       "40                1060.69                  55.22          44998814   \n",
       "41                2259.30                 255.78         282363198   \n",
       "42                2372.30                 278.12         317055766   \n",
       "43               26198.37                 291.46        6642504366   \n",
       "\n",
       "    model_size_mb       edge_feasibility  \n",
       "0          768.18               feasible  \n",
       "1          768.18               feasible  \n",
       "2           32.02               feasible  \n",
       "3           32.02               feasible  \n",
       "4        25339.14  requires_optimization  \n",
       "5        25339.14  requires_optimization  \n",
       "6         1209.47               feasible  \n",
       "7         1209.47               feasible  \n",
       "8         1077.13               feasible  \n",
       "9         1077.13               feasible  \n",
       "10         768.18               feasible  \n",
       "11         768.18               feasible  \n",
       "12          32.02               feasible  \n",
       "13          32.02               feasible  \n",
       "14         171.66               feasible  \n",
       "15         171.66               feasible  \n",
       "16        1077.13               feasible  \n",
       "17        1077.13               feasible  \n",
       "18        1077.13               feasible  \n",
       "19        1077.13               feasible  \n",
       "20        1209.47               feasible  \n",
       "21        1209.47               feasible  \n",
       "22       25339.14  requires_optimization  \n",
       "23       25339.14  requires_optimization  \n",
       "24           0.00            challenging  \n",
       "25           0.00            challenging  \n",
       "26       25339.14            challenging  \n",
       "27        1209.47            challenging  \n",
       "28        1077.13            challenging  \n",
       "29         171.66            challenging  \n",
       "30        1077.13            challenging  \n",
       "31        1209.47            challenging  \n",
       "32       25339.14            challenging  \n",
       "33           0.00            challenging  \n",
       "34           0.00            challenging  \n",
       "35       25339.14            challenging  \n",
       "36        1209.47            challenging  \n",
       "37        1077.13            challenging  \n",
       "38           0.00            challenging  \n",
       "39           0.00            challenging  \n",
       "40         171.66            challenging  \n",
       "41        1077.13            challenging  \n",
       "42        1209.47            challenging  \n",
       "43       25339.14            challenging  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create efficiency comparison table using working function\n",
    "comparison_table = create_efficiency_table(inference_data)\n",
    "print(f\"‚úÖ Efficiency table created: {comparison_table.shape}\")\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "17a05b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary report generated!\n",
      "==================================================\n",
      "# üöÄ LLM Performance Analysis Report\n",
      "==================================================\n",
      "\n",
      "## üìä Dataset Overview\n",
      "- **Total Records**: 44\n",
      "- **Unique Models**: 6\n",
      "- **Experiment Types**: 5\n",
      "- **Records with Timing Data**: 28\n",
      "- **Records with Power Data**: 20\n",
      "\n",
      "## ‚ö° Performance Summary\n",
      "- **Fastest Model**: Time-LLM-TinyBERT (Distilled) (1271.3ms)\n",
      "- **Most Memory Efficient**: Time-LLM-TinyBERT (Distilled) (1002.3MB)\n",
      "\n",
      "## üì± Edge Deployment Readiness\n",
      "- üü° **Feasible**: 5 models\n",
      "- üî¥ **Challenging**: 1 models\n",
      "\n",
      "---\n",
      "*Report generated by Enhanced LLM Efficiency Analysis Tool*\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive efficiency analysis report\n",
    "summary_report = generate_efficiency_report(inference_data)\n",
    "print(\"‚úÖ Summary report generated!\")\n",
    "print(\"=\" * 50)\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef1fe8",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Generate Reports\n",
    "\n",
    "Create analysis reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dcabfaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found distillation results at: /home/amma/LLM-TIME/efficiency_experiments/distillation_experiments/pipeline_results.csv\n",
      "Loaded distillation data: 1 records\n"
     ]
    }
   ],
   "source": [
    "# Load distillation results using working analyzer\n",
    "distillation_data = training_analyzer.load_distillation_results()\n",
    "print(f\"Loaded distillation data: {len(distillation_data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9a8bb51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference data loaded: (44, 49)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>mode</th>\n",
       "      <th>report_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>model_dtype</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>avg_inference_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>gpu_reserved_mb</th>\n",
       "      <th>nvidia_system_vram_mb</th>\n",
       "      <th>nvidia_avg_system_vram_mb</th>\n",
       "      <th>peak_memory_utilization_percent</th>\n",
       "      <th>average_memory_utilization_percent</th>\n",
       "      <th>average_temperature_celsius</th>\n",
       "      <th>gpu_measurement_count</th>\n",
       "      <th>nvidia_ml_measurement_count</th>\n",
       "      <th>system_average_ram_mb</th>\n",
       "      <th>ram_measurements_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-base</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.184570</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>84.120291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-base</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.184570</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>84.120291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-tiny</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.022461</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>37.162438</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chronos_inference_ohiot1dm</td>\n",
       "      <td>chronos-t5-tiny</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.022461</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>37.162438</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_llm_inference_ohiot1dm</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>inference</td>\n",
       "      <td>efficiency_reports</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.143242</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>Model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               experiment_type       model_name       mode  \\\n",
       "0   chronos_inference_ohiot1dm  chronos-t5-base  inference   \n",
       "1   chronos_inference_ohiot1dm  chronos-t5-base  inference   \n",
       "2   chronos_inference_ohiot1dm  chronos-t5-tiny  inference   \n",
       "3   chronos_inference_ohiot1dm  chronos-t5-tiny  inference   \n",
       "4  time_llm_inference_ohiot1dm            LLAMA  inference   \n",
       "\n",
       "          report_type                                          file_path  \\\n",
       "0  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "1  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "2  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "3  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "4  efficiency_reports  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "\n",
       "   total_parameters  model_size_mb    model_dtype model_architecture  \\\n",
       "0         201374976     768.184570  torch.float32       ChronosModel   \n",
       "1         201374976     768.184570  torch.float32       ChronosModel   \n",
       "2           8394496      32.022461  torch.float32       ChronosModel   \n",
       "3           8394496      32.022461  torch.float32       ChronosModel   \n",
       "4        6642504366   25339.143242  torch.float32              Model   \n",
       "\n",
       "   avg_inference_time_ms  ...  gpu_reserved_mb  nvidia_system_vram_mb  \\\n",
       "0              84.120291  ...              NaN                    NaN   \n",
       "1              84.120291  ...              NaN                    NaN   \n",
       "2              37.162438  ...              NaN                    NaN   \n",
       "3              37.162438  ...              NaN                    NaN   \n",
       "4                    NaN  ...              NaN                    NaN   \n",
       "\n",
       "   nvidia_avg_system_vram_mb  peak_memory_utilization_percent  \\\n",
       "0                        NaN                              NaN   \n",
       "1                        NaN                              NaN   \n",
       "2                        NaN                              NaN   \n",
       "3                        NaN                              NaN   \n",
       "4                        NaN                              NaN   \n",
       "\n",
       "   average_memory_utilization_percent  average_temperature_celsius  \\\n",
       "0                                 NaN                          NaN   \n",
       "1                                 NaN                          NaN   \n",
       "2                                 NaN                          NaN   \n",
       "3                                 NaN                          NaN   \n",
       "4                                 NaN                          NaN   \n",
       "\n",
       "   gpu_measurement_count  nvidia_ml_measurement_count  system_average_ram_mb  \\\n",
       "0                    NaN                          NaN                    NaN   \n",
       "1                    NaN                          NaN                    NaN   \n",
       "2                    NaN                          NaN                    NaN   \n",
       "3                    NaN                          NaN                    NaN   \n",
       "4                    NaN                          NaN                    NaN   \n",
       "\n",
       "   ram_measurements_count  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the loaded inference data\n",
    "print(f\"‚úÖ Inference data loaded: {inference_data.shape}\")\n",
    "inference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "959cdb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Energy metrics: (6, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>avg_power_w</th>\n",
       "      <th>energy_per_prediction_wh</th>\n",
       "      <th>daily_energy_moderate_wh</th>\n",
       "      <th>carbon_per_prediction_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>141.282153</td>\n",
       "      <td>2.272605</td>\n",
       "      <td>2272.605198</td>\n",
       "      <td>1.136303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>109.081088</td>\n",
       "      <td>0.838050</td>\n",
       "      <td>838.049538</td>\n",
       "      <td>0.419025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>185.961041</td>\n",
       "      <td>25.226457</td>\n",
       "      <td>25226.457354</td>\n",
       "      <td>12.613229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>175.197675</td>\n",
       "      <td>15.170550</td>\n",
       "      <td>15170.550003</td>\n",
       "      <td>7.585275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>164.152812</td>\n",
       "      <td>9.494360</td>\n",
       "      <td>9494.360427</td>\n",
       "      <td>4.747180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Time-LLM-TinyBERT (Distilled)</td>\n",
       "      <td>55.221700</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>19.500974</td>\n",
       "      <td>0.009750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model_name  avg_power_w  energy_per_prediction_wh  \\\n",
       "0                Chronos-T5-Base   141.282153                  2.272605   \n",
       "1                Chronos-T5-Tiny   109.081088                  0.838050   \n",
       "2                 Time-LLM-LLaMA   185.961041                 25.226457   \n",
       "3                 Time-LLM-GPT-2   175.197675                 15.170550   \n",
       "4                  Time-LLM-BERT   164.152812                  9.494360   \n",
       "5  Time-LLM-TinyBERT (Distilled)    55.221700                  0.019501   \n",
       "\n",
       "   daily_energy_moderate_wh  carbon_per_prediction_g  \n",
       "0               2272.605198                 1.136303  \n",
       "1                838.049538                 0.419025  \n",
       "2              25226.457354                12.613229  \n",
       "3              15170.550003                 7.585275  \n",
       "4               9494.360427                 4.747180  \n",
       "5                 19.500974                 0.009750  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show energy metrics table\n",
    "print(f\"‚úÖ Energy metrics: {energy_metrics.shape}\")\n",
    "energy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c6748796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference summary: (6, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>total_records</th>\n",
       "      <th>records_with_timing</th>\n",
       "      <th>records_with_memory</th>\n",
       "      <th>records_with_power</th>\n",
       "      <th>avg_inference_time_ms</th>\n",
       "      <th>min_inference_time_ms</th>\n",
       "      <th>max_inference_time_ms</th>\n",
       "      <th>std_inference_time_ms</th>\n",
       "      <th>throughput_predictions_per_sec</th>\n",
       "      <th>...</th>\n",
       "      <th>estimated_gpu_latency_ms</th>\n",
       "      <th>peak_power_usage_watts</th>\n",
       "      <th>peak_gpu_utilization_percent</th>\n",
       "      <th>average_gpu_utilization_percent</th>\n",
       "      <th>nvidia_system_vram_mb</th>\n",
       "      <th>nvidia_avg_system_vram_mb</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>edge_feasibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chronos-T5-Base</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>57908.083340</td>\n",
       "      <td>82.726276</td>\n",
       "      <td>390011.057777</td>\n",
       "      <td>146485.441696</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>...</td>\n",
       "      <td>20.14</td>\n",
       "      <td>220.465000</td>\n",
       "      <td>48.333333</td>\n",
       "      <td>19.053384</td>\n",
       "      <td>6678.53125</td>\n",
       "      <td>5188.678862</td>\n",
       "      <td>201374976</td>\n",
       "      <td>768.184570</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronos-T5-Tiny</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>27658.124667</td>\n",
       "      <td>35.835989</td>\n",
       "      <td>179945.969535</td>\n",
       "      <td>67227.326547</td>\n",
       "      <td>0.036156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>158.254667</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>4.633381</td>\n",
       "      <td>1919.53125</td>\n",
       "      <td>1556.946576</td>\n",
       "      <td>8394496</td>\n",
       "      <td>32.022461</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time-LLM-LLaMA</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>488356.303489</td>\n",
       "      <td>91435.458492</td>\n",
       "      <td>885277.148487</td>\n",
       "      <td>458324.713413</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>...</td>\n",
       "      <td>664.25</td>\n",
       "      <td>208.105000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>45.554722</td>\n",
       "      <td>34006.87500</td>\n",
       "      <td>33654.572793</td>\n",
       "      <td>6642504366</td>\n",
       "      <td>25339.143242</td>\n",
       "      <td>Model</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time-LLM-GPT-2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>311727.767871</td>\n",
       "      <td>2425.929184</td>\n",
       "      <td>621029.606558</td>\n",
       "      <td>357150.999654</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>...</td>\n",
       "      <td>31.71</td>\n",
       "      <td>200.021000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>36.444844</td>\n",
       "      <td>6526.87500</td>\n",
       "      <td>6433.397086</td>\n",
       "      <td>317055766</td>\n",
       "      <td>1209.471764</td>\n",
       "      <td>Model</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time-LLM-BERT</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>208218.775673</td>\n",
       "      <td>2250.552981</td>\n",
       "      <td>414186.998365</td>\n",
       "      <td>237831.617631</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>...</td>\n",
       "      <td>28.24</td>\n",
       "      <td>201.442000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.245594</td>\n",
       "      <td>6547.87500</td>\n",
       "      <td>6455.541232</td>\n",
       "      <td>282363198</td>\n",
       "      <td>1077.130119</td>\n",
       "      <td>Model</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Time-LLM-TinyBERT (Distilled)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1271.302853</td>\n",
       "      <td>1271.302853</td>\n",
       "      <td>1271.302853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786595</td>\n",
       "      <td>...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>77.877000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1199.87500</td>\n",
       "      <td>1120.143750</td>\n",
       "      <td>44998814</td>\n",
       "      <td>171.656853</td>\n",
       "      <td>Model</td>\n",
       "      <td>feasible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model_name  total_records  records_with_timing  \\\n",
       "0                Chronos-T5-Base              7                    7   \n",
       "1                Chronos-T5-Tiny              7                    7   \n",
       "2                 Time-LLM-LLaMA              8                    4   \n",
       "3                 Time-LLM-GPT-2              8                    4   \n",
       "4                  Time-LLM-BERT             10                    4   \n",
       "5  Time-LLM-TinyBERT (Distilled)              4                    2   \n",
       "\n",
       "   records_with_memory  records_with_power  avg_inference_time_ms  \\\n",
       "0                    7                   3           57908.083340   \n",
       "1                    7                   3           27658.124667   \n",
       "2                    8                   4          488356.303489   \n",
       "3                    8                   4          311727.767871   \n",
       "4                   10                   4          208218.775673   \n",
       "5                    4                   2            1271.302853   \n",
       "\n",
       "   min_inference_time_ms  max_inference_time_ms  std_inference_time_ms  \\\n",
       "0              82.726276          390011.057777          146485.441696   \n",
       "1              35.835989          179945.969535           67227.326547   \n",
       "2           91435.458492          885277.148487          458324.713413   \n",
       "3            2425.929184          621029.606558          357150.999654   \n",
       "4            2250.552981          414186.998365          237831.617631   \n",
       "5            1271.302853            1271.302853               0.000000   \n",
       "\n",
       "   throughput_predictions_per_sec  ...  estimated_gpu_latency_ms  \\\n",
       "0                        0.017269  ...                     20.14   \n",
       "1                        0.036156  ...                      0.84   \n",
       "2                        0.002048  ...                    664.25   \n",
       "3                        0.003208  ...                     31.71   \n",
       "4                        0.004803  ...                     28.24   \n",
       "5                        0.786595  ...                      4.50   \n",
       "\n",
       "   peak_power_usage_watts  peak_gpu_utilization_percent  \\\n",
       "0              220.465000                     48.333333   \n",
       "1              158.254667                     41.666667   \n",
       "2              208.105000                     79.500000   \n",
       "3              200.021000                     47.000000   \n",
       "4              201.442000                     50.000000   \n",
       "5               77.877000                     14.000000   \n",
       "\n",
       "   average_gpu_utilization_percent  nvidia_system_vram_mb  \\\n",
       "0                        19.053384             6678.53125   \n",
       "1                         4.633381             1919.53125   \n",
       "2                        45.554722            34006.87500   \n",
       "3                        36.444844             6526.87500   \n",
       "4                        30.245594             6547.87500   \n",
       "5                         1.700000             1199.87500   \n",
       "\n",
       "   nvidia_avg_system_vram_mb  total_parameters  model_size_mb  \\\n",
       "0                5188.678862         201374976     768.184570   \n",
       "1                1556.946576           8394496      32.022461   \n",
       "2               33654.572793        6642504366   25339.143242   \n",
       "3                6433.397086         317055766    1209.471764   \n",
       "4                6455.541232         282363198    1077.130119   \n",
       "5                1120.143750          44998814     171.656853   \n",
       "\n",
       "   model_architecture  edge_feasibility  \n",
       "0        ChronosModel          feasible  \n",
       "1        ChronosModel          feasible  \n",
       "2               Model       challenging  \n",
       "3               Model          feasible  \n",
       "4               Model          feasible  \n",
       "5               Model          feasible  \n",
       "\n",
       "[6 rows x 31 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show inference summary table  \n",
    "print(f\"‚úÖ Inference summary: {inference_metrics.shape}\")\n",
    "inference_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2377084",
   "metadata": {},
   "source": [
    "## üìä Quick Data Inspection\n",
    "\n",
    "Check what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "03b765fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>training_time_hours</th>\n",
       "      <th>training_time_minutes</th>\n",
       "      <th>epochs_completed</th>\n",
       "      <th>final_train_loss</th>\n",
       "      <th>final_val_loss</th>\n",
       "      <th>best_train_loss</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>peak_ram_mb</th>\n",
       "      <th>avg_power_w</th>\n",
       "      <th>peak_power_w</th>\n",
       "      <th>peak_gpu_mb</th>\n",
       "      <th>avg_gpu_util_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>chronos-t5-base</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1531.207031</td>\n",
       "      <td>207.952649</td>\n",
       "      <td>377.355</td>\n",
       "      <td>776.309570</td>\n",
       "      <td>51.754746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>chronos-t5-tiny</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1531.285156</td>\n",
       "      <td>127.787712</td>\n",
       "      <td>259.712</td>\n",
       "      <td>40.147461</td>\n",
       "      <td>12.288202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2742.804688</td>\n",
       "      <td>134.960307</td>\n",
       "      <td>230.475</td>\n",
       "      <td>1832.453125</td>\n",
       "      <td>10.802920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2730.183594</td>\n",
       "      <td>167.733600</td>\n",
       "      <td>292.939</td>\n",
       "      <td>2206.436523</td>\n",
       "      <td>25.664516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>/home/amma/LLM-TIME/efficiency_experiments/exp...</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1907.636719</td>\n",
       "      <td>300.858576</td>\n",
       "      <td>356.820</td>\n",
       "      <td>39177.378906</td>\n",
       "      <td>76.196970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment_type                                          file_path  \\\n",
       "0        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "1        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "2        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "3        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "4        training  /home/amma/LLM-TIME/efficiency_experiments/exp...   \n",
       "\n",
       "        model_name training_time_hours training_time_minutes epochs_completed  \\\n",
       "0  chronos-t5-base                None                  None             None   \n",
       "1  chronos-t5-tiny                None                  None             None   \n",
       "2             BERT                None                  None             None   \n",
       "3             GPT2                None                  None             None   \n",
       "4            LLAMA                None                  None             None   \n",
       "\n",
       "  final_train_loss final_val_loss best_train_loss best_val_loss  peak_ram_mb  \\\n",
       "0             None           None            None          None  1531.207031   \n",
       "1             None           None            None          None  1531.285156   \n",
       "2             None           None            None          None  2742.804688   \n",
       "3             None           None            None          None  2730.183594   \n",
       "4             None           None            None          None  1907.636719   \n",
       "\n",
       "   avg_power_w  peak_power_w   peak_gpu_mb  avg_gpu_util_percent  \n",
       "0   207.952649       377.355    776.309570             51.754746  \n",
       "1   127.787712       259.712     40.147461             12.288202  \n",
       "2   134.960307       230.475   1832.453125             10.802920  \n",
       "3   167.733600       292.939   2206.436523             25.664516  \n",
       "4   300.858576       356.820  39177.378906             76.196970  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Distillation data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pipeline_id</th>\n",
       "      <th>pipeline_dir</th>\n",
       "      <th>patient_ids</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>teacher_model</th>\n",
       "      <th>student_model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>distilled_mape</th>\n",
       "      <th>distillation_time</th>\n",
       "      <th>distillation_status</th>\n",
       "      <th>teacher_to_distilled_rmse_improvement</th>\n",
       "      <th>teacher_to_distilled_rmse_improvement_pct</th>\n",
       "      <th>student_to_distilled_rmse_improvement</th>\n",
       "      <th>student_to_distilled_rmse_improvement_pct</th>\n",
       "      <th>pipeline_status</th>\n",
       "      <th>total_runtime</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-21 08:02:17</td>\n",
       "      <td>patient_570</td>\n",
       "      <td>distillation_experiments/pipeline_runs/pipelin...</td>\n",
       "      <td>570</td>\n",
       "      <td>ohiot1dm</td>\n",
       "      <td>831363</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>prajjwal1/bert-tiny</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>5.579967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.097586</td>\n",
       "      <td>0.572448</td>\n",
       "      <td>-0.306728</td>\n",
       "      <td>-1.843003</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>451.0</td>\n",
       "      <td>Patient 570 complete 3-phase pipeline run</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  pipeline_id  \\\n",
       "0  2025-10-21 08:02:17  patient_570   \n",
       "\n",
       "                                        pipeline_dir  patient_ids  \\\n",
       "0  distillation_experiments/pipeline_runs/pipelin...          570   \n",
       "\n",
       "  dataset_name    seed      teacher_model        student_model  learning_rate  \\\n",
       "0     ohiot1dm  831363  bert-base-uncased  prajjwal1/bert-tiny          0.001   \n",
       "\n",
       "   batch_size  ...  distilled_mape  distillation_time  distillation_status  \\\n",
       "0          32  ...        5.579967                NaN            completed   \n",
       "\n",
       "   teacher_to_distilled_rmse_improvement  \\\n",
       "0                               0.097586   \n",
       "\n",
       "   teacher_to_distilled_rmse_improvement_pct  \\\n",
       "0                                   0.572448   \n",
       "\n",
       "   student_to_distilled_rmse_improvement  \\\n",
       "0                              -0.306728   \n",
       "\n",
       "   student_to_distilled_rmse_improvement_pct  pipeline_status  total_runtime  \\\n",
       "0                                  -1.843003          SUCCESS          451.0   \n",
       "\n",
       "                                       notes  \n",
       "0  Patient 570 complete 3-phase pipeline run  \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show training data with pandas display\n",
    "print(\"‚úÖ Training data:\")\n",
    "if isinstance(training_data, pd.DataFrame) and not training_data.empty:\n",
    "    display(training_data.head())\n",
    "else:\n",
    "    print(f\"Training data: {type(training_data)}\")\n",
    "    \n",
    "print(\"‚úÖ Distillation data:\")\n",
    "if isinstance(distillation_data, pd.DataFrame) and not distillation_data.empty:\n",
    "    display(distillation_data.head())\n",
    "else:\n",
    "    print(f\"Distillation data: {type(distillation_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7824424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging TinyBERT GPU measurement extraction...\n"
     ]
    }
   ],
   "source": [
    "# Debug TinyBERT GPU values\n",
    "print(\"Debugging TinyBERT GPU measurement extraction...\")\n",
    "\n",
    "# Find TinyBERT data\n",
    "tinybert_data = None\n",
    "for model_name, data in inference_data.items():\n",
    "    if \"tinybert\" in model_name.lower():\n",
    "        tinybert_data = data\n",
    "        print(f\"Found TinyBERT data under key: {model_name}\")\n",
    "        break\n",
    "\n",
    "if tinybert_data:\n",
    "    print(\"\\nTinyBERT measurements breakdown:\")\n",
    "    \n",
    "    # Check summary calculations\n",
    "    from analysis_utils import calculate_inference_summary\n",
    "    summary = calculate_inference_summary(tinybert_data)\n",
    "    \n",
    "    print(f\"Summary VRAM: {summary.get('avg_vram_mb', 'N/A')}\")\n",
    "    \n",
    "    # Check raw GPU measurements\n",
    "    if 'gpu_vram_metrics' in summary:\n",
    "        vram_metrics = summary['gpu_vram_metrics']\n",
    "        print(f\"VRAM metrics found: {vram_metrics}\")\n",
    "        \n",
    "        if vram_metrics:\n",
    "            avg_vram = sum(vram_metrics) / len(vram_metrics)\n",
    "            print(f\"Calculated avg VRAM: {avg_vram:.1f}MB\")\n",
    "    \n",
    "    # Check raw data structure\n",
    "    print(f\"\\nRaw data keys available: {list(tinybert_data.keys())}\")\n",
    "    \n",
    "    # Look for GPU data in measurements\n",
    "    for measurement in tinybert_data.get('measurements', []):\n",
    "        gpu_data = measurement.get('gpu_memory_usage', {})\n",
    "        if gpu_data:\n",
    "            print(f\"\\nGPU memory in measurement:\")\n",
    "            print(f\"  PyTorch allocated: {gpu_data.get('peak_allocated_mb', 'N/A')}\")\n",
    "            print(f\"  NVIDIA ML used: {gpu_data.get('nvidia_ml', {}).get('peak_used_memory_mb', 'N/A')}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reloaded latex_table_generator module\n"
     ]
    }
   ],
   "source": [
    "# Reload the module to ensure we get the updated version\n",
    "import importlib\n",
    "from utils import latex_table_generator\n",
    "importlib.reload(latex_table_generator)\n",
    "from utils.latex_table_generator import generate_all_tables\n",
    "\n",
    "print(\"üîÑ Reloaded latex_table_generator module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046557b",
   "metadata": {},
   "source": [
    "## üéØ That's it!\n",
    "\n",
    "**No more 79-cell nightmare!** \n",
    "\n",
    "‚úÖ Clean, organized functions  \n",
    "‚úÖ One-click complete analysis  \n",
    "‚úÖ Real experimental data  \n",
    "‚úÖ CPU + GPU metrics  \n",
    "‚úÖ LaTeX tables ready for publication  \n",
    "\n",
    "**All outputs saved to:** `/home/amma/LLM-TIME/notebooks/outputs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
