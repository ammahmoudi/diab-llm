2025-10-08 17:29:17,191 | INFO | logger.py:69 | Logging initialized. Logs for this run will be saved in: ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17
2025-10-08 17:29:17,191 | INFO | logger.py:70 | Log level set to: INFO
2025-10-08 17:29:17,191 | INFO | main.py:92 | Logging initialized at directory: ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17
2025-10-08 17:29:17,191 | INFO | main.py:96 | using fix seed:238822
2025-10-08 17:29:17,192 | INFO | main.py:114 | setting torch dtype to torch.float32
2025-10-08 17:29:17,238 | INFO | data_sets.py:93 | Dataset 'TRAIN' initialized:
2025-10-08 17:29:17,239 | INFO | data_sets.py:94 |  - Total samples after applying 100% and spliting: 10982
2025-10-08 17:29:17,239 | INFO | data_sets.py:101 |  - Sequence Length: 6, Context Length: 6, Prediction Length: 9.
2025-10-08 17:29:17,239 | INFO | data_sets.py:106 |  - Target Feature: target
2025-10-08 17:29:17,239 | INFO | data_sets.py:107 |  - Scaling Enabled: False
2025-10-08 17:29:17,239 | INFO | data_sets.py:108 |  - Validation Split: 0%
2025-10-08 17:29:17,254 | INFO | data_sets.py:93 | Dataset 'TEST' initialized:
2025-10-08 17:29:17,254 | INFO | data_sets.py:94 |  - Total samples after applying 100% and spliting: 2745
2025-10-08 17:29:17,255 | INFO | data_sets.py:101 |  - Sequence Length: 6, Context Length: 6, Prediction Length: 9.
2025-10-08 17:29:17,255 | INFO | data_sets.py:106 |  - Target Feature: target
2025-10-08 17:29:17,255 | INFO | data_sets.py:107 |  - Scaling Enabled: False
2025-10-08 17:29:17,255 | INFO | data_sets.py:108 |  - Validation Split: 0%
2025-10-08 17:29:17,255 | INFO | time_llm.py:31 | Initializing TimeLLM model...
2025-10-08 17:29:20,641 | INFO | spawn.py:60 | x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpd4hvsmq8/test.c -o /tmp/tmpd4hvsmq8/test.o
2025-10-08 17:29:20,659 | INFO | spawn.py:60 | x86_64-linux-gnu-gcc /tmp/tmpd4hvsmq8/test.o -laio -o /tmp/tmpd4hvsmq8/a.out
2025-10-08 17:29:21,104 | INFO | spawn.py:60 | x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpabbvtj56/test.c -o /tmp/tmpabbvtj56/test.o
2025-10-08 17:29:21,119 | INFO | spawn.py:60 | x86_64-linux-gnu-gcc /tmp/tmpabbvtj56/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmpabbvtj56/a.out
2025-10-08 17:29:21,536 | INFO | main.py:479 | üîç Calculating efficiency metrics for TimeLLM...
2025-10-08 17:29:21,536 | INFO | efficiency_calculator.py:37 | Calculating efficiency metrics for time_llm
2025-10-08 17:29:21,722 | WARNING | logging.py:328 | BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
2025-10-08 17:29:21,722 | [93mWARNING[0m | logging.py:328 | BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
2025-10-08 17:29:22,984 | INFO | efficiency_calculator.py:259 | ‚úÖ Efficiency report saved to: distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17/efficiency_report_time_llm_2025-10-08_17-29-22.json
2025-10-08 17:29:22,985 | INFO | main.py:487 | ‚úÖ Efficiency metrics calculated and saved successfully!
2025-10-08 17:29:22,985 | INFO | time_llm.py:96 | Loading model checkpoint from ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/student_distilled.pth
2025-10-08 17:29:23,542 | ERROR | time_llm.py:126 | RuntimeError encountered while loading model: Error(s) in loading state_dict for Model:
	Unexpected key(s) in state_dict: "llm_model.encoder.layer.4.attention.self.query.weight", "llm_model.encoder.layer.4.attention.self.query.bias", "llm_model.encoder.layer.4.attention.self.key.weight", "llm_model.encoder.layer.4.attention.self.key.bias", "llm_model.encoder.layer.4.attention.self.value.weight", "llm_model.encoder.layer.4.attention.self.value.bias", "llm_model.encoder.layer.4.attention.output.dense.weight", "llm_model.encoder.layer.4.attention.output.dense.bias", "llm_model.encoder.layer.4.attention.output.LayerNorm.weight", "llm_model.encoder.layer.4.attention.output.LayerNorm.bias", "llm_model.encoder.layer.4.intermediate.dense.weight", "llm_model.encoder.layer.4.intermediate.dense.bias", "llm_model.encoder.layer.4.output.dense.weight", "llm_model.encoder.layer.4.output.dense.bias", "llm_model.encoder.layer.4.output.LayerNorm.weight", "llm_model.encoder.layer.4.output.LayerNorm.bias", "llm_model.encoder.layer.5.attention.self.query.weight", "llm_model.encoder.layer.5.attention.self.query.bias", "llm_model.encoder.layer.5.attention.self.key.weight", "llm_model.encoder.layer.5.attention.self.key.bias", "llm_model.encoder.layer.5.attention.self.value.weight", "llm_model.encoder.layer.5.attention.self.value.bias", "llm_model.encoder.layer.5.attention.output.dense.weight", "llm_model.encoder.layer.5.attention.output.dense.bias", "llm_model.encoder.layer.5.attention.output.LayerNorm.weight", "llm_model.encoder.layer.5.attention.output.LayerNorm.bias", "llm_model.encoder.layer.5.intermediate.dense.weight", "llm_model.encoder.layer.5.intermediate.dense.bias", "llm_model.encoder.layer.5.output.dense.weight", "llm_model.encoder.layer.5.output.dense.bias", "llm_model.encoder.layer.5.output.LayerNorm.weight", "llm_model.encoder.layer.5.output.LayerNorm.bias", "llm_model.encoder.layer.6.attention.self.query.weight", "llm_model.encoder.layer.6.attention.self.query.bias", "llm_model.encoder.layer.6.attention.self.key.weight", "llm_model.encoder.layer.6.attention.self.key.bias", "llm_model.encoder.layer.6.attention.self.value.weight", "llm_model.encoder.layer.6.attention.self.value.bias", "llm_model.encoder.layer.6.attention.output.dense.weight", "llm_model.encoder.layer.6.attention.output.dense.bias", "llm_model.encoder.layer.6.attention.output.LayerNorm.weight", "llm_model.encoder.layer.6.attention.output.LayerNorm.bias", "llm_model.encoder.layer.6.intermediate.dense.weight", "llm_model.encoder.layer.6.intermediate.dense.bias", "llm_model.encoder.layer.6.output.dense.weight", "llm_model.encoder.layer.6.output.dense.bias", "llm_model.encoder.layer.6.output.LayerNorm.weight", "llm_model.encoder.layer.6.output.LayerNorm.bias", "llm_model.encoder.layer.7.attention.self.query.weight", "llm_model.encoder.layer.7.attention.self.query.bias", "llm_model.encoder.layer.7.attention.self.key.weight", "llm_model.encoder.layer.7.attention.self.key.bias", "llm_model.encoder.layer.7.attention.self.value.weight", "llm_model.encoder.layer.7.attention.self.value.bias", "llm_model.encoder.layer.7.attention.output.dense.weight", "llm_model.encoder.layer.7.attention.output.dense.bias", "llm_model.encoder.layer.7.attention.output.LayerNorm.weight", "llm_model.encoder.layer.7.attention.output.LayerNorm.bias", "llm_model.encoder.layer.7.intermediate.dense.weight", "llm_model.encoder.layer.7.intermediate.dense.bias", "llm_model.encoder.layer.7.output.dense.weight", "llm_model.encoder.layer.7.output.dense.bias", "llm_model.encoder.layer.7.output.LayerNorm.weight", "llm_model.encoder.layer.7.output.LayerNorm.bias", "llm_model.encoder.layer.8.attention.self.query.weight", "llm_model.encoder.layer.8.attention.self.query.bias", "llm_model.encoder.layer.8.attention.self.key.weight", "llm_model.encoder.layer.8.attention.self.key.bias", "llm_model.encoder.layer.8.attention.self.value.weight", "llm_model.encoder.layer.8.attention.self.value.bias", "llm_model.encoder.layer.8.attention.output.dense.weight", "llm_model.encoder.layer.8.attention.output.dense.bias", "llm_model.encoder.layer.8.attention.output.LayerNorm.weight", "llm_model.encoder.layer.8.attention.output.LayerNorm.bias", "llm_model.encoder.layer.8.intermediate.dense.weight", "llm_model.encoder.layer.8.intermediate.dense.bias", "llm_model.encoder.layer.8.output.dense.weight", "llm_model.encoder.layer.8.output.dense.bias", "llm_model.encoder.layer.8.output.LayerNorm.weight", "llm_model.encoder.layer.8.output.LayerNorm.bias", "llm_model.encoder.layer.9.attention.self.query.weight", "llm_model.encoder.layer.9.attention.self.query.bias", "llm_model.encoder.layer.9.attention.self.key.weight", "llm_model.encoder.layer.9.attention.self.key.bias", "llm_model.encoder.layer.9.attention.self.value.weight", "llm_model.encoder.layer.9.attention.self.value.bias", "llm_model.encoder.layer.9.attention.output.dense.weight", "llm_model.encoder.layer.9.attention.output.dense.bias", "llm_model.encoder.layer.9.attention.output.LayerNorm.weight", "llm_model.encoder.layer.9.attention.output.LayerNorm.bias", "llm_model.encoder.layer.9.intermediate.dense.weight", "llm_model.encoder.layer.9.intermediate.dense.bias", "llm_model.encoder.layer.9.output.dense.weight", "llm_model.encoder.layer.9.output.dense.bias", "llm_model.encoder.layer.9.output.LayerNorm.weight", "llm_model.encoder.layer.9.output.LayerNorm.bias", "llm_model.encoder.layer.10.attention.self.query.weight", "llm_model.encoder.layer.10.attention.self.query.bias", "llm_model.encoder.layer.10.attention.self.key.weight", "llm_model.encoder.layer.10.attention.self.key.bias", "llm_model.encoder.layer.10.attention.self.value.weight", "llm_model.encoder.layer.10.attention.self.value.bias", "llm_model.encoder.layer.10.attention.output.dense.weight", "llm_model.encoder.layer.10.attention.output.dense.bias", "llm_model.encoder.layer.10.attention.output.LayerNorm.weight", "llm_model.encoder.layer.10.attention.output.LayerNorm.bias", "llm_model.encoder.layer.10.intermediate.dense.weight", "llm_model.encoder.layer.10.intermediate.dense.bias", "llm_model.encoder.layer.10.output.dense.weight", "llm_model.encoder.layer.10.output.dense.bias", "llm_model.encoder.layer.10.output.LayerNorm.weight", "llm_model.encoder.layer.10.output.LayerNorm.bias", "llm_model.encoder.layer.11.attention.self.query.weight", "llm_model.encoder.layer.11.attention.self.query.bias", "llm_model.encoder.layer.11.attention.self.key.weight", "llm_model.encoder.layer.11.attention.self.key.bias", "llm_model.encoder.layer.11.attention.self.value.weight", "llm_model.encoder.layer.11.attention.self.value.bias", "llm_model.encoder.layer.11.attention.output.dense.weight", "llm_model.encoder.layer.11.attention.output.dense.bias", "llm_model.encoder.layer.11.attention.output.LayerNorm.weight", "llm_model.encoder.layer.11.attention.output.LayerNorm.bias", "llm_model.encoder.layer.11.intermediate.dense.weight", "llm_model.encoder.layer.11.intermediate.dense.bias", "llm_model.encoder.layer.11.output.dense.weight", "llm_model.encoder.layer.11.output.dense.bias", "llm_model.encoder.layer.11.output.LayerNorm.weight", "llm_model.encoder.layer.11.output.LayerNorm.bias". 
	size mismatch for word_embeddings: copying a param with shape torch.Size([30522, 768]) from checkpoint, the shape in current model is torch.Size([30522, 312]).
	size mismatch for llm_model.embeddings.word_embeddings.weight: copying a param with shape torch.Size([30522, 768]) from checkpoint, the shape in current model is torch.Size([30522, 312]).
	size mismatch for llm_model.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([512, 312]).
	size mismatch for llm_model.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 312]).
	size mismatch for llm_model.embeddings.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.embeddings.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1200, 312]).
	size mismatch for llm_model.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1200]).
	size mismatch for llm_model.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([312, 1200]).
	size mismatch for llm_model.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1200, 312]).
	size mismatch for llm_model.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1200]).
	size mismatch for llm_model.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([312, 1200]).
	size mismatch for llm_model.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1200, 312]).
	size mismatch for llm_model.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1200]).
	size mismatch for llm_model.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([312, 1200]).
	size mismatch for llm_model.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1200, 312]).
	size mismatch for llm_model.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1200]).
	size mismatch for llm_model.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([312, 1200]).
	size mismatch for llm_model.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for llm_model.pooler.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([312, 312]).
	size mismatch for llm_model.pooler.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
	size mismatch for reprogramming_layer.key_projection.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([256, 312]).
	size mismatch for reprogramming_layer.value_projection.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([256, 312]).
	size mismatch for reprogramming_layer.out_projection.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([312, 256]).
	size mismatch for reprogramming_layer.out_projection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([312]).
2025-10-08 17:29:23,583 | INFO | time_llm.py:188 | Model checkpoint loaded successfully.
2025-10-08 17:29:23,587 | INFO | main.py:505 | üìä Starting real inference performance measurement...
2025-10-08 17:29:23,656 | INFO | time_llm.py:326 | Efficiency metrics will be calculated automatically
2025-10-08 17:29:28,107 | INFO | result_saver.py:82 | Results saved to ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17/inference_results.csv
2025-10-08 17:29:28,280 | INFO | result_saver.py:87 | Reformatted results saved to ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17/inference_results_reformatted.csv
2025-10-08 17:29:34,698 | INFO | result_saver.py:98 | Plots saved to ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17/plots
2025-10-08 17:29:34,796 | INFO | main.py:531 | üìà Real performance report saved to: ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17/real_performance_report_time_llm_2025-10-08_17-29-34.json
2025-10-08 17:29:34,796 | INFO | main.py:532 | ‚úÖ Captured 1 real inference measurements
2025-10-08 17:29:34,796 | INFO | main.py:539 | Metric results: {'rmse': 22.852802, 'mae': 16.473343, 'mape': 0.083620735}
2025-10-08 17:29:34,797 | INFO | main.py:827 | ‚úÖ Comprehensive performance report saved: ./distillation_experiments/distillation_runs/distill_2025-10-08_17-24-20/inference_logs/logs_2025-10-08_17-29-17/comprehensive_performance_report_time_llm_comprehensive_2025-10-08_17-29-34.json
