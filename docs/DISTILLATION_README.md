# ğŸ§  Knowledge Distillation Pipeline

## Overview

The Knowledge Distillation Pipeline enables you to distill knowledge from larger, more capable teacher models (like BERT-base) to smaller, more efficient student models (like TinyBERT). This technique maintains high performance while significantly reducing model size and computational requirements.

## ğŸš€ Quick Start

### Single Patient Pipeline
```bash
# Run complete 3-phase pipeline for one patient
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 570 \
  --dataset ohiot1dm \
  --seed 42 \
  --teacher-epochs 1 \
  --student-epochs 1 \
  --distill-epochs 1
```

### Multi-Patient Pipeline
```bash
# Run complete 3-phase pipeline for multiple patients
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 570,584 \
  --dataset ohiot1dm \
  --seed 42 \
  --teacher-epochs 1 \
  --student-epochs 1 \
  --distill-epochs 1
```

### Dry Run (Check Configuration)
```bash
# Preview what the pipeline will do without running training
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 570,584 \
  --dataset ohiot1dm \
  --seed 42 \
  --teacher-epochs 1 \
  --student-epochs 1 \
  --distill-epochs 1 \
  --dry-run
```

## ğŸ“‹ Pipeline Phases

The distillation pipeline consists of three phases that run sequentially for each patient:

### Phase 1: Teacher Training ğŸ“
- Trains the large teacher model (e.g., BERT-base-uncased)
- Saves trained model checkpoints for knowledge transfer
- Generates comprehensive performance metrics (RMSE, MAE, MAPE)
- Creates training summaries and efficiency reports

### Phase 2: Student Baseline ğŸ‘¨â€ğŸ“
- Trains the smaller student model independently (e.g., prajjwal1/bert-tiny)
- Establishes baseline performance without teacher knowledge
- Saves student model checkpoints and configuration
- Records baseline metrics for comparison

### Phase 3: Knowledge Distillation ğŸ§ 
- Transfers knowledge from trained teacher to student model
- Uses distillation parameters (alpha, beta, temperature, KL weight)
- Combines teacher predictions with ground truth for student training
- Produces final distilled model with improved performance

## ğŸ”§ Parameters

### Required Parameters
- `--teacher`: Teacher model name (e.g., bert-base-uncased)
- `--student`: Student model name (e.g., prajjwal1/bert-tiny)
- `--patients`: Patient IDs (comma-separated for multiple patients)
- `--dataset`: Dataset name (ohiot1dm, d1namo)
- `--seed`: Random seed for reproducibility
- `--teacher-epochs`: Number of epochs for teacher training
- `--student-epochs`: Number of epochs for student baseline training
- `--distill-epochs`: Number of epochs for knowledge distillation

### Optional Parameters
- `--lr`: Learning rate (default: 0.001)
- `--batch-size`: Batch size (default: 32)
- `--alpha`: Distillation loss weight (default: 0.5)
- `--beta`: Ground truth loss weight (default: 0.5)
- `--kl-weight`: KL divergence loss weight (default: 0.1)
- `--temperature`: Softmax temperature for distillation (default: 3.0)
- `--dry-run`: Preview mode without actual training

## ğŸ¯ Supported Models

### Teacher Models (HuggingFace Compatible)
- `bert-base-uncased` - Standard BERT base model
- `bert-large-uncased` - Large BERT model
- `distilbert-base-uncased` - DistilBERT model
- `roberta-base` - RoBERTa base model

### Student Models (HuggingFace Compatible)
- `prajjwal1/bert-tiny` - 2-layer, 128-hidden BERT
- `prajjwal1/bert-mini` - 4-layer, 256-hidden BERT  
- `prajjwal1/bert-small` - 4-layer, 512-hidden BERT
- `prajjwal1/bert-medium` - 8-layer, 512-hidden BERT

### Auto-Detection
The pipeline automatically handles HuggingFace model names and applies appropriate configurations. Model names with forward slashes are sanitized for safe file operations.

## ğŸ—‚ï¸ Output Structure

For each pipeline run, the following directory structure is created:

```
distillation_experiments/
â”œâ”€â”€ pipeline_runs/
â”‚   â””â”€â”€ pipeline_2025-10-16_20-45-27/          # Timestamped pipeline run
â”‚       â”œâ”€â”€ patient_570/                        # Individual patient results
â”‚       â”‚   â”œâ”€â”€ phase_1_teacher/                # Teacher training outputs
â”‚       â”‚   â”‚   â”œâ”€â”€ config_teacher_*.gin        # Teacher configuration
â”‚       â”‚   â”‚   â”œâ”€â”€ teacher_training_summary.json
â”‚       â”‚   â”‚   â””â”€â”€ bert_570_1epochs/           # Model logs and checkpoints
â”‚       â”‚   â”œâ”€â”€ phase_2_student/                # Student baseline outputs
â”‚       â”‚   â”‚   â”œâ”€â”€ config_student_*.gin        # Student configuration
â”‚       â”‚   â”‚   â”œâ”€â”€ student_baseline_summary.json
â”‚       â”‚   â”‚   â””â”€â”€ tinybert_570_1epochs/       # Model logs and checkpoints
â”‚       â”‚   â””â”€â”€ phase_3_distillation/           # Distillation outputs
â”‚       â”‚       â”œâ”€â”€ config_distill_*.gin        # Distillation configuration
â”‚       â”‚       â”œâ”€â”€ distillation_summary.json  # Final metrics
â”‚       â”‚       â””â”€â”€ bert_to_bert_tiny_570/      # Distilled model and logs
â”‚       â””â”€â”€ patient_584/                        # Additional patients...
â”œâ”€â”€ pipeline_results.csv                        # Comprehensive CSV results
â””â”€â”€ pipeline_csv_logger.py                     # CSV logging utility
```

## ğŸ“Š Results & Metrics

### Automatic CSV Logging
After each patient's complete pipeline run, results are automatically logged to `distillation_experiments/pipeline_results.csv` with the following information:

- **Pipeline Configuration**: Teacher/student models, hyperparameters, patient IDs
- **Phase 1 Metrics**: Teacher RMSE, MAE, MAPE performance
- **Phase 2 Metrics**: Student baseline RMSE, MAE, MAPE performance  
- **Phase 3 Metrics**: Distilled model RMSE, MAE, MAPE performance
- **Improvement Analysis**: Performance improvements vs teacher and baseline
- **Runtime Information**: Total pipeline execution time
- **Status Tracking**: Success/failure status for each phase

### Example Results
```csv
timestamp,patient_ids,teacher_rmse,student_baseline_rmse,distilled_rmse,teacher_to_distilled_improvement_pct
2025-10-16 20:51:59,570,17.234,17.418,16.463,4.47%
2025-10-16 20:57:50,584,26.608,26.822,26.430,0.67%
```

### Performance Expectations
- **Successful Distillation**: Distilled model typically achieves 3-8% RMSE improvement over teacher
- **Model Compression**: 80-95% reduction in model parameters (e.g., BERT-base 110M â†’ TinyBERT 4.4M)
- **Speed Improvement**: 5-15x faster inference compared to teacher model
- **Memory Reduction**: 70-90% reduction in memory usage

## ğŸ”¬ Multi-Patient Execution

When multiple patients are specified (e.g., `--patients 570,584`), the pipeline:

1. **Sequential Processing**: Runs all 3 phases for patient 570, then all 3 phases for patient 584
2. **Individual Logging**: Each patient's results are logged to CSV immediately after completion
3. **Isolated Execution**: Each patient's training is completely independent
4. **Organized Storage**: Each patient gets their own directory structure
5. **Progress Tracking**: Clear progress indicators show which patient is being processed

This approach ensures:
- âœ… **Fault Tolerance**: If one patient fails, others can still complete
- âœ… **Real-time Results**: CSV is updated after each patient completion
- âœ… **Resource Management**: Memory is freed between patients
- âœ… **Easy Analysis**: Individual patient results are clearly separated

## ğŸ“Š Available Datasets

### OhioT1DM (Recommended for Research)
- **Patients**: 540, 559, 563, 570, 575, 588, 591, 596
- **Use Cases**: Research experiments, production training
- **Characteristics**: Larger datasets, longer training times, more robust results

### D1NAMO (Good for Testing)
- **Patients**: 001, 002, 003, 004, 005, 006, 007
- **Use Cases**: Quick testing, development, proof of concept
- **Characteristics**: Smaller datasets, faster training, good for validation

### Data Types
- `raw_standardized`: Standard preprocessed data (recommended for most use cases)
- `noisy_standardized`: Data with added noise for robustness testing
- `missing_data_*`: Data with simulated missing values for imputation research

## ğŸ›ï¸ Advanced Usage

### Custom Hyperparameters
```bash
# Fine-tune distillation parameters
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 570,584 \
  --dataset ohiot1dm \
  --seed 42 \
  --teacher-epochs 3 \
  --student-epochs 2 \
  --distill-epochs 5 \
  --lr 0.0001 \
  --batch-size 16 \
  --alpha 0.7 \
  --beta 0.3 \
  --temperature 5.0
```

### Different Model Combinations
```bash
# BERT-base â†’ BERT-mini
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-mini \
  --patients 570 \
  --dataset ohiot1dm \
  --seed 42 \
  --teacher-epochs 2 \
  --student-epochs 2 \
  --distill-epochs 3

# DistilBERT â†’ TinyBERT  
bash distill_pipeline.sh \
  --teacher distilbert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 584 \
  --dataset d1namo \
  --seed 238822 \
  --teacher-epochs 2 \
  --student-epochs 2 \
  --distill-epochs 3
```

### Development vs Production Workflows

#### Development Workflow
```bash
# 1. Start with small dataset and single epoch
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 001 \
  --dataset d1namo \
  --seed 42 \
  --teacher-epochs 1 \
  --student-epochs 1 \
  --distill-epochs 1 \
  --dry-run

# 2. Test with real training
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 001 \
  --dataset d1namo \
  --seed 42 \
  --teacher-epochs 1 \
  --student-epochs 1 \
  --distill-epochs 1
```

#### Production Workflow
```bash
# High-quality training with more epochs and multiple patients
bash distill_pipeline.sh \
  --teacher bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 570,575,588,591 \
  --dataset ohiot1dm \
  --seed 42 \
  --teacher-epochs 10 \
  --student-epochs 8 \
  --distill-epochs 5 \
  --lr 0.0001 \
  --batch-size 32
```

## ğŸ› Troubleshooting

### Common Issues

#### Pipeline Fails to Start
```bash
# Check if virtual environment is activated
source venv/bin/activate

# Verify dependencies are installed
pip install -r requirements.txt

# Check GPU availability (optional but recommended)
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

#### Model Loading Errors
```bash
# For HuggingFace model access issues
pip install --upgrade transformers
pip install --upgrade torch

# For model name issues (forward slashes)
# The pipeline automatically sanitizes model names, but verify the model exists:
# https://huggingface.co/prajjwal1/bert-tiny
```

#### Memory Issues
```bash
# Reduce batch size for limited GPU memory
bash distill_pipeline.sh ... --batch-size 8

# Use CPU if GPU memory is insufficient (slower but works)
# Edit configs to set device: 'cpu' instead of 'cuda'
```

#### CSV Logging Issues
```bash
# Check permissions on distillation_experiments directory
ls -la distillation_experiments/

# Manually run CSV logger to test
python distillation/scripts/pipeline_csv_logger.py --help
```

### Performance Optimization

#### GPU Optimization
- Use batch sizes that maximize GPU utilization (16, 32, 64)
- Monitor GPU memory usage during training
- Consider mixed precision training for larger models

#### Training Speed
- Start with fewer epochs (1-2) for quick validation
- Use smaller student models (bert-tiny) for faster training
- Enable gradient checkpointing for memory efficiency

## ğŸ“ˆ Best Practices

### Experiment Design
1. **Start Small**: Test with single patient and 1 epoch first
2. **Validate Setup**: Use `--dry-run` to check configurations
3. **Monitor Progress**: Watch CSV updates to track success
4. **Save Results**: Keep CSV files for analysis and comparison

### Model Selection
1. **Teacher Size**: Larger teachers provide better knowledge but train slower
2. **Student Size**: Choose based on deployment constraints
3. **Architecture Match**: BERT family models work well together
4. **Domain Relevance**: Pre-trained models should match your domain

### Hyperparameter Tuning
1. **Temperature**: Start with 3-5, higher for softer distributions
2. **Alpha/Beta**: Balance distillation vs ground truth (0.5/0.5 baseline)
3. **Learning Rate**: Often lower than standard training (0.0001-0.001)
4. **Epochs**: Teacher > Student â‰¥ Distillation for best results

## ğŸ” Monitoring & Analysis

### Real-time Monitoring
```bash
# Watch CSV file updates
tail -f distillation_experiments/pipeline_results.csv

# Monitor GPU usage during training
watch -n 1 nvidia-smi

# Check pipeline logs
tail -f distillation_experiments/pipeline_runs/*/patient_*/phase_*/*/logs/*/main.log
```

### Post-Training Analysis
```bash
# Load and analyze CSV results
python -c "
import pandas as pd
df = pd.read_csv('distillation_experiments/pipeline_results.csv')
print(df[['patient_ids', 'teacher_rmse', 'distilled_rmse', 'teacher_to_distilled_improvement_pct']])
"

# Compare multiple experiments
python scripts/analyze_distillation_results.py
```

## ğŸš€ What's Next?

After successful knowledge distillation:

1. **Deploy Models**: Use distilled models in production for faster inference
2. **Further Compression**: Apply quantization or pruning techniques
3. **Ensemble Methods**: Combine multiple distilled models
4. **Transfer Learning**: Use distilled models as base for new tasks
5. **Continuous Learning**: Retrain with new data while maintaining efficiency

---

## ğŸ“ Getting Started Checklist

Before running your first distillation experiment:

- [ ] **Environment Setup**
  - [ ] Clone repository: `git clone --recursive https://github.com/ammahmoudi/diab-llm.git`
  - [ ] Create virtual environment: `python3 -m venv venv`
  - [ ] Activate environment: `source venv/bin/activate`
  - [ ] Install dependencies: `pip install -r requirements.txt`

- [ ] **Initial Testing**
  - [ ] Test pipeline setup: `bash distill_pipeline.sh --help`
  - [ ] Run dry run: `bash distill_pipeline.sh ... --dry-run`
  - [ ] Check GPU availability: `python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"`

- [ ] **First Experiment**
  - [ ] Start with small dataset (D1NAMO patient 001)
  - [ ] Use 1 epoch for each phase
  - [ ] Verify results in `distillation_experiments/`
  - [ ] Check CSV logging works correctly

- [ ] **Scale Up**
  - [ ] Move to OhioT1DM dataset for production experiments
  - [ ] Increase epochs for better performance
  - [ ] Test multi-patient execution
  - [ ] Analyze performance improvements in CSV results

For more details on the underlying models and training procedures, see:
- [Time-LLM Documentation](README_timellm_commands.md)
- [Main Project README](../README.md)
- [Chronos Model Documentation](README_chronos_commands.md)

## ğŸš€ Quick Start

### Basic Usage
```bash
# Simple distillation with default parameters
./distill_pipeline.sh --teacher bert-base-uncased --student prajjwal1/bert-tiny --patients 570 --dataset ohiot1dm --seed 42

# Custom training configuration
./distill_pipeline.sh \
    --teacher bert-base-uncased \
    --student prajjwal1/bert-tiny \
    --patients 570,584 \
    --dataset ohiot1dm \
    --seed 42 \
    --teacher-epochs 5 \
    --student-epochs 3 \
    --distill-epochs 3 \
    --lr 0.002 \
    --batch-size 16 \
    --alpha 0.6 \
    --beta 0.4 \
    --kl-weight 0.15 \
    --temperature 4.0

# Dry run (preview commands without execution)
./distill_pipeline.sh --teacher bert-base-uncased --student prajjwal1/bert-tiny --patients 570 --dataset ohiot1dm --dry-run
```

## ğŸ“‹ Parameters

### Required Parameters
- `--teacher`: Teacher model name (see supported models below)
- `--student`: Student model name (see supported models below)
- `--patients`: Patient IDs (comma-separated or single, e.g., "570" or "570,584")
- `--dataset`: Dataset name ("ohiot1dm" or "d1namo")

### Optional Parameters

#### Training Configuration
- `--seed`: Random seed for reproducibility (default: 238822)
- `--teacher-epochs`: Teacher training epochs (default: 1)
- `--student-epochs`: Student baseline epochs (default: 1)
- `--distill-epochs`: Knowledge distillation epochs (default: 1)

#### Optimization Parameters
- `--lr`: Learning rate for all phases (default: 0.001)
- `--batch-size`: Batch size for all phases (default: 32)

#### Distillation Hyperparameters
- `--alpha`: Weight for ground truth loss (student learning from actual labels, default: 0.5)
- `--beta`: Weight for teacher output loss (student learning from teacher predictions, default: 0.5)
- `--kl-weight`: Weight for KL divergence loss (knowledge transfer strength, default: 0.1)
- `--temperature`: Temperature for knowledge distillation softmax (controls soft target smoothness, default: 3.0)

#### Utility
- `--dry-run`: Preview commands without execution

## ğŸ¤– Supported Models

### Teacher Models (Large, High-Capacity)
- `bert` / `bert-base-uncased` / `bert-base-cased`
- `bert-large-uncased` / `bert-large-cased`
- `distilbert` / `distilbert-base-uncased` / `distilbert-base-cased`
- `tinybert`

### Student Models (Small, Efficient)
- `tinybert`
- `distilbert` / `distilbert-base-uncased` / `distilbert-base-cased`
- `prajjwal1/bert-tiny` (2 layers, 128 dim)
- `prajjwal1/bert-mini` (4 layers, 256 dim)
- `prajjwal1/bert-small` (4 layers, 512 dim)
- `prajjwal1/bert-medium` (8 layers, 512 dim)

## ğŸ“ Output Structure

The pipeline creates organized output directories:
```
distillation_experiments/pipeline_runs/pipeline_YYYY-MM-DD_HH-MM-SS/
â”œâ”€â”€ phase_1_teacher/          # Teacher model training
â”‚   â”œâ”€â”€ config_teacher_*.gin   # Training configuration
â”‚   â”œâ”€â”€ bert-base-uncased_570_1epochs/  # Model outputs
â”‚   â””â”€â”€ logs/                  # Training logs
â”œâ”€â”€ phase_2_student/           # Student baseline training
â”‚   â”œâ”€â”€ config_student_*.gin   # Training configuration  
â”‚   â”œâ”€â”€ prajjwal1-bert-tiny_570_1epochs/  # Model outputs
â”‚   â””â”€â”€ logs/                  # Training logs
â””â”€â”€ phase_3_distillation/      # Knowledge distillation
    â”œâ”€â”€ config_distill_*.gin   # Distillation configuration
    â”œâ”€â”€ distilled_models/      # Final distilled models
    â””â”€â”€ logs/                  # Distillation logs
```

## ğŸ›ï¸ Individual Script Usage

### 1. Teacher Training
```bash
python distillation/scripts/train_teachers.py \
    --model bert-base-uncased \
    --patients 570 \
    --dataset ohiot1dm \
    --epochs 5 \
    --lr 0.001 \
    --batch-size 32 \
    --seed 42
```

### 2. Student Baseline Training
```bash
python distillation/scripts/train_students.py \
    --model prajjwal1/bert-tiny \
    --patients 570 \
    --dataset ohiot1dm \
    --epochs 3 \
    --lr 0.001 \
    --seed 42
```

### 3. Knowledge Distillation
```bash
python distillation/scripts/distill_students.py \
    --teacher bert-base-uncased \
    --student prajjwal1/bert-tiny \
    --patients 570 \
    --dataset ohiot1dm \
    --distill-epochs 3 \
    --lr 0.001 \
    --batch-size 32 \
    --alpha 0.5 \
    --beta 0.5 \
    --kl-weight 0.1 \
    --temperature 3.0 \
    --seed 42
```

## ğŸ“Š Datasets

### Supported Datasets
- **ohiot1dm**: Ohio T1DM dataset for glucose forecasting
- **d1namo**: D1NAMO dataset for continuous glucose monitoring

### Data Path Auto-Detection
The system automatically detects data paths:
- `ohiot1dm` â†’ `./data/ohiot1dm/raw_standardized/`
- `d1namo` â†’ `./data/d1nemo/raw_standardized/`

Expected data structure:
```
data/
â”œâ”€â”€ ohiot1dm/
â”‚   â””â”€â”€ raw_standardized/
â”‚       â”œâ”€â”€ 570-ws-training.csv
â”‚       â”œâ”€â”€ 570-ws-testing.csv
â”‚       â”œâ”€â”€ 584-ws-training.csv
â”‚       â”œâ”€â”€ 584-ws-testing.csv
â”‚       â””â”€â”€ t1dm_prompt.txt
â””â”€â”€ d1namo/
    â””â”€â”€ raw_standardized/
        â”œâ”€â”€ [patient files]
        â””â”€â”€ t1dm_prompt.txt
```

## ğŸ”§ Configuration

### Model Architecture Mapping
The system maps HuggingFace model names to internal configurations:

| Model Name | Internal Config | Layers | Hidden Size |
|------------|----------------|---------|-------------|
| bert-base-uncased | BERT | 12 | 768 |
| bert-large-uncased | BERT | 24 | 1024 |
| distilbert-base-uncased | DistilBERT | 6 | 768 |
| prajjwal1/bert-tiny | BERT | 2 | 128 |
| prajjwal1/bert-mini | BERT | 4 | 256 |
| prajjwal1/bert-small | BERT | 4 | 512 |
| prajjwal1/bert-medium | BERT | 8 | 512 |

### Distillation Loss Components

The knowledge distillation loss combines multiple components:

```
Total Loss = Î± Ã— Ground_Truth_Loss + Î² Ã— Teacher_Output_Loss + Î» Ã— KL_Divergence_Loss
```

- **Ground Truth Loss (Î±)**: Student learns from actual labels
- **Teacher Output Loss (Î²)**: Student mimics teacher predictions  
- **KL Divergence Loss (Î»)**: Knowledge transfer via soft targets with temperature scaling

## ğŸ“ˆ Performance Tips

### Hyperparameter Tuning
- **Higher Î±**: More focus on task accuracy
- **Higher Î²**: More knowledge transfer from teacher
- **Higher temperature**: Softer probability distributions
- **Higher KL weight**: Stronger knowledge distillation effect

### Computational Efficiency
- Use smaller batch sizes for GPU memory constraints
- Start with fewer epochs and scale up based on convergence
- Use `--dry-run` to verify configurations before training

## ğŸ› Troubleshooting

### Common Issues
1. **Model not found**: Ensure model names match supported choices exactly
2. **Data path errors**: Verify data files exist in expected structure
3. **Memory errors**: Reduce batch size or use smaller models
4. **Permission errors**: Ensure write access to output directories

### Debug Commands
```bash
# Check available models
python distillation/scripts/train_teachers.py --model all --list-checkpoints

# Verify data paths with dry run
./distill_pipeline.sh --teacher bert --student tinybert --patients 570 --dataset ohiot1dm --dry-run

# Test individual components
python distillation/scripts/train_teachers.py --model bert --patients 570 --dataset ohiot1dm --dry-run
```

## ğŸ“ Examples

### Example 1: Quick Distillation
```bash
# Fast distillation for testing
./distill_pipeline.sh \
    --teacher bert-base-uncased \
    --student prajjwal1/bert-tiny \
    --patients 570 \
    --dataset ohiot1dm \
    --teacher-epochs 1 \
    --student-epochs 1 \
    --distill-epochs 1
```

### Example 2: High-Quality Distillation
```bash
# Comprehensive distillation with multiple patients
./distill_pipeline.sh \
    --teacher bert-large-uncased \
    --student prajjwal1/bert-small \
    --patients 570,584 \
    --dataset ohiot1dm \
    --teacher-epochs 10 \
    --student-epochs 5 \
    --distill-epochs 8 \
    --lr 0.0005 \
    --batch-size 64 \
    --alpha 0.3 \
    --beta 0.7 \
    --kl-weight 0.2 \
    --temperature 5.0
```

### Example 3: Multi-Dataset Experiments
```bash
# Train on ohiot1dm
./distill_pipeline.sh --teacher bert-base-uncased --student prajjwal1/bert-tiny --patients 570 --dataset ohiot1dm --seed 42

# Train on d1namo  
./distill_pipeline.sh --teacher bert-base-uncased --student prajjwal1/bert-tiny --patients 570 --dataset d1namo --seed 42
```

## ğŸ—ï¸ Architecture

### Pipeline Flow
```
Input Data â†’ Teacher Training â†’ Student Baseline â†’ Knowledge Distillation â†’ Compressed Model
     â†“              â†“                â†“                     â†“                      â†“
   Raw CSV    Large Model      Small Model        Distilled Model         Efficient Model
   Patients     Training        Training           (Teacherâ†’Student)      Ready for Deployment
```

### File Organization
```
distillation/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_teachers.py      # Teacher model training
â”‚   â”œâ”€â”€ train_students.py      # Student baseline training
â”‚   â””â”€â”€ distill_students.py    # Knowledge distillation
â”œâ”€â”€ __init__.py
â””â”€â”€ utils/                     # Utility functions
```

---

**Note**: This system is designed for temporal forecasting tasks with Time-LLM models. Ensure your data follows the expected CSV format with target columns and appropriate temporal structure.