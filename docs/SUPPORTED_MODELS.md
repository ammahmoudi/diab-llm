# ğŸ¤– Supported Models in Time-LLM Distillation Pipeline

This document lists all models supported across the Time-LLM implementation and distillation pipeline.

## ğŸ“‹ Model Compatibility Matrix

| Model Name | HuggingFace ID | Parameters | Time-LLM | Teacher | Student | Distillation |
|------------|----------------|------------|----------|---------|---------|--------------|
| **LLAMA** | `huggyllama/llama-7b` | ~6.7B | âœ… | âœ… | âŒ | âŒ |
| **GPT2** | `openai-community/gpt2` | ~117M | âœ… | âœ… | âŒ | âœ… |
| **BERT** | `google-bert/bert-base-uncased` | ~110M | âœ… | âœ… | âœ… | âœ… |
| **BERT-Large** | `bert-large-uncased` | ~340M | âŒ | âœ… | âŒ | âŒ |
| **DistilBERT** | `distilbert/distilbert-base-uncased` | ~66M | âœ… | âœ… | âœ… | âœ… |
| **TinyBERT** | `huawei-noah/TinyBERT_General_4L_312D` | ~14M | âœ… | âœ… | âœ… | âœ… |
| **BERT-tiny** | `prajjwal1/bert-tiny` | ~4.4M | âœ… | âœ… | âœ… | âœ… |
| **BERT-mini** | `prajjwal1/bert-mini` | ~11M | âŒ | âœ… | âœ… | âœ… |
| **BERT-small** | `prajjwal1/bert-small` | ~29M | âŒ | âœ… | âœ… | âœ… |
| **BERT-medium** | `prajjwal1/bert-medium` | ~41M | âŒ | âœ… | âœ… | âœ… |
| **MiniLM** | `nreimers/MiniLMv2-L6-H384-distilled-from-BERT-Large` | ~33M | âœ… | âœ… | âœ… | âœ… |
| **MobileBERT** | `google/mobilebert-uncased` | ~25M | âœ… | âœ… | âœ… | âœ… |
| **ALBERT** | `albert/albert-base-v2` | ~12-18M | âœ… | âœ… | âœ… | âœ… |
| **OPT-125M** | `facebook/opt-125m` | ~125M | âœ… | âœ… | âœ… | âœ… |

## ğŸ¯ Recommended Teacher-Student Pairs

### High Performance Pairs
```bash
# BERT â†’ TinyBERT (Most tested)
--teacher bert-base-uncased --student prajjwal1/bert-tiny

# BERT â†’ DistilBERT (Balanced)
--teacher bert-base-uncased --student distilbert-base-uncased

# DistilBERT â†’ BERT-tiny (Good compression)
--teacher distilbert-base-uncased --student prajjwal1/bert-tiny
```

### Experimental Pairs
```bash
# MobileBERT â†’ BERT-tiny (Mobile optimized)
--teacher google/mobilebert-uncased --student prajjwal1/bert-tiny

# ALBERT â†’ MiniLM (Efficient pair)
--teacher albert/albert-base-v2 --student nreimers/MiniLMv2-L6-H384-distilled-from-BERT-Large
```

## ğŸ”§ Model Name Mappings

### Input Formats Accepted
The distillation pipeline accepts both short names and full HuggingFace model IDs:

**Short Names:**
- `bert`, `distilbert`, `tinybert`, `minilm`, `mobilebert`, `albert`

**Full HuggingFace IDs:**
- `google-bert/bert-base-uncased`
- `distilbert/distilbert-base-uncased`
- `prajjwal1/bert-tiny`
- `huawei-noah/TinyBERT_General_4L_312D`
- `nreimers/MiniLMv2-L6-H384-distilled-from-BERT-Large`
- `google/mobilebert-uncased`
- `albert/albert-base-v2`
- `facebook/opt-125m`

### Example Usage
```bash
# Using short names
bash distill_pipeline.sh --teacher bert --student tinybert --patients 570 --dataset ohiot1dm

# Using full HuggingFace IDs
bash distill_pipeline.sh \
  --teacher google-bert/bert-base-uncased \
  --student prajjwal1/bert-tiny \
  --patients 570 --dataset ohiot1dm
```

## ğŸ“Š Model Characteristics

### Teacher Models (Large, High Accuracy)
- **BERT** (110M): Best general performance
- **DistilBERT** (66M): Good balance of size/performance
- **GPT2** (117M): Decoder-only architecture
- **OPT-125M** (125M): Meta's efficient decoder

### Student Models (Small, Fast Inference)
- **BERT-tiny** (4.4M): Smallest, fastest
- **TinyBERT** (14M): Purpose-built for distillation
- **MiniLM** (33M): Good performance/size trade-off
- **MobileBERT** (25M): Mobile-optimized

## âš ï¸ Important Notes

1. **LLAMA models** are only supported in the base Time-LLM model, not in distillation (too large for typical distillation scenarios)

2. **BERT variants** (`prajjwal1/bert-*`) are correctly mapped to their respective configurations in all scripts

3. **Model configurations** are automatically set based on the model name, including:
   - Layer count (`llm_layers`)
   - Hidden dimensions (`llm_dim`)
   - Model comments for tracking

4. **Filename sanitization** is applied automatically for model names with forward slashes

## ğŸ” Verification

To verify model support, you can test the mappings:

```bash
cd /home/amma/LLM-TIME
python3 -c "
from distillation.scripts.train_teachers import TeacherTrainer
teacher = TeacherTrainer()
print('Supported teacher models:', list(teacher.teacher_models.keys()))
"
```

This ensures consistent model support across all components of the distillation pipeline.